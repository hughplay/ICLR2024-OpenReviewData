id,title,keywords,ratings,confidences,withdraw,review_lengths,abstract,comments,url,ratings_avg,ratings_std,confidence_avg,confidence_std
zgQ0PHeGnL,Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction,"['Equivariant Graph Neural Network', 'rigid body protein-protein docking', 'interface fitting']","[5, 8, 8, 3]","[4, 3, 3, 4]",0,"[525, 714, 290, 338]","The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods, and outperforms state-of-the-art learning-based methods, like DiffDock-PP and Alphafold-Multimer, for particularly antibody-antigen docking.",17,https://openreview.net/forum?id=zgQ0PHeGnL,6.0,2.1213203435596424,3.5,0.5
zUHgYRRAWl,Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE,"['VAE', 'molecule generation']","[1, 3, 1]","[5, 4, 4]",1,"[80, 568, 34]","Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the number of active molecules generated by the multi-stage VAE in comparison to their one-stage equivalent. For each of the two tasks, our baselines include methods that use learned property predictors to incorporate target metrics directly into the training objective and we discuss complications that arise with this methodology.",3,https://openreview.net/forum?id=zUHgYRRAWl,1.6666666666666667,0.9428090415820634,4.333333333333333,0.4714045207910317
zMPHKOmQNb,Protein Discovery with Discrete Walk-Jump Sampling,"['generative modeling', 'langevin mcmc', 'energy-based models', 'score-based models', 'protein design', 'protein discovery']","[8, 8, 8]","[4, 3, 4]",0,"[591, 581, 523]","We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our $\textit{Discrete Walk-Jump Sampling}$ formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the $\textit{distributional conformity score}$ to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100\% of generated samples are successfully expressed and purified and 70\% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a single round of laboratory experiments. We also report the first demonstration of long-run fast-mixing MCMC chains where diverse antibody protein classes are visited in a single MCMC chain.",7,https://openreview.net/forum?id=zMPHKOmQNb,8.0,0.0,3.6666666666666665,0.4714045207910317
z3mPLBLfGY,Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning,['unified representation; molecular interaction; equivariant transformer'],"[6, 5, 8, 5]","[5, 4, 3, 3]",0,"[175, 389, 157, 335]","Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the universal underlying interaction physics. 
In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels.  Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains.",25,https://openreview.net/forum?id=z3mPLBLfGY,6.0,1.224744871391589,3.75,0.82915619758885
yRrPfKyJQ2,Conversational Drug Editing Using Retrieval and Domain Feedback,"['Large Language Models', 'prompt', 'retrieval', 'domain feedback', 'conversation', 'drug editing', 'drug optimization', 'controllable generation', 'small molecule', 'peptide', 'protein']","[6, 6, 6]","[2, 3, 4]",0,"[202, 195, 394]","Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",11,https://openreview.net/forum?id=yRrPfKyJQ2,6.0,0.0,3.0,0.816496580927726
yBmMgvaEtO,Stochastic Adaptive Sequential  Black-box Optimization for  Diffusion Targeted Generation,"['Black-box Optimization', 'Diffusion Model', 'Targeted Generation']","[5, 6, 3, 6]","[2, 2, 3, 2]",0,"[375, 232, 971, 175]","Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc. However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging.   To address this issue, we first formulate the fine-tuning of the inference phase of a pre-trained diffusion model as a sequential black-box optimization problem.  Furthermore, we propose a novel stochastic adaptive sequential optimization algorithm to optimize cumulative black-box scores under unknown transition dynamics.   Theoretically, we prove a $O(\frac{d^2}{\sqrt{T}})$ convergence rate for cumulative convex functions without smooth and strongly convex assumptions.   Empirically, we can naturally apply our algorithm for diffusion black-box targeted generation. Experimental results demonstrate the ability of our method to generate target-guided images with high target scores.",10,https://openreview.net/forum?id=yBmMgvaEtO,5.0,1.224744871391589,2.25,0.4330127018922193
xlQrAm3LE4,DiffSim: Aligning Diffusion Model and Molecular Dynamics Simulation for Accurate Blind Docking,"['Diffusion Model', 'Molecular Dynamics Simulation', 'Blind Docking', 'Drug Discovery', 'Molecular Conformation Generation']","[3, 5, 3, 3]","[5, 3, 4, 5]",0,"[208, 297, 370, 238]","Predicting the ligand’s binding conformation within a target protein is a pivotal step in drug discovey. Based on prior knowledge of the binding site (protein pocket) on the target protein, biochemical researchers use molecular docking software to generate the ligand conformation within that pocket. Despite its speed, molecular docking is ill-suited for blind docking where the pocket is unknown, and the generated ligand conformation often lacks required precision. Recently, deep generative models, especially diffusion models, have been proposed for accurate blind docking. However, it is found that while deep generative models excel in locating the pocket, they still lag behind traditional methods in terms of conformation generation. Thus, bridging such gap with a hybrid approach is naturally expected to further improve the model performance. Therefore, in this study, we introduce a blind docking approach named DiffSim to seamlessly integrate the diffusion model with molecular dynamics (MD) simulation. We propose a novel loss function to align reverse diffusion sampling with MD simulation trajectories, aiming to efficiently generate ligand conformations informed by MD-modelled protein-ligand interactions at atomic resolution. Through theoretical analysis, we unveil the consistency in dynamics between diffusion models and MD simulation, demonstrating that the diffusion model is essentially a coarse-grained simulator for MD simulation. Empirical results demonstrate the effectiveness of our approach and highlight the potential of combining physics-informed MD simulation with deep learning models in drug discovery.",8,https://openreview.net/forum?id=xlQrAm3LE4,3.5,0.8660254037844386,4.25,0.82915619758885
xhEN0kJh4q,Robust Model-Based Optimization for Challenging Fitness Landscapes,"['Model based optimization', 'protein engineering']","[6, 8, 5, 6]","[1, 3, 2, 3]",0,"[276, 203, 478, 387]","Protein design, a grand challenge of the day, involves optimization on a fitness landscape, and leading methods adopt a model-based approach where a model is trained on a training set (protein sequences and fitness) and proposes candidates to explore next. These methods are challenged by sparsity of high-fitness samples in the training set, a problem that has been in the literature. A less recognized but equally important problem stems from the distribution of training samples in the design space: leading methods are not designed for scenarios where the desired optimum is in a region that is not only poorly represented in training data, but also relatively far from the highly represented low-fitness regions. We show that this problem of “separation” in the design space is a significant bottleneck in existing model-based optimization tools and propose a new approach that uses a novel VAE as its search model to overcome the problem. We demonstrate its advantage over prior methods in robustly finding improved samples, regardless of the imbalance and separation between low- and high-fitness samples. Our comprehensive benchmark on real and semi-synthetic protein datasets as well as solution design for physics-informed neural networks, showcases the generality of our approach in discrete and continuous design spaces. Our implementation is available at https://anonymous.4open.science/r/PPGVAE-F83E.",13,https://openreview.net/forum?id=xhEN0kJh4q,6.25,1.0897247358851685,2.25,0.82915619758885
xcMmebCT7s,Learning to design protein-protein interactions with enhanced generalization,"['protein-protein interactions', 'protein design', 'generalization', 'self-supervised learning', 'equivariant 3D representations']","[3, 6, 6, 8, 6]","[5, 2, 4, 5, 3]",0,"[821, 209, 392, 244, 212]","Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage PPIRef to pre-train PPIformer, a new SE(3)-equivariant model, generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on the new non-leaking splits of the standard labeled PPI mutational data and independent case studies optimizing a human antibody against SARS-CoV-2 and increasing staphylokinase thrombolytic activity.",31,https://openreview.net/forum?id=xcMmebCT7s,5.8,1.6,3.8,1.16619037896906
wmq67R2PIu,DockGame: Cooperative Games for Multimeric Rigid Protein Docking,"['protein docking', 'multi-chain docking', 'score matching', 'multi-agent diffusion']","[3, 3, 3, 5]","[5, 4, 5, 5]",0,"[2066, 631, 194, 355]","Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a 
diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains.",15,https://openreview.net/forum?id=wmq67R2PIu,3.5,0.8660254037844386,4.75,0.4330127018922193
wZNJ7rj1ok,SAGMAN: Stability Analysis of Graph Neural Networks (GNNs) on the Manifolds,"['Manifolds', 'Stability', 'Graph Neural Networks']","[6, 5, 3, 3, 5]","[3, 3, 3, 4, 3]",1,"[536, 327, 193, 300, 373]","Graph neural networks (GNNs)  are highly effective at tasks that involve analyzing graph-structured data, such as predicting protein interactions, modeling social networks, and identifying communities within graphs. However,  modern GNNs can be sensitive to changes in the input graph structure and node features,   leading to unpredictable behavior and degraded performance. In this work, we introduce a spectral framework (SAGMAN) for analyzing the stability of GNNs. SAGMAN quantifies the stability of each node by examining the distance mapping distortions (DMDs) on the input/output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant nodes (data samples) on the output manifold, it implies a large DMD and thus poor GNN stability.  To create low-dimensional input/output manifolds for meaningful DMD estimations while exploiting both the input graph topology and node features, we propose a spectral sparsification framework for estimating probabilistic graphical models (PGMs)  such that the constructed input/output graph structures can well preserve pair-wise distances on the manifolds. Our empirical evaluations show that SAGMAN can effectively reveal each node's stability under various edge/feature perturbations, offering a scalable approach for assessing the stability of GNNs.",5,https://openreview.net/forum?id=wZNJ7rj1ok,4.4,1.2,3.2,0.39999999999999997
umUIYdLtvh,EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction,"['Pocket Detection', 'Binding Site Prediction', 'Graph Neural Network', 'Drug Discovery']","[5, 6, 5, 6]","[4, 4, 3, 4]",0,"[256, 286, 214, 340]","Predicting the binding sites of target proteins plays a fundamental role in drug discovery. 
Most existing deep-learning methods consider a protein as a 3D image by spatially clustering its atoms into voxels and then feed the voxelized protein into a 3D CNN for prediction. However, the CNN-based methods encounter several critical issues: 1) defective in representing irregular protein structures; 2) sensitive to rotations; 3) insufficient to characterize the protein surface; 4) unaware of protein size shift. To address the above issues, this work proposes EquiPocket, an E(3)-equivariant Graph Neural Network (GNN) for binding site prediction. 
In particular, EquiPocket consists of the three modules: the first one to extract local geometric information for each surface atom, the second one to model both the chemical and spatial structure of protein and the last one to capture the geometry of the surface via equivariant message passing over the surface atoms. We further propose a dense attention output layer to alleviate the effect incurred by the variable protein size. Extensive experiments on several representative benchmarks demonstrate the superiority of our framework to the state-of-the-art methods.",24,https://openreview.net/forum?id=umUIYdLtvh,5.5,0.5,3.75,0.4330127018922193
uUEvmY8Gfz,De novo Drug Design using Reinforcement Learning with Dynamic Vocabulary,"['De novo drug design', 'Molecular generation', 'Reinforcement learning', 'Dynamic vocabulary']","[3, 3, 3, 3]","[4, 3, 3, 4]",1,"[356, 333, 349, 396]","De novo drug design constitutes a fundamental challenge within the domain of computer-aided drug discovery (CADD). Generative models relying on SMILES molecular strings have emerged as promising tools for this purpose. However, extant SMILES-based generative models all adopt a fixed vocabulary, leading to deficiencies in both sampling efficiency and interpretability. 
In this paper, we propose RLDV, a reinforcement learning (RL) algorithm based on a GPT agent, which uses a dynamic chemical vocabulary (DV) during RL iterations. Specifically, we utilize SMILES pair encoding to analyze high-scoring molecular SMILES strings generated during the RL process, and extract their high-frequency common substrings, which are then added as new tokens to the agent's vocabulary. These additions aid in the generation of molecules during subsequent RL steps. Experimental results on the GuacaMol benchmark demonstrate that our algorithm outperforms existing models across multiple tasks, highlighting the practical significance of the dynamic vocabulary in drug design. Furthermore, the application of our algorithm in the design of protein-targeting drugs for SARS-CoV-2 underscores its substantial practical relevance.",4,https://openreview.net/forum?id=uUEvmY8Gfz,3.0,0.0,3.5,0.5
uMAujpVi9m,Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment,"['Drug Discovery', 'Pretraining']","[6, 6, 6, 6]","[4, 3, 1, 5]",0,"[252, 224, 379, 390]","Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.",19,https://openreview.net/forum?id=uMAujpVi9m,6.0,0.0,3.25,1.479019945774904
uKB4cFNQFg,BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks,"['Biological sequence analysis', 'enhancer annotation', 'gene finding', 'gene annotation', 'Language model', 'genome modelling', 'benchmark', 'LLM', 'embeddings', 'representations', 'DNA']","[3, 6, 6, 5]","[4, 4, 5, 5]",0,"[255, 657, 342, 505]","The genome sequence contains the blueprint for governing cellular processes. 
  While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. 
  Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce **BEND**, a **BEN**chmark for **D**NA language models, featuring
  a collection of realistic and biologically meaningful downstream tasks defined on the human genome.
  We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.
  BEND is available at https://anonymous.4open.science/r/BEND-8C42/README.md",12,https://openreview.net/forum?id=uKB4cFNQFg,5.0,1.224744871391589,4.5,0.5
uH0FGECSEI,Expected flow networks in stochastic environments and two-player zero-sum games,"['generative flow networks', 'GFlowNets', 'protein design', 'game theory', 'self-play', 'adversarial learning', 'stochastic environments', 'quantal response equilibrium', 'Luce agents', 'sequential decision making']","[6, 3, 8, 5]","[3, 5, 3, 4]",0,"[322, 873, 157, 211]","Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.",21,https://openreview.net/forum?id=uH0FGECSEI,5.5,1.8027756377319946,3.75,0.82915619758885
tPjVRmHqCg,Curiosity Driven Protein Sequence Generation via Reinforcement Learning,"['Protein Sequence Design', 'RL']","[5, 3, 5]","[4, 4, 2]",0,"[1316, 710, 192]","Protein sequence design is a critical problem in the field of bio-engineering and biotechnology. However, the search space for protein sequence design is incredibly vast and sparsely populated, which poses significant challenges. On the other hand, generative models struggle to adapt to different usage scenarios and objectives, leading to limited adaptability and generalization. To address these challenges, we explore a reinforcement learning algorithm based on latent space that enables protein sequence generation and mutation for different scenarios. Our approach has several advantages: (1) The reinforcement learning algorithm allows us to adjust the reward function according to different tasks and scenarios, enabling the model to generate and mutate protein sequences in a targeted manner. (2) The latent space mapped by ESM-2 is continuous, unlike the initial sparse and discrete space, and the curiosity mechanism further improves search efficiency. We evaluate our method in completely different scenarios, including different protein functions and sequences, and our experimental results demonstrate significant performance improvement over existing methods. We conduct multiple ablation studies to validate the rationality of our design.",3,https://openreview.net/forum?id=tPjVRmHqCg,4.333333333333333,0.9428090415820634,3.3333333333333335,0.9428090415820634
tNAucRS0QQ,General-purpose Pre-trained Model Towards Cross-domain Molecule Learning,"['molecular representation learning', 'self-supervised pre-training', 'multimodal learning']","[5, 8, 5, 3]","[4, 4, 4, 4]",0,"[482, 340, 594, 349]","Self-supervised pre-training on biomolecules has achieved remarkable success in various biochemical applications, such as drug discovery and protein design. However, in most approaches, the learning model is primarily constructed based on the characteristics of either small molecules or proteins, without exploring their potential binding interactions -- an essential cross-domain relationship crucial for driving numerous biological processes. In this paper, inspired by the success of multimodal learning, we fill this gap by proposing a general-purpose foundation model named **BIT** (an abbreviation for **B**iomolecular **I**nteraction **T**ransformer), which is capable of encoding a range of biochemical entities, including small molecules, proteins, and protein-ligand complexes, as well as various data formats, encompassing both 2D and 3D structures, all within a shared Transformer backbone, via multiple unified self-supervised atom-level *denoising* tasks. We introduce *Mixture-of-Domain-Experts* (MoDE) to handle the biomolecules from diverse chemical domains and incorporate separate structural channels to capture positional dependencies in the molecular structures. The proposed MoDE allows BIT to enable both deep fusion and domain-specific encoding and learn cross-domain relationships on protein-ligand complexes with 3D cocrystal structures. Experimental results demonstrate that BIT achieves exceptional performance in both protein-ligand binding and molecular learning downstream tasks, including binding affinity prediction, virtual screening, and molecular property prediction.",13,https://openreview.net/forum?id=tNAucRS0QQ,5.25,1.7853571071357126,4.0,0.0
t0m0DdCCQ2,Liteformer: Lightweight Evoformer for Protein Structure Prediction,"['protein structure prediction', 'efficient transformer']","[3, 5, 5]","[4, 3, 4]",0,"[460, 332, 303]","AlphaFold2 has achieved seminal success in predicting structures from amino acid sequences with remarkable atomic accuracy. However, its Evoformer module faces a critical challenge in terms of high memory consumption, particularly concerning the computational complexity associated with sequence length $L$ and the number of Multiple Sequence Alignments (MSA), denoted as $s$. This challenge arises from the attention mechanism involving third-order MSA and pair-wise tensors, leading to a complexity of $\mathcal{O}(L^3+sL^2)$.
This memory bottleneck poses difficulties when working with lengthy protein sequences. To tackle this problem, we introduce a novel and lightweight variant of Evoformer named Liteformer. Liteformer employs an innovative attention linearization mechanism, reducing complexity to $\mathcal{O}(L^2+sL)$ through the implementation of a bias-aware flow attention mechanism, which seamlessly integrates MSA sequences and pair-wise information. Our extensive experiments, conducted on both monomeric and multimeric benchmark datasets, showcase the efficiency gains of our framework.  Specifically, compared with Evoformer, Liteformer achieves up to a 44\% reduction in memory usage and a 23\% acceleration in training speed, all while maintaining competitive accuracy in protein structure prediction.",17,https://openreview.net/forum?id=t0m0DdCCQ2,4.333333333333333,0.9428090415820634,3.6666666666666665,0.4714045207910317
siemhfrFXV,CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention,"['AI for science', 'protein crystallography', 'Transformer model']","[3, 6, 3, 3]","[3, 4, 3, 3]",1,"[513, 608, 304, 288]","Determining the structure of a protein has been a decades-long open question. A protein's three-dimensional structure often poses nontrivial computation costs, when classical simulation algorithms are utilized. Advances in the transformer neural network architecture --such as AlphaFold-- achieve significant improvements for this problem, by learning from a large dataset of sequence information and corresponding protein structures. Yet, such methods only focus on sequence information; other available prior knowledge, such as protein crystallography and partial structure of amino acids, could be potentially utilized. To the best of our knowledge, we propose the first transformer-based model that directly utilizes protein crystallography and partial structure information to predict the electron density maps of proteins. Via two new datasets of peptide fragments (2-residue and 15-residue) , we demonstrate our method, dubbed CrysFormer, can achieve accurate predictions, based on a much smaller dataset size and with reduced computation costs.",4,https://openreview.net/forum?id=siemhfrFXV,3.75,1.299038105676658,3.25,0.4330127018922193
sTYuRVrdK3,Evaluating Representation Learning on the Protein Structure Universe,"['Protein', 'Representation', 'Learning', 'Protein Structure']","[8, 6, 5, 6]","[4, 5, 5, 3]",0,"[388, 573, 259, 265]","Protein structure representation learning is the foundation for promising applications in drug discovery, protein design, and protein function prediction. However, there remains a need for a robust, standardised benchmark to track the progress of new and established methods with greater granularity and relevance to downstream applications. In this work, we introduce a comprehensive and open benchmark suite for evaluating protein structure representation learning methods.

We provide several pre-training methods, downstream tasks and pre-training corpora comprised of both experimental and predicted structures, offering a balanced challenge to representation learning algorithms. These tasks enable the systematic evaluation of the quality of the learned embeddings, the structural and functional relationships captured, and their usefulness in downstream tasks. We benchmark state-of-the-art protein-specific and generic geometric Graph Neural Networks and the extent to which they benefit from different types of pre-training. We find that pre-training consistently improves the performance of both rotation-invariant and equivariant models, and that equivariant models seem to benefit even more from pre-training compared to invariant models.

We aim to establish a common ground for the machine learning and computational biology communities to collaborate, compare, and advance protein structure representation learning. By providing a standardised and rigorous evaluation platform, we expect to accelerate the development of novel methodologies and improve our understanding of protein structures and their functions. The codebase incorporates several engineering contributions which considerably reduces the barrier to entry for pre-training and working with large structure-based datasets. Our benchmark is available at: https://anonymous.4open.science/r/ProteinWorkshop-B8F5/",16,https://openreview.net/forum?id=sTYuRVrdK3,6.25,1.0897247358851685,4.25,0.82915619758885
sFJr7okOBi,NL2ProGPT: Taming Large Language Model for Conversational Protein Design,"['Protein design', 'Large language model']","[3, 5, 5, 5]","[5, 3, 3, 2]",0,"[675, 134, 812, 297]","Large Language Models (LLMs), like ChatGPT, excel in cross-modal tasks thanks to their powerful abilities in natural language comprehension, generalization, and reasoning. Meanwhile, the wealth of human-curated protein knowledge in text form presents a unique opportunity for LLMs to contribute to advanced protein design. In this work, we propose a new LLMs-based framework, namely NL2ProGPT, for macromolecular protein sequence generation that bridges the domain gap between natural and protein languages. Specifically, we first combine the protein functions and properties to create specific text guidelines for designing the protein, ensuring it follows precise controls. Second, to form a more informative and generalizable protein description, we explicitly inject protein structural information by clustering the embeddings from pre-trained protein language models. Third, we train a reward model to align the protein language model with the Rosetta energy function, following an RLAIF (reinforced learning from AI feedback) fashion. We empirically verify the effectiveness of NL2ProGPT from three aspects: (1) outperforms existing protein sequence design methods in different evaluations; (2) exhibits more than 90\% consistency in text-to-protein generation; (3) has effective exploration potential in disordered regions.",10,https://openreview.net/forum?id=sFJr7okOBi,4.5,0.8660254037844386,3.25,1.0897247358851685
s4mPCrSNUZ,PROTEIN DESIGNER BASED ON SEQUENCE PROFILE USING ULTRAFAST SHAPE RECOGNITION,"['Protein sequence design', 'Sequence profile', 'Ultrafast shape recognition', 'Protein language models', 'Graph neural network']","[3, 5, 5, 5]","[3, 3, 4, 4]",1,"[324, 695, 285, 237]","The process of designing proteins with specified structure and function, which can deepen our understanding of living systems and facilitate the fight against disease, involves a critical component known as sequence design. With the continuous development of deep learning, existing methods have shown excellent performance in protein sequence design. However, most of them focus on optimizing the network architecture to improve performance, while ignoring the explicit biochemical features of proteins. Observing the remarkable success achieved through structural templates and pre-trained knowledge in protein structure prediction, we explored whether similar sequence patterns and representations of underlying structural knowledge can be used in protein sequence design. In this work, we proposed SPDesign, a method for protein sequence design based on sequence profile using ultrafast shape recognition. For an input backbone structure, SPDesign utilizes ultrafast shape recognition vectors to search for similar protein structures (structural analogs) in the PAcluster80 structure library. It then extracts the sequence profile from the analogs through structural alignment. Along with structural pre-trained knowledge and geometric features, they are further condensed to provide reliable sequence patterns for an improved graph neural network. Experimental results show that SPDesign significantly outperforms the state-of-the-art methods on CATH 4.2 benchmark, such as LM-Design and Pifold, leading to 11.4\% and 15.54\% accuracy gains in sequence recovery rate, respectively. Encouraging results have been achieved on the TS50 and TS500 benchmarks, with performance reaching 68.64\% and 71.63\%, respectively. Particularly noteworthy is that our method also achieved significant performance on de novo designed proteins and orphan proteins that are close to practical application scenarios. Finally, the structural modeling verification experiment shows that the sequences designed by our method can fold into the native structures more accurately.",4,https://openreview.net/forum?id=s4mPCrSNUZ,4.5,0.8660254037844386,3.5,0.5
rxlF2Zv8x0,Improving protein optimization with smoothed fitness landscapes,"['protein design', 'discrete optimization', 'protein engineering', 'markov chain monte carlo', 'graph signal processing']","[6, 6, 6, 3]","[4, 3, 3, 3]",0,"[976, 444, 350, 292]","The ability to engineer novel proteins with higher fitness on a given task would be revolutionary for biotechnology and medicine. Modeling the combinatorially large space of sequences is infeasible; prior methods often constrain optimization to a small mutational radius, but this drastically limits the design space. Instead of heuristics, we propose smoothing the fitness landscape to facilitate protein optimization. First, we formulate protein fitness as a graph signal then use Tikunov regularization to smooth the fitness landscape. We find optimizing in this smoothed landscape leads to improved performance across multiple methods in the GFP and AAV benchmarks. Second, we achieve state-of-the-art results utilizing discrete energy-based models and MCMC in the smoothed landscape. Our method, called Gibbs sampling with Graph-based Smoothing (GGS), demonstrates a unique ability to achieve 2.5 fold fitness improvement (using an in-silico oracle) over its training set. GGS demonstrates potential to optimize proteins in the limited data regime. Code is included as part of the supplementary material.",18,https://openreview.net/forum?id=rxlF2Zv8x0,5.25,1.299038105676658,3.25,0.4330127018922193
qwYKE3VB2h,From Graphs to Hypergraphs: Hypergraph Projection and its Remediation,"['hypergraphs', 'hypergraph projection', 'learning-based reconstruction']","[6, 8, 8, 8]","[4, 5, 3, 5]",0,"[628, 509, 254, 316]","We study the implications of the modeling choice to use a graph, instead of a hypergraph, to represent real-world interconnected systems whose constituent relationships are of higher order by nature. Such a modeling choice typically involves an underlying projection process that maps the original hypergraph onto a graph, and is prevalent in graph-based analysis.  While hypergraph projection can potentially lead to loss of higher-order relations, there exists very limited studies on the consequences of doing so, as well as its remediation. This work fills this gap by doing two things: (1) we develop analysis based on graph and set theory, showing two ubiquitous patterns of hyperedges that are root to structural information loss in all hypergraph projections; we also quantify the combinatorial impossibility of recovering the lost higher-order structures if no extra help is provided; (2) we still seek to recover the lost higher-order structures in hypergraph projection, and in light of (1)'s findings we make reasonable assumptions to allow the help of some prior knowledge of the application domain. Under this problem setting, we develop a learning-based hypergraph reconstruction method based on an important statistic of hyperedge distributions that we find. Our reconstruction method is systematically evaluated on 8 real-world datasets under different settings, and exhibits consistently top performance. We also demonstrate benefits of the reconstructed hypergraphs through use cases of protein rankings and link predictions.",20,https://openreview.net/forum?id=qwYKE3VB2h,7.5,0.8660254037844386,4.25,0.82915619758885
qg2boc2AwU,Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model,"['protein-protein docking', 'energy-based model', 'geometric deep learning', 'energy-function']","[6, 8, 3, 6]","[4, 3, 4, 5]",0,"[482, 262, 264, 433]","Protein complex formation, a pivotal challenge in contemporary biology, has recently gained interest from the machine learning community, particularly concerning protein-ligand docking tasks. In this paper, we delve into the equally crucial but comparatively under-investigated domain of protein-protein docking. Specifically, we propose a  geometric deep learning framework, termed EBMDock,  which employs statistical potential as its energy function. This approach produces a probability distribution over docking poses, such that the identified docking pose aligns with a minimum point in the energy landscape. We employ a differential algorithm grounded in Langevin dynamics to efficiently sample from the docking pose distribution. Additionally, we incorporate energy-based training using contrastive divergence, enhancing both performance and stability. Empirical results demonstrate that our approach achieves superior performance on two benchmark datasets DIPS and DB5.5. Furthermore, the results suggest EBMDock can serve as an orthogonal enhancement to existing methods. Source code will be released.",25,https://openreview.net/forum?id=qg2boc2AwU,5.75,1.7853571071357126,4.0,0.7071067811865476
qH9nrMNTIW,Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models,"['Diffusion Models', 'Structure-Based Drug Design', 'Molecule Generation']","[6, 5, 8, 6]","[3, 3, 4, 3]",0,"[326, 539, 387, 429]","Generating 3D ligand molecules that bind to specific protein targets via diffusion models has shown great promise for structure-based drug design. The key idea is to disrupt molecules into noise through a fixed forward process and learn its reverse process to generate molecules from noise in a denoising way. However, existing diffusion models primarily focus on incorporating protein-ligand interaction information solely in the reverse process, and neglect the interactions in the forward process. The inconsistency between forward and reverse processes may impair the binding affinity of generated molecules towards target protein. In this paper, we propose a novel Interaction Prior-guided Diffusion model (IPDiff) for the protein-specific 3D molecular generation by introducing geometric protein-ligand interactions into both diffusion and sampling process. Specifically, we begin by pretraining a protein-ligand interaction prior network (IPNet) by utilizing the binding affinity signals as supervision. Subsequently, we leverage the pretrained prior network to (1) integrate interactions between the target protein and the molecular ligand into the forward process for adapting the molecule diffusion trajectories (prior-shifting), and (2) enhance the binding-aware molecule sampling process (prior-conditioning). Empirical studies on CrossDocked2020 dataset show IPDiff can generate molecules with more realistic 3D structures and state-of-the-art binding affinities towards the protein targets, with up to -6.42 Avg. Vina Score, while maintaining proper molecular properties.",24,https://openreview.net/forum?id=qH9nrMNTIW,6.25,1.0897247358851685,3.25,0.4330127018922193
o0C2v4xTdS,CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation,"['conformer generation', 'coarse-grained', 'coarse-graining', '3D molecule generation', 'equivariance', 'SE(3)-equivariance', 'ligand', 'protein-ligand', 'binding affinity', 'structure-based drug discovery', 'variational autoencoder']","[5, 6, 5, 8]","[3, 3, 3, 4]",0,"[180, 281, 258, 332]","Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furthermore, we evaluate the chemical and biochemical quality of our generated conformers on multiple downstream applications, including property prediction and oracle-based protein docking. Overall, CoarsenConf generates more accurate conformer ensembles compared to prior generative models.",17,https://openreview.net/forum?id=o0C2v4xTdS,6.0,1.224744871391589,3.25,0.4330127018922193
nqlymMx42E,Searching for High-Value Molecules Using Reinforcement Learning and Transformers,"['chemistry', 'reinforcement learning', 'language models']","[8, 6, 8, 6]","[4, 4, 2, 2]",0,"[502, 334, 531, 453]","Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",20,https://openreview.net/forum?id=nqlymMx42E,7.0,1.0,3.0,1.0
mpqMVWgqjn,KW-Design: Pushing the Limit of Protein Deign via Knowledge Refinement,"['Protein Deign', 'Graph', 'Finetuning']","[6, 6, 6]","[5, 3, 3]",0,"[570, 327, 370]","Recent studies have shown competitive performance in protein inverse folding, while most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. Given the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, TS500, and PDB datasets and our results show that our KW-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. KW-Design is the first method that achieves 60+\% recovery on all these benchmarks. We also provide additional analysis to demonstrate the effectiveness of our proposed method. The code will be publicly available upon acceptance.",3,https://openreview.net/forum?id=mpqMVWgqjn,6.0,0.0,3.6666666666666665,0.9428090415820634
mJgymwRsWw,Active Probabilistic Drug Discovery,"['Drug Discovery', 'Active Learning', 'Molecule Clustering']","[1, 3, 3, 3]","[5, 3, 2, 5]",0,"[568, 783, 173, 926]","Early drug discovery plays a crucial role in the development of new medications
by focusing on the identification and optimization of lead molecules that specif-
ically bind to target proteins. However, this process is accompanied by various
challenges, such as the vastness of molecule libraries, high attrition rate, and the
intricate nature of molecular interactions. To overcome these challenges, there is
a paradigm shift towards integrating intelligence and automation into end-to-end
operations. Intelligent computing aids in the discovery and recommendation of
molecules, while automated experiments offer data validation and feedback. This
innovative approach can be viewed as an active probabilistic learning problem,
assuming that active molecules binding to a specific target are typically a small
proportion and exhibit cluster-distributed characteristics. Based on this formu-
lation, we propose a novel active probabilistic drug discovery (APDD) method,
which iteratively updates the binding probabilities of molecules to progressively
enhance drug discovery performance with three consecutive steps of probabilistic
clustering, selective docking, and active wet-experiment. We conduct extensive
experiments on two benchmark datasets of DUD-E and LIT-PCBA and a simu-
lated virtual library. The results demonstrate the feasibility and efficiency of our
approach, showcasing substantial cost savings with an average reduction of 80% in
computational docking expenses and 70% in wet experimental costs, while main-
taining high accuracy in lead molecule discovery.",4,https://openreview.net/forum?id=mJgymwRsWw,2.5,0.8660254037844386,3.75,1.299038105676658
kJFIH23hXb,SE(3)-Stochastic Flow Matching for Protein Backbone Generation,['Proteins; Equivariance; Riemannian; Flow Matching; Generative models'],"[8, 8, 8, 8]","[4, 3, 4, 4]",0,"[253, 891, 365, 527]","The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce \foldflow, a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\mathrm{D}$ rigid motions---i.e. the group $\mathrm{SE}(3)$---enabling accurate modeling of protein backbones. We first introduce FoldFlow-Base, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\mathrm{SE}(3)$. We next accelerate training by incorporating Riemannian optimal transport to create FoldFlow-OT, leading to the construction of both more simple and stable flows. Finally, we design FoldFlow-SFM, coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\mathrm{SE}(3)$. Our family of FoldFlow, generative models offers several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over $\mathrm{SE}(3)$. Empirically, we validate our FoldFlow, models on protein backbone generation of up to $300$ amino acids leading to high-quality designable, diverse, and novel samples.",24,https://openreview.net/forum?id=kJFIH23hXb,8.0,0.0,3.75,0.4330127018922193
jsQPjIaNNh,Illuminating Protein Function Prediction through Inter-Protein Similarity Modeling,"['Protein function prediction', 'retrieveal-based methods', 'transductive learning']","[6, 6, 6, 6, 5, 5, 3, 5]","[3, 3, 3, 4, 4, 3, 4, 3]",0,"[348, 354, 464, 531, 336, 448, 244, 432]","Proteins, central to biological systems, are complex due to interactions between sequences, structures, and functions shaped by physics and evolution, posing a challenge for accurate function prediction. Recent advancements in deep learning techniques demonstrate substantial potential for precise function prediction through learning representations from extensive protein sequences and structures. Nevertheless, practical function annotation heavily relies on modeling protein similarity using sequence or structure retrieval tools, given their accuracy and interpretability. To study the effect of inter-protein similarity modeling, in this paper, we comprehensively benchmark the retriever-based methods against predictors on protein function tasks, demonstrating the potency of retriever-based approaches. Inspired by these findings, we introduce an innovative variational pseudo-likelihood framework, ProtIR, designed to improve function prediction through iterative refinement between predictors and retrievers. ProtIR combines the strengths of both predictors and retrievers, showcasing an around 10% improvement over vanilla predictor-based methods. Furthermore, it achieves comparable performance to the state-of-the-art protein language model-based methods with significantly smaller training time, highlighting the efficacy of our approach.",21,https://openreview.net/forum?id=jsQPjIaNNh,5.25,0.9682458365518543,3.375,0.4841229182759271
jZPqf2G9Sw,Dynamics-Informed Protein Design with Structure Conditioning,"['Diffusion Models', 'Generative Modeling', 'Protein Design', 'Normal Mode Analysis']","[5, 5, 6, 6]","[4, 3, 4, 4]",0,"[510, 497, 373, 1133]","Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein’s dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to the classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future.",4,https://openreview.net/forum?id=jZPqf2G9Sw,5.5,0.5,3.75,0.4330127018922193
jJCeMiwHdH,BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph,"['drug discovery', 'foundation model', 'multi-modal learning', 'knowledge graph']","[8, 6, 6, 8]","[5, 3, 4, 4]",0,"[519, 302, 479, 476]","Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone.
To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs.
Our empirical results demonstrate that BioBridge can
beat the best baseline KG embedding methods (on average by $\sim 76.3\%$) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBridge presents itself as a general-purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.",19,https://openreview.net/forum?id=jJCeMiwHdH,7.0,1.0,4.0,0.7071067811865476
itGkF993gz,MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding,"['Bioinformatics', 'Protein-Protein Interaction', 'Protein Sequence-Structure Co-Modeling']","[8, 6, 3]","[5, 4, 4]",0,"[443, 230, 364]","Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. However, the complexity of protein structure modeling hinders its application to large-scale PPI prediction. To address this challenge and take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the ""vocabulary"" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment ``vocabulary"" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.",10,https://openreview.net/forum?id=itGkF993gz,5.666666666666667,2.0548046676563256,4.333333333333333,0.4714045207910317
iKsu33WcmU,ProtChatGPT: Towards Understanding Proteins with Large Language Models,"['Large Language Models', 'ChatGPT-like system', 'Protein Understanding']","[6, 5, 6, 3]","[4, 3, 4, 4]",0,"[413, 308, 597, 279]","Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code will be publicly available.",4,https://openreview.net/forum?id=iKsu33WcmU,5.0,1.224744871391589,3.75,0.4330127018922193
iBAWiEjogY,ProteiNexus: Illuminating Protein Pathways through Structural Pre-training,"['Protein Representation Learning', 'Large-Scale 3D Protein Pretraining', 'Structural biology']","[3, 5, 3]","[5, 4, 4]",0,"[886, 465, 1447]","Protein representation learning has emerged as a powerful tool for various biological tasks. Language models derived from protein sequences represent the predominant trend in many current approaches. However, recent advances reveal that protein sequences alone cannot fully encapsulate the abundant information contained within protein structures, critical for understanding protein function and aiding innovative protein design. In this study, we present ProteiNexus, an innovative approach, effectively integrating protein structure learning with numerous downstream tasks. We propose a structural encoding mechanism adept at capturing fine-grained distance details and spatial positioning. By implementing a robust pre-training strategy and fine-tuning with lightweight decoders designed for specific downstream tasks, our model exhibits outstanding performance, establishing new benchmarks across a range of tasks. The code and models could be found at github repos.",14,https://openreview.net/forum?id=iBAWiEjogY,3.6666666666666665,0.9428090415820634,4.333333333333333,0.4714045207910317
hEGcVa1l4I,Efficient Parameter Tuning of Large Protein Language Models for De Novo Protein Design,['De novo protein design; protein language model; prefix tuning.'],"[3, 5, 3]","[4, 4, 4]",1,"[357, 355, 283]","Protein language models (ProtLMs) have achieved unprecedented breakthroughs in protein design. However, optimizing ProtLMs effectively with limited data has been challenging due to their large number of parameters. In this study, we introduce prefix tuning to efficiently prompt the pre-trained ProtLMs for de novo protein design with desired structures and functions. During the training process, only the prefix virtual token is trainable,  while the pre-trained ProtLM is frozen. We trained two prefix virtual tokens on antimicrobial peptide (AMP) dataset and alpha-helix strucutre dataset, respectively. Our results demonstrate that prefix tuning is efficient to prompt the pre-trained ProtLM by optimizing fewer trainable parameters to achieve superior results compared with fine tuning, even under low-data settings. Furthermore, these two prefix virtual tokens can be combined to precisely control protein generation with both desired properties, which is not possessed by other tuning methods. We anticipate that prefix tuning will contribute to the protein discovery and biomedical advancement.",3,https://openreview.net/forum?id=hEGcVa1l4I,3.6666666666666665,0.9428090415820634,4.0,0.0
gBV21wK07P,3D Autoencoding Diffusion Model for Molecule Interpolation and Manipulation,"['diffusion models', '3D molecule optimization', 'controllable generation', 'equivariant GNN']","[3, 3, 3, 5]","[5, 4, 3, 2]",0,"[550, 438, 448, 256]","Manipulating known molecules and interpolating between them is useful for many applications in drug design and protein engineering, where exploration around the molecular templates is involved. Recent studies using equivariant diffusion models have made significant progress in the de novo generation of high-quality molecules, but using these models to directly manipulate a specified template remains less explored. This is mainly due to an intrinsic property of diffusion models: the lack of a latent semantic space that is easy to operate on. To address this issue, we propose the first semantics-guided equivariant diffusion model that leverages the “semantic” embedding of a 3D molecule, learned from an auxiliary encoder, to control the generative denoising process. By modifying the embedding, we can steer the generation towards another specified molecule or a desired molecular property. We show that our model can effectively manipulate basic chemical properties, outperforming several baselines. We further verify that our approach can achieve smoother interpolation between 3D molecular pairs compared to standard diffusion models.",10,https://openreview.net/forum?id=gBV21wK07P,3.5,0.8660254037844386,3.5,1.118033988749895
fmoknhh7CH,Harmonic Prior Flow Matching for Multi-Ligand Docking and Binding Site Design,"['flow matching', 'generative models', 'proteins', 'molecules']","[5, 5, 6, 5, 5]","[4, 5, 4, 5, 3]",0,"[255, 256, 108, 248, 416]","A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design.",22,https://openreview.net/forum?id=fmoknhh7CH,5.2,0.39999999999999997,4.2,0.7483314773547882
fNOewRJLgQ,Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders,"['Protein Structure Generative Models', 'Hierarchical Protein Latent Models', 'SO(3)-Equivariant Neural Networks', 'Physics-inspired Machine Learning']","[3, 3, 5]","[2, 4, 3]",0,"[460, 215, 414]","Three-dimensional native states of natural proteins display recurring and hierarchical patterns. Yet, traditional graph-based modeling of protein structures is often limited to operate within a single fine-grained resolution, and lacks hourglass neural architectures to learn those high-level building blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries. Our model departs from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. We train Ophiuchus on contiguous fragments of PDB monomers, investigating its reconstruction capabilities across different compression rates. We examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure snapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion probabilistic models (DDPM) to efficiently sample readily-decodable latent embeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be a scalable basis for efficient protein modeling and generation.",7,https://openreview.net/forum?id=fNOewRJLgQ,3.6666666666666665,0.9428090415820634,3.0,0.816496580927726
f6KkyweyYh,Biological Sequence Analysis Using B ́ezier Curve,"['Bio-sequence Analysis', 'Bezier Curve', 'Chaos Game Representation', 'Deep Learning-based Classification', 'Image Classification']","[6, 3, 6, 5]","[3, 3, 3, 2]",0,"[164, 338, 386, 329]","The analysis of biological (e.g., protein and DNA) sequences is essential for disease diagnosis, biomaterial engineering, genetic engineering, and drug discovery domains. Conventional analytical methods focus on transforming sequences into numerical representations for applying machine learning/deep learning-based sequence characterization. However, their efficacy is constrained by the intrinsic nature of deep learning (DL) models, which tend to exhibit suboptimal performance when applied to tabular data.
An alternative group of methodologies endeavors to convert biological sequences into image forms by applying the concept of Chaos Game Representation (CGR). However, a noteworthy drawback of these methods lies in their tendency to map individual elements of the sequence onto a relatively small subset of designated pixels within the generated image. The resulting sparse image representation may not adequately encapsulate the comprehensive sequence information, potentially resulting in suboptimal predictions.
In this study, we introduce a novel approach to transform biological sequences into images using the Bézier curve concept for element mapping. Mapping the elements onto a curve enhances the sequence information representation in the respective images, hence yielding better DL-based classification performance.  We employed three distinct protein sequence datasets to validate our system by doing three different classification tasks, and the results illustrate that our Bézier curve method is able to achieve good performance for all the tasks. 
For instance, it has shown tremendous improvement for a protein subcellular location prediction task over the baseline methods, such as improved accuracy by 39.4\% as compared to the FCGR baseline technique using a 2-layer CNN classifier. Moreover, for Coronavirus host classification, our Bézier method has achieved 5.3\% more AUC ROC score than the FCGR using a 3-layer CNN classifier.",25,https://openreview.net/forum?id=f6KkyweyYh,5.0,1.224744871391589,2.75,0.4330127018922193
evk6pPJqMy,Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs,"['Biomedical Knowledge Graph', 'Benchmark', 'Knowledge Base Integration']","[1, 3, 6, 5]","[3, 3, 5, 4]",0,"[46, 207, 226, 149]","Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and structures, enabling the utilization of emerging natural language processing methods and multi-modal data integration strategies. We evaluate KG representation models on Know2BIO, demonstrating its effectiveness as a benchmark for KG representation learning in the biomedical field. Data and source code of Know2BIO are available at https://anonymous.4open.science/r/Know2BIO/.",15,https://openreview.net/forum?id=evk6pPJqMy,3.75,1.920286436967152,3.75,0.82915619758885
eP6ZSy5uRj,Endowing Protein Language Models with Structural Knowledge,"['protein representation learning', 'protein language models', 'self-supervised learning', 'graph transformers']","[6, 5, 6, 5, 5]","[5, 4, 3, 5, 5]",0,"[402, 381, 198, 324, 577]","Protein language models have shown strong performance in predicting function and structure across diverse tasks. 
These models undergo unsupervised pretraining on vast sequence databases to generate rich protein representations, followed by finetuning with labeled data on specific downstream tasks.
The recent surge in computationally predicted protein structures opens new opportunities in protein representation learning.
In our study, we introduce a novel framework to enhance transformer protein language models specifically on protein structures.
Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules.
This refined model, termed the Protein Structure Transformer (PST), is further pretrained on a protein structure database such as AlphaFoldDB, using the same masked language modeling objective as traditional protein language models.
Our empirical findings show superior performance on several benchmark datasets. 
Notably, PST consistently outperforms the foundation model for protein sequences, ESM-2, upon which it is built. Our code and pretrained models will be released upon publication.",21,https://openreview.net/forum?id=eP6ZSy5uRj,5.4,0.48989794855663565,4.4,0.7999999999999999
dKPh4CLmYp,"Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs","['Aggregation', 'Information Theory', 'Sets', 'Graphs', 'Graph Neural Networks']","[3, 3, 5, 3, 6, 5, 5]","[2, 4, 4, 4, 2, 4, 3]",0,"[301, 255, 777, 181, 298, 931, 227]","Set-based learning is an essential component of modern deep learning and network science. Graph Neural Networks (GNNs) and their edge-free counterparts DeepSets (DS) have proven remarkably useful on ragged and topologically challenging datasets. The key to learning informative embeddings for set members is a specified aggregation function, usually a sum, max, or mean. We propose Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of data for both Bayesian inference and graph aggregation. We demonstrate that i) Fishnets neural summaries can be scaled optimally to an arbitrary number of data objects, ii) Fishnets aggregations are robust to changes in data distribution, unlike standard deepsets, iii) Fishnets saturate Bayesian information content and extend to regimes where MCMC techniques fail and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We show that by adopting a Fishnets aggregation scheme for message passing, GNNs can acheive state-of-the-art performance versus architecture size on ogbn-protein data over existing benchmarks with a fraction of learnable parameters and faster training time.",17,https://openreview.net/forum?id=dKPh4CLmYp,4.285714285714286,1.1605769149479943,3.2857142857142856,0.880630571852711
dFQL7nwksh,PDC-Net: Probability Density Cloud Representations of Proteins for Mutation Effect Prediction,"['Protein-protein interaction', 'Thermodynamics', 'Geometric Deep Learning']","[5, 3, 3, 6]","[2, 3, 3, 4]",1,"[274, 416, 862, 477]","Understanding the ramifications of mutations at a protein level can have significant implications in various domains such as drug development, disease pathways, and the broader field of genomics. Despite the promise of data-driven and deep learning (DL) strategies, existing algorithms still face a significant challenge in integrating the dynamic changes of biomolecules to accurately predict protein-protein interaction binding affinity changes following mutations ($\Delta \Delta G$). Within this study, we introduce an inventive approach aimed at capturing the equilibrium fluctuations and discerning induced conformational changes at the interface, which is particularly important for forecasting mutational effects on binding. This novel technique harnesses probability density clouds (PDC) to describe the magnitude and intensity of their movement during and after the binding process and puts forth aligned networks to propagate distributions of the equilibrium of molecular systems. To fully unleash the potential of PDC-Net, we further present two physics-inspired pretraining tasks to employ the molecular dynamics (MD) simulation trajectories and the extensive collection of static crystal protein structures. Experiments demonstrate that our approach surpasses the performance of both empirical energy functions and alternative DL methods.",4,https://openreview.net/forum?id=dFQL7nwksh,4.25,1.299038105676658,3.0,0.7071067811865476
c4JNoRRNtV,CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations,"['Neural Representation', 'Cryo-EM reconstruction', 'Structural Biology']","[3, 5, 5, 6]","[4, 5, 3, 2]",1,"[382, 566, 352, 170]","Cryo-electron microscopy (cryo-EM) allows for the high-resolution reconstruction of 3D structures of proteins and other biomolecules. Successful reconstruction of both shape and movement greatly helps understand the fundamental processes of life. However, it is still challenging to reconstruct the continuous motions of 3D structures from hundreds of thousands of noisy and randomly oriented 2D cryo-EM images. While recent advancements using Fourier domain coordinate-based neural networks show compelling results in modeling continuous 3D conformations, these methods often struggle to capture local flexible regions accurately. We propose CryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction. Our approach constructs an implicit feature volume directly in the real domain as the 3D representation. We also design a query-based deformation transformer decoder to effectively predict the density. Our approach is capable of refining pre-computed pose estimations and locating flexible regions. In experiments, our method outperforms current approaches on three public datasets (1 synthetic and 2 experimental) and a new synthetic dataset of PEDV spike protein. The code and new synthetic dataset will be released for better reproducibility of our results.",8,https://openreview.net/forum?id=c4JNoRRNtV,4.75,1.0897247358851685,3.5,1.118033988749895
bgIZDxd2bM,"Generation, Reconstruction, Representation All-in-One: A Joint Autoencoding Diffusion Model","['diffusion model', 'generative model', 'latent model']","[6, 3, 3]","[3, 4, 3]",1,"[287, 425, 400]","The vast applications of deep generative models are founded on the premise of three fundamental capabilities: generating new instances (e.g., image/text synthesis and molecule design), reconstructing inputs (e.g., data editing and restoration), and learning latent representations (e.g., structure discovery and downstream classification). Existing model families, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities but fall short in others. We introduce Joint Autoencoding Diffusion (JEDI), a new generative framework that unifies all three core capabilities, offering versatile applications and strong performance in a single model. Specifically, JEDI generalizes the noising/denoising transformations (based on simple Gaussian noise) in diffusion process by introducing parameterized encoder/decoder transformations between raw data and compact representations. Crucially, the encoder/decoder parameters are learned jointly with all other diffusion model parameters under the standard probabilistic diffusion formalism. This results in a model that not only inherits the strong generation abilities of diffusion models but also enables compact data representation and faithful reconstruction. Additionally, by choosing appropriate encoder/decoder, JEDI can naturally accommodate discrete data (such as text and protein sequences) which have been difficult for diffusion models. Extensive experiments across different data modalities, including images, text, and proteins, demonstrate JEDI's general applicability to diverse tasks and strong improvement over existing specialized deep generative models.",3,https://openreview.net/forum?id=bgIZDxd2bM,4.0,1.4142135623730951,3.3333333333333335,0.4714045207910317
bM6LUC2lec,MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions,"['Protein Language Model', 'Protein Structure Prediction']","[6, 6, 5]","[4, 3, 4]",0,"[381, 548, 445]","Deep learning, epitomized by models like AlphaFold2 \citep{jumper2021highly}, has achieved unparalleled accuracy in protein structure prediction. However, the depth of multiple sequence alignment (MSA) remains a bottleneck, especially for proteins lacking extensive homologous families. Addressing this, we present \METHODNAME{}, a self-supervised generative protein language model, pre-trained on a sequence\textbf{s}-to-sequence\textbf{s} task with an automatically constructed dataset. Equipped with protein-specific attention mechanisms, \METHODNAME{} harnesses large-scale protein databases to generate virtual, informative MSAs, enriching subpar MSAs and amplifying prediction accuracy. Our experiments with CASP14 and CASP15 benchmarks showcase marked LDDT improvements, especially for challenging sequences, enhancing both AlphaFold2 and RoseTTAFold's performance.",8,https://openreview.net/forum?id=bM6LUC2lec,5.666666666666667,0.4714045207910317,3.6666666666666665,0.4714045207910317
ZlEtXIxl3q,Contrastive losses as generalized models of global epistasis,"['Computational Biology', 'Contrastive losses', 'Protein engineering']","[3, 6, 6, 3]","[4, 3, 2, 4]",0,"[774, 336, 164, 427]","Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrastive losses are able to accurately estimate a ranking function from limited data even in regimes where MSE is ineffective. We validate the practical utility of this insight by showing contrastive loss functions result in consistently improved performance on benchmark tasks.",15,https://openreview.net/forum?id=ZlEtXIxl3q,4.5,1.5,3.25,0.82915619758885
UfBIxpTK10,The Discovery of Binding Modes Requires Rethinking Docking Generalization,"['generalization', 'molecular docking', 'protein-ligand binding', 'diffusion models', 'benchmark', 'bootstrapping', 'self-training']","[6, 5, 5, 8, 6]","[2, 4, 5, 3, 4]",0,"[66, 389, 293, 1039, 780]","Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, it is critical that docking methods generalize well across the proteome. However, existing benchmarks fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that machine learning-based docking models have very weak generalization abilities even when combined with various data augmentation strategies. Instead, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between a diffusion and a confidence model. Unlike previous self-training methods from other domains, we directly exploit the multi-resolution generation process of diffusion models using rollouts and confidence scores to reduce the generalization gap. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.",15,https://openreview.net/forum?id=UfBIxpTK10,6.0,1.0954451150103321,3.6,1.019803902718557
Tlsdsb6l9n,Mol-Instructions - A Large-Scale Biomolecular Instruction Dataset for Large Language Models,"['instruction dataset', 'large language models', 'biomolecular studies', 'molecule', 'protein']","[6, 8, 8, 6]","[3, 4, 3, 4]",0,"[414, 185, 255, 747]","Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",16,https://openreview.net/forum?id=Tlsdsb6l9n,7.0,1.0,3.5,0.5
SmZD7yxpPC,GlycoNMR: A Carbohydrate-Specific NMR Chemical Shift Dataset for Machine Learning Research,"['AI for science', 'Glycoscience', 'Graph Neural Network', 'Nuclear Magnetic Resonance']","[5, 6, 6]","[2, 3, 4]",0,"[266, 319, 423]","Molecular representation learning (MRL) is a powerful contribution by machine learning to chemistry as it converts molecules into numerical representations, which serves as fundamental for diverse biochemical applications, such as property prediction and drug design. While MRL has had great success with proteins and general biomolecules, it has yet to be explored for carbohydrates in the growing fields of glycoscience and glycomaterials (the study and design of carbohydrates). This under-exploration can be primarily attributed to the limited availability of comprehensive and well-curated carbohydrate-specific datasets and a lack of machine learning (ML) techniques tailored to meet the unique problems presented by carbohydrate data. Interpreting and annotating carbohydrate data is generally more complicated than protein data, and requires substantial domain knowledge. In addition, existing MRL methods were predominately optimized for proteins and small biomolecules, and may not be effective for carbohydrate applications without special modifications. To address this challenge, accelerate progress in glycoscience and glycomaterials, and enrich the data resources of the ML community, we introduce GlycoNMR. GlycoNMR contains two laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotated nuclear magnetic resonance (NMR) atomic-level chemical shifts that can be used to train ML models for precise atomic-level prediction. NMR data is one of the most appealing starting points for developing ML techniques to facilitate glycoscience and glycomaterials research, as NMR is the preeminent technique in carbohydrate structure research, and biomolecule structure is among the foremost predictors of functions and properties. We tailored a set of carbohydrate-specific features and adapted existing MRL models to effectively tackle the problem of predicting NMR shifts. For illustration, we benchmark these modified MRL models on the GlycoNMR.",13,https://openreview.net/forum?id=SmZD7yxpPC,5.666666666666667,0.4714045207910317,3.0,0.816496580927726
SjgfWbamtN,"MiniFold: Simple, Fast and Accurate Protein Structure Prediction","['protein', 'structure prediction', 'efficiency', 'hardware-optimization']","[3, 5, 6, 3]","[5, 3, 4, 4]",0,"[775, 178, 456, 349]","Protein structure prediction has emerged as a powerful tool for biologists and drug makers. However, the computational toll associated with state-of-the-art models, such as AlphaFold2 or ESMFold, hinders their use in large-scale applications like virtual screening or mutational scanning, where a single experiment may involve processing millions of protein sequences. In an effort to develop a more efficient model, we aimed to understand which of the complex architectural choices proposed in AlphaFold2 were essential to achieve high performance, and which could be omitted without significantly compromising accuracy. This analysis culminated in a simple, yet highly expressive architecture for protein structure prediction. Our model, MiniFold, consists of a minimal Evoformer variant, a parameter-free coordinate recovery algorithm, and a custom hardware-optimized implementation composed of newly designed GPU kernels. When compared against ESMFold, MiniFold achieves over 100x speedup and shows improved scalability to long protein sequences while conserving over 95% of the original performance, making it a promising candidate for large-scale applications.",12,https://openreview.net/forum?id=SjgfWbamtN,4.25,1.299038105676658,4.0,0.7071067811865476
SFCHv2G33F,Protein Language Models Enable Accurate Cryptic Ligand Binding Pocket Prediction,"['Protein Language Models', 'Protein Binding Pockets', 'Protein Binding Sites', 'Cryptic Protein Binding Pockets']","[3, 3, 3, 5]","[5, 3, 3, 4]",0,"[666, 185, 162, 519]","Accurate prediction of protein-ligand binding pockets is a critical task in protein functional analysis and small molecule pharmaceutical design.  However, the flexible and dynamic nature of proteins conceal an unknown number of potentially invaluable ""cryptic"" pockets.  Current approaches for cryptic pocket discovery rely on molecular dynamics (MD), leading to poor scalability and bias.  Even recent ML-based cryptic pocket discovery approaches require large, post-processed MD datasets to train their models.  In contrast, this work presents ``Efficient Sequence-based cryptic Pocket prediction'' (ESP) leveraging advanced Protein Language Models (PLMs), and demonstrates significant improvement in predictive efficacy compared to ML-based cryptic pocket prediction SOTA (ROCAUC 0.93 vs 0.87).  ESP achieves detection of cryptic pockets via training on readily available, non cryptic-pocket-specific data from the PDBBind dataset, rather than costly simulation and post-processing.  Further, while SOTA's predictions often include positive signal broadly distributed over a target structure, ESP produces more spatially-focused predictions which increase downstream utility.",9,https://openreview.net/forum?id=SFCHv2G33F,3.5,0.8660254037844386,3.75,0.82915619758885
S4zpk61r6G,DiffMaSIF: Score-Based Diffusion Models for Protein Surfaces,"['geometric deep learning', 'protein-protein docking', 'diffusion model', 'equivariant network', 'protein surface']","[3, 6, 5]","[4, 4, 5]",0,"[1160, 395, 359]","Predicting protein-protein complexes is one of the central challenges of computational structural biology. Inspired by recent generative machine learning (ML) techniques which have shown promise in the realms of protein docking, we introduce DiffMaSIF, a novel score-based diffusion model for rigid protein-protein docking. While existing methods rely on co-evolution learned on a residue level, this information is not sufficient for transient, weakly evolved, or newly designed interfaces. DiffMaSIF’s efficacy hinges on its surface-based molecular representation which can capture the complementarity inherent in the physical surfaces of interacting protein interfaces. We follow an end-to-end two-tier prediction schema: initially identifying contact sites on the protein surface and constraining each molecular graph to these sites, followed by an equivariant network to position the two proteins. This data reduction step enables the use of more sophisticated networks and more training steps. In addition to developing this model, we introduce new dataset splits accounting for structural leakage at the interface and thus tailored for benchmarking protein-protein interface prediction performance. Our results demonstrate that DiffMaSIF not only outperforms contemporary ML methods in rigid protein docking, but also matches traditional docking tools at considerably fewer numbers of generated decoys. Through DiffMaSIF, we pave the way for surface-centric interface prediction methods, thus advancing accurate prediction of protein interactions across a wide spectrum of difficult and novelty.",3,https://openreview.net/forum?id=S4zpk61r6G,4.666666666666667,1.247219128924647,4.333333333333333,0.4714045207910317
RemfXx7ebP,Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design,['Bioinformatics'],"[3, 3, 6]","[5, 4, 4]",0,"[367, 969, 824]","While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset will be released publicly.",9,https://openreview.net/forum?id=RemfXx7ebP,4.0,1.4142135623730951,4.333333333333333,0.4714045207910317
RY0yYsTxGM,Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interaction Prediction,"['compound-protein interaction', 'graph transformer', 'hierarchy graph', 'biomolecule affinity prediction']","[1, 3, 5, 3]","[4, 4, 5, 4]",1,"[588, 228, 528, 557]","Predicting compound-protein interactions (CPIs) is critical for AI-aided drug design. Recent deep learning (DL) methods have successfully modeled molecular interactions at the atomic level, achieving both efficiency and accuracy improvements compared to traditional energy-based methods. However, these models do not always align with the chemical realities of CPIs, as molecular fragments (i.e., motifs) often participate in the interactions dominantly. In this paper, we aim to fill this gap by considering the role of motifs for CPIs. We propose a pair-wise hierarchical interaction representation learning (Phi-former) method. Phi-former represents the compound or protein hierarchically and employs a pair-wise specific pre-training framework for modeling the interactions in a more systematic way~(i.e., atom-atom, motif-motif, and atom-motif). We propose an intra-level and inter-level Phi-former pipeline for learning the pair-wise biomolecular graph representation, making learning the different interaction levels mutually beneficial. We demonstrate that Phi-former can achieve superior performance on CPI-related tasks. Furthermore, a case study indicates that our method can accurately identify the specific atoms or motifs activated in CPIs, and thus provide good model explanations that may give insights into molecular structural optimization.",4,https://openreview.net/forum?id=RY0yYsTxGM,3.0,1.4142135623730951,4.25,0.4330127018922193
RS827PjAUs,InstructProtein: Aligning Human and Protein Language via Knowledge Instruction,['large language model; insturction tuning; knowledge graph; protein;'],"[6, 5, 3, 6]","[3, 4, 4, 4]",1,"[542, 394, 518, 364]","Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.",12,https://openreview.net/forum?id=RS827PjAUs,5.0,1.224744871391589,3.75,0.4330127018922193
RBs0IfPj5e,Backdiff: a diffusion model for generalized transferable protein backmapping,"['computational biology', 'protein conformation generation', 'generative model', 'score-based diffusion model']","[3, 6, 6]","[4, 4, 4]",0,"[303, 505, 330]","Coarse-grained (CG) models play a crucial role in the study of protein structures, protein thermodynamic properties and protein conformation dynamics. Due to the information loss in the coarse-graining process, backmapping from CG to all-atom configuations is essential in many protein design and drug discovery applications when detailed atomic representations are needed for in-depth studies. Despite recent progress in data-driven backmapping approaches, devising a backmapping method that can be universally applied across various CG models and proteins remains unresolved. In this work, we propose BackDiff, a new generative model designed to achieve generalization and reliability in the  protein backmapping problem. BackDiff leverages the conditional score-based diffusion model with geometric representations. Since different CG models can contain different coarse-grained sites which include selected atoms (CG atoms) and simple auxiliary functions of atomistic coordinates (auxiliary variables), we design a self-supervised training framework to adapt to different CG atoms, and constrain the diffusion sampling paths with arbitrary auxiliary variables as conditions. Our method facilitates end-to-end training and allows efficient sampling across different proteins and diverse CG models without the need for retraining. Comprehensive experiments over multiple popular CG models demonstrate BackDiff's superior performance to existing state-of-the-art approaches, and generalization and flexibility that these approaches cannot achieve. A pretrained BackDiff model can offer a convenient yet reliable plug-and-play solution for protein researchers, enabling them to investigate further from their own CG models.",7,https://openreview.net/forum?id=RBs0IfPj5e,5.0,1.4142135623730951,4.0,0.0
OVPoEhbsDm,Thermodynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling,"['Protein-protein Interaction', 'Mutation Effect Prediction']","[5, 6, 3]","[4, 4, 4]",0,"[952, 248, 318]","Modeling protein-protein interactions (PPI) represents a central challenge within the field of biology, and accurately predicting the consequences of mutations in this context is crucial for various applications, such as drug design and protein engineering. Recent advances in deep learning (DL) have shown promise in forecasting the effects of such mutations. However, the effectiveness of these models is hindered by two primary constraints. First and foremost, obtaining the structures of mutant proteins is a persistent challenge, as they are often elusive to acquire. Secondly, interactions take place dynamically, but thermodynamics is rarely integrated into the DL architecture design. To address these obstacles, we present a novel framework known as Refine-PPI, which incorporates two key enhancements. On the one hand, we introduce a structure refinement module that is trained by a mask mutation modeling (MMM) task on available wide-type structures and then is transferred to hallucinate the inaccessible mutant protein structures. Additionally, we employ a new kind of geometric networks to capture the dynamic 3D variations and encode the uncertainty associated with PPI. Through comprehensive experiments conducted on the established benchmark dataset SKEMPI, our results substantiate the superiority of the Refine-PPI framework. These findings underscore the effectiveness of our hallucination strategy to address the absence of mutant protein structure and hope to shed light on the prediction of the free energy change.",21,https://openreview.net/forum?id=OVPoEhbsDm,4.666666666666667,1.247219128924647,4.0,0.0
OHpvivXrQr,Protein Multimer Structure Prediction via PPI-guided Prompt Learning,"['docking path prediction', 'protein complex structure', 'prompt learning']","[8, 5, 5, 3]","[3, 3, 3, 4]",0,"[315, 745, 347, 316]","Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction (MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions (PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a poor generalization to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales (i.e., chain numbers). Specifically, we propose PromptMSP, a pre-training and Prompt tuning framework for Multimer Structure Prediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We utilize the meta-learning approach to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. For instance, when both methods utilize AlphaFold-Multimer to prepare dimers, PromptMSP achieves a 21.43\% improvement in TM-Score with only 0.5\% of the running time compared to the competitive MoLPC baseline.",20,https://openreview.net/forum?id=OHpvivXrQr,5.25,1.7853571071357126,3.25,0.4330127018922193
O0dW800ukz,"Multimodal Distillation of Protein Sequence, Structure, and Function","['protein sequence', 'structure', 'function', 'knowledge distillation', 'representation learning']","[6, 5, 6]","[4, 4, 5]",0,"[623, 323, 1089]","Proteins are the fundamental building blocks of life, carrying out essential biological functions in biology. Learning effective representations of proteins is critical for important applications like drug design and function prediction. Language models (LMs) and graph neural networks (GNNs) have shown promising performance for modeling proteins. However, multiple data modalities exist for proteins, including sequence, structure, and functional annotations. Frameworks integrating these diverse sources without large-scale pre-training remain underdeveloped. In this work, we propose ProteinSSA, a multimodal knowledge distillation framework to incorporate {\bf Protein} {\bf S}equence, {\bf S}tructure, and Gene Ontology (GO) {\bf A}nnotation for unified representations. Our approach trains a teacher and student model connected via distillation. The student GNN encodes protein sequences and structures, while the teacher model leverages GNN and an auxiliary GO encoder to incorporate the functional knowledge, generating hybrid multimodal embeddings passed to the student to learn the function-enriched representations by distribution approximation. Experiments on tasks like protein fold and enzyme commission (EC) prediction show that ProteinSSA significantly outperforms state-of-the-art baselines, demonstrating the benefits of our multimodal framework.",27,https://openreview.net/forum?id=O0dW800ukz,5.666666666666667,0.4714045207910317,4.333333333333333,0.4714045207910317
MamHzZHs0h,SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design,"['in-context learning', 'drug synergy prediction', 'inverse drug design', 'precision medicine']","[5, 3, 5, 3]","[5, 5, 2, 4]",0,"[380, 498, 482, 428]","Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small ""personalized dataset"" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to ""in-context learn"" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn ""drug synergy functions"". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient's ""personalized dataset"". Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.",4,https://openreview.net/forum?id=MamHzZHs0h,4.0,1.0,4.0,1.224744871391589
MNwXif6AWA,Periodic Set Transformer: Material Property Prediction from Continuous Isometry Invariants,['Material Property Prediction'],"[5, 3, 3, 6]","[4, 4, 5, 3]",0,"[727, 752, 359, 514]","Material or crystal property prediction using machine learning has grown popular in recent years as it provides an accurate and computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a fixed number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and incorporate compositional information via a spatial encoding method. This model is tested thoroughly with and without the use of compositional information on a variety of crystal datasets including the commonly used crystals of the Materials Project.",20,https://openreview.net/forum?id=MNwXif6AWA,4.25,1.299038105676658,4.0,0.7071067811865476
MBIGXMT0qC,Multi-Scale Protein Language Model for Unified Molecular Modeling,"['Protein Pre-training', 'Unified Molecular Modeling']","[3, 6, 6, 5]","[5, 3, 3, 4]",0,"[627, 367, 112, 315]","Protein language models have shown great potential in protein engineering. However, the current protein language models mainly work in the residue scale, which cannot offer information in the atom scale. The strong power of protein language models could not be fully exploited to benefit the applications that cross protein and small molecules. In this paper, we propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding. Experimental results show that msESM outperforms previous methods in protein-molecule tasks and is on par with the state-of-the-art in protein-only and molecule-only tasks.",38,https://openreview.net/forum?id=MBIGXMT0qC,5.0,1.224744871391589,3.75,0.82915619758885
K3tHTPjFBM,Equivariant Protein Multi-task Learning,"['Equivariant neural network', 'Multitask learning', 'Heterogeneous graph']","[5, 5, 3, 3]","[4, 3, 3, 3]",0,"[407, 421, 369, 339]","Understanding and leveraging the 3D structures of proteins is central to various tasks in biology and drug discovery. While deep learning has been applied successfully for modeling protein structures, current methods usually employ distinct models for different tasks. Such a single-task strategy is not only resource-consuming when the number of tasks increases but also incapable of combining multi-source datasets for larger-scale model training, given that protein datasets are usually of small size for most structural tasks. In this paper, we propose to adopt one single model to address multiple tasks jointly, upon the input of 3D protein structures. In particular, we first construct a standard multi-task benchmark called PROMPT, consisting of 6 representative tasks integrated from 4 public datasets. The resulting benchmark contains partially labeled data for training and fully-labeled data for validation/testing. Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is equivariant to 3D rotations/translations/reflections of proteins and able to capture various relationships between different atoms owing to the heterogeneous multichannel graph construction of proteins. Besides, HeMeNet is able to achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations verify the effectiveness of multi-task learning on our benchmark, and our model generally surpasses state-of-the-art models. Our study is expected to open up a new venue for structure-based protein learning.",10,https://openreview.net/forum?id=K3tHTPjFBM,4.0,1.0,3.25,0.4330127018922193
J4V3lW9hq6,A Multi-Grained Group Symmetric Framework for Learning Protein-Ligand Binding Dynamics,"['protein-ligand binding', 'group symmetric', 'multi-grained', 'molecular simulation', 'Newtonian dynamics']","[3, 5, 6, 6]","[4, 3, 3, 5]",0,"[704, 566, 221, 200]","In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural ordinary differential equation solver that learns the trajectory under Newtonian mechanics. For the experiment, we design ten single-trajectory and three multi-trajectory binding simulation tasks. We show the efficiency and effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical MD simulation and outperforming all other ML approaches by up to ~80\% under the stability metric. We further qualitatively show that NeuralMD reaches more stable binding predictions.",21,https://openreview.net/forum?id=J4V3lW9hq6,5.0,1.224744871391589,3.75,0.82915619758885
IWfqn6Ythj,Clustering for Protein Representation Learning,"['Protein', 'Amino Acids', 'Protein Representation Learning', 'Clustering']","[1, 3, 8, 3]","[4, 4, 2, 3]",1,"[864, 343, 191, 526]","Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate our framework on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance. Our code will be released.",4,https://openreview.net/forum?id=IWfqn6Ythj,3.75,2.5860201081971503,3.25,0.82915619758885
ILtA2ebLYR,Efficient Interactive Preference Learning in Evolutionary Algorithms: Active Dueling Bandits and Active Learning Integration,"['Preference learning', 'multi-objective optimization', 'interactive multi-objective optimization', 'active learning']","[3, 3, 3, 3]","[3, 4, 3, 3]",0,"[697, 385, 291, 447]","Optimization problems find widespread use in both single-objective and multi-objective scenarios. In practical applications, users aspire for solutions that converge to the region of interest (ROI) along the Pareto front (PF). While the conventional approach involves approximating a fitness function or an objective function to reflect user preferences, this paper explores an alternative avenue. Specifically, we aim to discover a method that sidesteps the need for calculating the fitness function, relying solely on human guidance. Our proposed approach entails conducting a human-dominated search facilitated by an active dueling bandit algorithm.
    The experimental phase is structured into three sessions. Firstly, we accsess the performance of our active dueling bandit algorithm. Secondly, we implement our proposed method within the context of multi-objective Evolutionary Algorithms (EAs). Finally, we deploy our method in a practical problem, specifically in protein structure prediction (PSP). 
    This research presents a novel interactive preference-based EA framework that not only addresses the limitations of traditional techniques but also unveils new possibilities for optimization problems.",9,https://openreview.net/forum?id=ILtA2ebLYR,3.0,0.0,3.25,0.4330127018922193
HiYMiZYwkw,Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning,"['self-supervised learning', 'domain-agnostic learning', 'masked modeling', 'protein biology', 'chemistry', 'particle physics']","[6, 6, 6, 6, 6]","[3, 3, 4, 3, 2]",0,"[500, 306, 472, 313, 679]","Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",30,https://openreview.net/forum?id=HiYMiZYwkw,6.0,0.0,3.0,0.6324555320336759
GsNp4ob8BY,Mark My Words: Repurposing LLMs for Specialized Domains via Ability Tokens,"['LLM Adaptation', 'Specialized Domains']","[5, 5, 5, 6]","[4, 4, 4, 3]",0,"[395, 255, 310, 375]","Large Language Models (LLMs) have demonstrated remarkable proficiency in natural language understanding and generation. However, their capabilities wane in highly specialized domains, such as biomedical sciences, which are sparsely represented in the pretraining corpus. In this work, we explore how to repurpose general LMs as specialized task solvers. We introduce a novel and systematic framework for adding markup-style language extensions (which we term *`ability tokens""*) to pretrained LMs. These tokens are learned embeddings appended to the LM's embedding matrix, preserving the pretrained weights and the model's original capabilities. We introduce two types of ability tokens: *domain markers*, which delimit and aid in the processing of specialized inputs (e.g., molecular formulas), and *functional tokens*, which guide the model on how to leverage these inputs to solve specific tasks (e.g., predicting molecule properties). During inference, these tokens are inserted into the input text to wrap specialized information and provide problem context. Experimental results show that (i) our markup extensions significantly boost performance in various specialized domains, such as protein and molecular property prediction, matching and outperforming expert models specifically tailored to these tasks, and (ii) we can learn the ability tokens separately and combine them in a modular fashion, achieving zero-shot generalization to  unseen tasks. Overall, our framework offers a promising method to enhance LMs with domain-specific knowledge while maintaining their general capacities.",12,https://openreview.net/forum?id=GsNp4ob8BY,5.25,0.4330127018922193,3.75,0.4330127018922193
GKxmmAwxj1,Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules,"['normalizing flow', 'protein conformations', 'boltzmann generators', 'generative models']","[6, 6, 5, 6]","[4, 3, 4, 3]",0,"[884, 517, 285, 250]","The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35.",18,https://openreview.net/forum?id=GKxmmAwxj1,5.75,0.4330127018922193,3.5,0.5
FZfWQFrdBT,Split and Merge Proxy: pre-training protein inter-chain contact prediction by mining rich information from monomer data,"['Protein Bioinformatics', 'Protein Inter-chain Contact Prediction', 'Pre-training']","[6, 5, 6, 5]","[3, 3, 4, 4]",0,"[249, 385, 627, 255]","Protein inter-chain contact prediction is a key intelligent biology computation technology for protein multimer function analysis but still suffers from low accuracy. An important problem is that the number of training data cannot meet the requirements of deep-learning-based methods due to the expensive cost of capturing structure information of multimer data. In this paper, we solve this data volume bottleneck in a cheap way, borrowing rich information from monomer data. To utilize monomer (single chain) data in this multimer (multiple chains) problem, we propose a simple but effective pre-training method called Split and Merge Proxy (SMP), which utilizes monomer data to construct a proxy task for model pre-training. This proxy task cuts monomer data into two sub-parts, called pseudo multimer, and pre-trains the model to merge them back together by predicting their pseudo contacts. The pre-trained model is then used to initialize our target -- protein inter-chain contact prediction. Because of the consistency between this proxy task and the final target, the whole method brings a stronger pre-trained model for subsequent fine-tuning, leading to significant performance gains. Extensive experiments validate the effectiveness of our method and show the model performs better than the state-of-the-art (SOTA) method by 11.40\% and 2.97\% on the P@ $L/10$ metric for bounded benchmarks DIPS-Plus and CASP-CAPRI, respectively. Further, the model also achieves almost 1.5 times performance superiority to the SOTA approach on the harder unbounded benchmark DB5. Finally, we also effectively apply our SMP on docking and interaction site prediction tasks to verify the SMP is a general method for other multimer-related tasks. The code, model, and pre-training data will be released after this paper is accepted.",12,https://openreview.net/forum?id=FZfWQFrdBT,5.5,0.5,3.5,0.5
FWsGuAFn3n,Prompt-based 3D Molecular Diffusion Models for  Structure-based Drug Design,"['Diffusion Model', 'Structure-based Drug Design', 'Molecule Generation']","[3, 3, 6, 3]","[4, 4, 3, 4]",0,"[626, 325, 692, 179]","Generating ligand molecules that bind to specific protein targets via generative models holds substantial promise for advancing structure-based drug design. Existing methods generate molecules from scratch without reference or template ligands, which poses challenges in model optimization and may yield suboptimal outcomes. To address this problem, we propose an innovative prompt-based 3D molecular diffusion model named PromptDiff to facilitate target-aware molecule generation. PromptDiff leverages a curated set of ligand prompts, i.e., those with desired properties such as high binding affinity and synthesizability, to steer the diffusion model towards synthesizing ligands that satisfy design criteria. Specifically, we design a geometric protein-molecule interaction network (PMINet), and pretrain it with binding affinity signals to: (i) retrieve target-aware ligand molecules with high binding affinity to serve as prompts, and (ii) incorporate essential protein-ligand binding structures for steering molecular diffusion generation with two effective prompting mechanisms, i.e., exemplar prompting and self prompting. Empirical studies on CrossDocked2020 dataset show PromptDiff can generate molecules with more realistic 3D structures and achieve state-of-the-art binding affinities towards the protein targets, while maintaining proper molecular properties.",4,https://openreview.net/forum?id=FWsGuAFn3n,3.75,1.299038105676658,3.75,0.4330127018922193
Dr4qD9bzZd,Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design,"['metalloprotein design', 'protein sequence and structure co-design', 'functional protein design']","[6, 3, 3]","[5, 5, 4]",0,"[1181, 242, 674]","Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capability of our model to design protein sequences and structures that closely resemble their natural counterparts. Furthermore, in-depth analysis further confirms our model's ability to generate highly effective proteins capable of binding to their target metallocofactors.",31,https://openreview.net/forum?id=Dr4qD9bzZd,4.0,1.4142135623730951,4.666666666666667,0.4714045207910317
DkhYlWZq84,Protein Captioning: Bridging the Gap between Protein Sequences and Natural Languages,"['natural language processing', 'protein representation learning', 'protein captioning']","[6, 3, 3, 6]","[4, 3, 5, 4]",0,"[876, 573, 316, 201]","We introduce the task of Protein Captioning, which is an easy-to-understand and flexible way for protein analysis. Compared to specific protein recognition or classification tasks, such as enzyme reaction classification and gene ontology term prediction, protein captioning provides comprehensive textural descriptions for proteins, thus playing a key role in bridging the gap between protein sequences and natural languages. To address the problem, we propose a simple yet effective method, Protein-to-Text Generative Pre-trained Transformer (P2T-GPT), to translate the chain of amino acid residues in a protein to a sequence of natural language words, i.e., text. For the evaluation of protein captioning, we collect a ProteinCap dataset that contains 94,454 protein-text pairs. Experiments on  ProteinCap demonstrate the effectiveness of the proposed P2T-GPT on protein captioning. As minor contributions, first, P2T-GPT provides a way to connect protein science and Large Language Models (LLMs). By appending ChatGPT, our method can interact in a conversational way to answer questions given a protein. Second, we show that protein captioning can be treated as a pre-trained task that can benefit a range of downstream tasks, to a certain extent. The code has been submitted in the supplementary material and will be publicly available.",4,https://openreview.net/forum?id=DkhYlWZq84,4.5,1.5,4.0,0.7071067811865476
Dc4rXq3HIA,Improving Out-of-Domain Generalization with Domain Relations,['Out-of-Domain Generalization; Domain Relations; Distribution Shift'],"[6, 6, 6, 8, 8, 6]","[4, 3, 3, 4, 4, 4]",0,"[326, 250, 257, 235, 348, 395]","Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. In this paper, we focus on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called DG. Unlike previous approaches that aim to learn a single model that is domain invariant, DG leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, DG learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stronger out-of-domain generalization compared to the conventional averaging approach. Empirically, we evaluate the effectiveness of DG using both toy and real-world datasets for tasks such as temperature regression, land use classification, and molecule-protein binding affinity prediction. Our results show that DG consistently outperforms state-of-the-art methods.",25,https://openreview.net/forum?id=Dc4rXq3HIA,6.666666666666667,0.9428090415820634,3.6666666666666665,0.4714045207910317
DP4NkPZOpD,Bridging Sequence and Structure: Latent Diffusion for Conditional Protein Generation,"['Protein Design', 'Geometric Machine Learning', 'Latent Diffusion', 'Protein Docking']","[3, 3, 6, 8]","[5, 5, 4, 3]",0,"[326, 918, 353, 277]","Protein design encompasses a range of challenging tasks, including protein folding, inverse folding, and protein-protein docking. Despite significant progress in this domain, many existing methods address these tasks separately, failing to adequately leverage the joint relationship between protein sequence and three-dimensional structure. In this work, we propose a novel generative modeling technique to capture this joint distribution. Our approach is based on a diffusion model applied on a geometrically-structured latent space, obtained through an encoder that produces roto-translational invariant representations of the input protein complex. It can be used for any of the aforementioned tasks by using the diffusion model to sample the conditional distribution of interest. Our experiments show that our method outperforms competitors in protein docking and is competitive with state-of-the-art for protein inverse folding. Exhibiting a single model that excels on on both sequence-based and structure-based tasks represents a significant advancement in the field and paves the way for additional applications.",10,https://openreview.net/forum?id=DP4NkPZOpD,5.0,2.1213203435596424,4.25,0.82915619758885
C5u71ph75Q,Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters,"['Machine Learning', 'Structural Biology', 'Internal-Coordinate Density Modelling', 'Generative Modelling']","[6, 6, 5]","[3, 3, 3]",0,"[1622, 344, 349]","After the recent ground-breaking advances in protein structure prediction, one of the remaining challenges in protein machine learning is to reliably predict distributions of structural states. Parametric models of fluctuations are difficult to fit due to complex covariance structures between degrees of freedom in the protein chain, often causing models to either violate local or global structural constraints. In this paper, we present a new strategy for modelling protein densities in internal coordinates, which uses constraints in 3D space to induce covariance structure between the internal degrees of freedom. We illustrate the potential of the procedure by constructing a variational autoencoder with full covariance output induced by the constraints implied by the conditional mean in 3D, and demonstrate that our approach makes it possible to scale density models of internal coordinates to full protein backbones in two settings: 1) a unimodal setting for proteins exhibiting small fluctuations and limited amounts of available data, and 2) a multimodal setting for larger conformational changes in a high data regime.",14,https://openreview.net/forum?id=C5u71ph75Q,5.666666666666667,0.4714045207910317,3.0,0.0
C4BikKsgmK,Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling,"['proteins', 'conformational sampling', 'diffusion models', 'score-based models', 'generative modeling', 'equivariant network']","[6, 6, 6, 6]","[3, 4, 3, 3]",0,"[474, 404, 339, 773]","The dynamic nature of proteins is crucial for determining their biological functions and properties, for which Monte Carlo (MC) and molecular dynamics (MD) simulations stand as predominant tools to study such phenomena. By utilizing empirically derived force fields, MC or MD simulations explore the conformational space through numerically evolving the system via Markov chain or Newtonian mechanics. However, the high-energy barrier of the force fields can hamper the exploration of both methods by the rare event, resulting in inadequately sampled ensemble without exhaustive running. Existing learning-based approaches perform direct sampling yet heavily rely on target-specific simulation data for training, which suffers from high data acquisition cost and poor generalizability. Inspired by simulated annealing, we propose Str2Str, a novel structure-to-structure translation framework capable of zero-shot conformation sampling with roto-translation equivariant property. Our method leverages an amortized denoising score matching objective trained on general crystal structures and has no reliance on simulation data during both training and inference. Experimental results across several benchmarking protein systems demonstrate that Str2Str outperforms previous state-of-the-art generative structure prediction models and can be orders of magnitude faster compared with long MD simulations.",21,https://openreview.net/forum?id=C4BikKsgmK,6.0,0.0,3.25,0.4330127018922193
BxcEqwl9es,Microenvironment Probability Flows as Proficient Protein Engineers,['inverse folding; probabilistic flow'],"[6, 5, 5, 5]","[3, 3, 3, 4]",0,"[163, 354, 622, 369]","The inverse folding of proteins has tremendous applications in protein design and protein engineering. While machine learning approaches for inverse folding have made significant advancements in recent years, efficient generation of diverse and high-quality sequences remains a significant challenge, limiting their practical utility in protein design and engineering. We propose to do probabilistic flow framework that introduces three key designs for designing an amino acid sequence with target fold. At the input level, compare to existing inverse folding methods, rather than sampling sequences from the backbone scaffold, we demonstrate that analyzing a protein structure via the local chemical environment (micro-environment) at each residue can come to comparable performance. At the method level, rather than optimizing the recovery ratio, we generate diverse suggestions.  3) At the data level, during training, we propose to do data augmentation with sequence with high sequence similarity, and train a probability flow model to capture the diverse sequence information. 
We demonstrate that we achieve comparable recovery ratio as the SOTA inverse folding models with higher inference efficiency and flexibility by only using micro-environment as inputs, and further show that we outperforms existing inverse folding methods in several zero-shot thermal stability change prediction tasks.",12,https://openreview.net/forum?id=BxcEqwl9es,5.25,0.4330127018922193,3.25,0.4330127018922193
BIveOmD1Nh,Learning Scalar Fields for Molecular Docking with Fast Fourier Transforms,"['protein structure', 'structural biology', 'drug discovery', 'molecular docking']","[8, 5, 6]","[4, 3, 3]",0,"[226, 122, 512]","Molecular docking is critical to structure-based virtual screening, yet the throughput of such workflows is limited by the expensive optimization of scoring functions involved in most docking algorithms. We explore how machine learning can accelerate this process by learning a scoring function with a functional form that allows for more rapid optimization. Specifically, we define the scoring function to be the cross-correlation of multi-channel ligand and protein scalar fields parameterized by equivariant graph neural networks, enabling rapid optimization over rigid-body degrees of freedom with fast Fourier transforms. Moreover, the runtime of our approach can be amortized at several levels of abstraction, and is particularly favorable for virtual screening settings with a common binding pocket. We benchmark our scoring functions on two simplified docking-related tasks: decoy pose scoring and rigid conformer docking. Our method attains similar but faster performance on crystal structures compared to the Vina and Gnina scoring functions, and is more robust on computationally predicted structures.",12,https://openreview.net/forum?id=BIveOmD1Nh,6.333333333333333,1.247219128924647,3.3333333333333335,0.4714045207910317
BEH4mGo7zP,"Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning","['Protein representation learning', 'self-supervised learning', 'implicit neural representation']","[5, 6, 6, 6]","[5, 3, 2, 4]",0,"[643, 624, 232, 256]","Proteins can be represented in various ways, including their sequences, 3D structures, and surfaces. While recent studies have successfully employed sequence- or structure-based representations to address multiple tasks in protein science, there has been significant oversight in incorporating protein surface information, a critical factor for protein function. In this paper, we present a pre-training strategy that incorporates information from protein sequences, 3D structures, and surfaces to improve protein representation learning. Specifically, we utilize Implicit Neural Representations (INRs) for learning surface characteristics, and name it ProteinINR. We confirm that ProteinINR successfully reconstructs protein surfaces, and integrate this surface learning into the existing pre-training strategy of sequences and structures. Our results demonstrate that our approach can enhance performance in various downstream tasks, thereby underscoring the importance of including surface attributes in protein representation learning. These findings underline the importance of understanding protein surfaces for generating effective protein representations.",16,https://openreview.net/forum?id=BEH4mGo7zP,5.75,0.4330127018922193,3.5,1.118033988749895
AXbN2qMNiW,Protein-ligand binding representation learning from fine-grained interactions,"['Protein-ligand binding', 'representation learning', 'self-supervised']","[6, 6, 5]","[5, 3, 4]",0,"[559, 221, 350]","The binding between proteins and ligands plays a crucial role in the realm of drug discovery. Previous deep learning approaches have shown promising results over traditional computationally intensive methods, but resulting in poor generalization due to limited supervised data. In this paper, we propose to learn protein-ligand binding representation in a self-supervised learning manner. Different from existing pre-training approaches which treat proteins and ligands individually, we emphasize to discern the intricate binding patterns from fine-grained interactions. Specifically, this self-supervised learning problem is formulated as a prediction of the conclusive binding complex structure given a pocket and ligand with a Transformer based interaction module, which naturally emulates the binding process. To ensure the representation of rich binding information, we introduce two pre-training tasks, i.e. atomic pairwise distance map prediction and mask ligand reconstruction, which comprehensively model the fine-grained interactions from both structure and feature space. Extensive experiments have demonstrated the superiority of our method across various binding tasks, including protein-ligand affinity prediction, virtual screening and protein-ligand docking.",28,https://openreview.net/forum?id=AXbN2qMNiW,5.666666666666667,0.4714045207910317,4.0,0.816496580927726
9UIGyJJpay,De novo Protein Design Using Geometric Vector Field Networks,"['Protein design', 'Protein structure encoder', 'Inverse folding', 'Protein diffusion']","[8, 6, 8]","[4, 5, 4]",0,"[447, 538, 620]","Advances like protein diffusion have marked revolutionary progress in $\textit{de novo}$ protein design, a central topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Only a few basic encoders, like IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we introduce the Vector Field Network (VFN), that enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential $\textit{universal encoder}$. In protein diffusion (frame modeling), VFN exhibits a impressive performance advantage over IPA, excelling in terms of both designability ($\textbf{67.04}$\% vs. 53.58\%) and diversity ($\textbf{66.54}$\% vs. 51.98\%). In inverse folding(frame and atom modeling), VFN outperforms the previous SoTA model, PiFold ($\textbf{54.7}$\% vs. 51.66\%), on sequence recovery rate; we also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA ($\textbf{62.67}$\% vs. 55.65\%), LM-Design, by a substantial margin.",23,https://openreview.net/forum?id=9UIGyJJpay,7.333333333333333,0.9428090415820634,4.333333333333333,0.4714045207910317
8pYNdmwGAO,EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding,"['Mutation Prediction', 'Protein Property Prediction', 'Homologous Protein Sequence Modelling', 'Message Passing Neural Network']","[3, 3, 6]","[4, 4, 3]",0,"[1202, 296, 488]","Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNN can capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than state-of-the-art methods and attains 36X inference speedup in comparison with large pre-trained models. The code and models are available at https://anonymous.4open.science/r/EvolMPNN.",3,https://openreview.net/forum?id=8pYNdmwGAO,4.0,1.4142135623730951,3.6666666666666665,0.4714045207910317
8Ur2xmuw7w,Revisiting Link Prediction: a data perspective,['Link Prediction;Graph Neural Network'],"[6, 6, 8, 5]","[4, 5, 5, 3]",0,"[512, 212, 312, 298]","Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently underperforming on edges where the feature proximity factor dominates. Inspired by these new insights from a data perspective, we offer practical instruction for GNN4LP model design and guidelines for selecting appropriate benchmark datasets for more comprehensive evaluations.",21,https://openreview.net/forum?id=8Ur2xmuw7w,6.25,1.0897247358851685,4.25,0.82915619758885
8DLVrWL78S,Streamlining Generative Models for Structure-Based Drug Design,"['drug design', 'binding', 'docking', 'graph neural networks', 'generalization bounds']","[3, 3, 5, 5]","[3, 4, 4, 4]",0,"[348, 379, 361, 230]","Generative models for structure-based drug design (SBDD) aim to generate novel 3D molecules for specified protein targets $\textit{in silico}$. The prevailing paradigm focuses on model expressivity - typically with powerful Graph Neural Network (GNN) models - but is agnostic to binding affinity during training, potentially overlooking better molecules. We address this issue with a two-pronged approach: learn an economical surrogate for affinity to infer an unlabeled molecular graph, and optimize for labels conditioned on this graph and desired molecular properties (e.g., QED, SA). The resulting model FastSBDD achieves state-of-the-art results as well as streamlined computation and model size (up to 1000x faster and with 100x fewer trainable parameters compared to existing methods), paving way for improved docking software. We also establish rigorous theoretical results to expose the representation limits of GNNs in SBDD contexts and the generalizability of our affinity scoring model, advocating more emphasis on generalization going forward.",20,https://openreview.net/forum?id=8DLVrWL78S,4.0,1.0,3.75,0.4330127018922193
7JRbs3i9Ei,Machine Learning for PROTAC Engineering,"['Deep learning', 'Chemoinformatics', 'PROTAC', 'Drug design.']","[5, 5, 3]","[3, 4, 4]",0,"[464, 488, 673]","PROTACs are a promising therapeutic technology that harnesses the cell's built-in degradation processes to degrade specific proteins. Despite their potential, developing new PROTAC molecules is challenging and requires significant expertise, time, and cost. Meanwhile, machine learning has transformed various scientific fields, including drug development. In this work, we present a strategy for curating open-source PROTAC data and propose an open-source toolkit for predicting the degradation effectiveness, i.e., activity, of novel PROTAC molecules. We organized the curated data into 16 different datasets ready to be processed by machine learning models. The datasets incorporate important features such as $pDC_{50}$, $D_{max}$, E3 ligase type, POI amino acid sequence, and experimental cell type. Our toolkit includes a configurable PyTorch dataset class tailored to process PROTAC features, a customizable machine learning model for processing various PROTAC features, and a hyperparameter optimization mechanism powered by Optuna. To evaluate the system, three surrogate models were developed utilizing different PROTAC representations. Using our automatically-curated public datasets, the best models achieved a 71.4% validation accuracy and a 0.73 ROC-AUC validation score. This is not only comparable to state-of-the-art models for protein degradation prediction, but also open-source, easily-reproducible, and less computationally complex than existing approaches.",6,https://openreview.net/forum?id=7JRbs3i9Ei,4.333333333333333,0.9428090415820634,3.6666666666666665,0.4714045207910317
760br3YEtY,($\texttt{PEEP}$) $\textbf{P}$redicting $\textbf{E}$nzym$\textbf{e}$ $\textbf{P}$romiscuity with its Molecule Mate – an Attentive Metric Learning Solution,['Protein Engineering; Metric Learning;'],"[5, 6, 5, 6, 6]","[2, 3, 4, 4, 3]",0,"[178, 196, 231, 620, 275]","Annotating the functions of proteins (e.g., enzymes) is a fundamental challenge, due to their diverse functionalities and rapidly increased number of protein sequences in databases. Traditional approaches have limited capability and suffer from false positive predictions. Recent machine learning (ML) methods reach satisfactory prediction accuracy but still fail to generalize, especially for less-studied proteins and those with previously uncharacterized functions or promiscuity. To address these pain points, we propose a novel ML algorithm, PEEP, to predict enzyme promiscuity, which integrates biology priors of protein functionality to regularize the model learning. To be specific, at the input level, PEEP fuses the corresponding molecule into protein embeddings to gain their reaction information; at the model level, a tailored self-attention is leveraged to capture importance residues which we found are aligned with the active site in protein pocket structure; at the objective level, we embed functionality label hierarchy into metric learning objectives by imposing larger distance margin between proteins that have less functionality in common. PEEP is extensively validated on three public benchmarks, achieving up to 4.6%,3.1%,3.7% improvements on F-1 scores compared to existing methods. Moreover, it demonstrates impressive generalization to unseen protein sequences with unseen functionalities. Codes are included in the supplement.",16,https://openreview.net/forum?id=760br3YEtY,5.6,0.48989794855663565,3.2,0.7483314773547882
6MRm3G4NiU,SaProt: Protein Language Modeling with Structure-aware Vocabulary,"['Protein Language Models', 'Universal Representations', 'Downstream Tasks', 'Protein Structure Modeling']","[8, 8, 6]","[5, 5, 3]",0,"[297, 273, 502]","Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners in biology.  However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement. Motivated by this, we introduce the concept of a ``structure-aware vocabulary"" that  integrates residue tokens with structure tokens.    The structure tokens are  derived  by encoding the 3D structure of proteins using Foldseek. We then propose SaProt, a large-scale general-purpose PLM trained on an extensive dataset comprising approximately 40 million protein sequences and structures. Through extensive evaluation, our SaProt model surpasses well-established and renowned baselines across 10 significant downstream tasks, demonstrating its exceptional capacity and broad applicability. We have made the code, pre-trained model, and all relevant materials available at https://github.com/SaProt/SaProt.",12,https://openreview.net/forum?id=6MRm3G4NiU,7.333333333333333,0.9428090415820634,4.333333333333333,0.9428090415820634
4MsfQ2H0lP,Generative Adversarial Policy Network for Modelling Protein Complexes,"['protein complex structure prediction', 'docking path prediction', 'policy network', 'reinforcement learning']","[6, 6, 6, 6]","[5, 3, 3, 3]",0,"[406, 119, 442, 299]","Structure prediction of large protein complexes (a.k.a., protein multimer mod-
elling, PMM) can be achieved through the one-by-one assembly using provided
dimer structures and predicted docking paths. However, existing PMM methods
struggle with vast search spaces and generalization challenges: (1) The assembly
of a N -chain multimer can be depicted using graph structured data, with each
chain represented as a node and assembly actions as edges. Thus the assembly
graph can be arbitrary acyclic undirected connected graph, leading to the com-
binatorial optimization space of N^(N −2) for the PMM problem. (2) Knowledge
transfer in the PMM task is non-trivial. The gradually limited data availability as
the chain number increases necessitates PMM models that can generalize across
multimers of various chains. To address these challenges, we propose GAPN, a
Generative Adversarial Policy Network powered by domain-specific rewards and
adversarial loss through policy gradient for automatic PMM prediction. Specifi-
cally, GAPN learns to efficiently search through the immense assembly space and
optimize the direct docking reward through policy gradient. Importantly, we de-
sign a adversarial reward function to enhance the receptive field of our model. In
this way, GAPN will simultaneously focus on a specific batch of multimers and
the global assembly rules learned from multimers with varying chain numbers.
Empirically, we have achieved both significant accuracy (measured by RMSD
and TM-Score) and efficiency improvements compared to leading complex mod-
eling software. GAPN outperforms the state-of-the-art method (MoLPC) with up
to 27% improvement in TM-Score, with a speed-up of 600×.",21,https://openreview.net/forum?id=4MsfQ2H0lP,6.0,0.0,3.5,0.8660254037844386
3pgJNIx3gc,AlphaFold Distillation for Protein Design,"['Inverse Protein Folding Design', 'Protein Design', 'Model Distillation', 'AlphaFold', 'Protein Folding']","[3, 5, 3]","[5, 4, 4]",0,"[307, 285, 262]","Inverse protein folding, the process of designing sequences that fold into a specific 3D structure, is crucial in bio-engineering and drug discovery. Traditional methods rely on experimentally resolved structures, but these cover only a small fraction of protein sequences. Forward folding models like AlphaFold offer a potential solution by accurately predicting structures from sequences. However, these models are too slow for integration into the optimization loop of inverse folding models during training.
To address this, we propose using knowledge distillation on folding model confidence metrics, such as pTM or pLDDT scores, to create faster and end-to-end differentiable distilled model. This model can then be used as a structure consistency regularizer in training the inverse folding model. Our technique is versatile and can be applied to other design tasks, such as sequence-based protein infilling.
Experimental results show that our method outperforms non-regularized baselines, yielding up to 3\% improvement in sequence recovery and up to 45\% improvement in protein diversity while maintaining structural consistency in generated sequences. Anonymized code for this work is available at https://anonymous.4open.science/r/AFDistill-28C3",9,https://openreview.net/forum?id=3pgJNIx3gc,3.6666666666666665,0.9428090415820634,4.333333333333333,0.4714045207910317
3K3aWRpRNq,Reducing Atomic Clashes in Geometric Diffusion Models for 3D Structure-Based Drug Design,"['Structure Based Drug Design', 'Geometric Molecular Generation', 'Diffusion Models']","[3, 3, 6, 3]","[3, 4, 4, 3]",0,"[233, 238, 413, 418]","In the domain of Three-dimensional Structure-Based Drug Design (3D SBDD), the 3D spatial structures of target pockets serve as inputs for the generation of molecular geometric graphs. The Geometric Diffusion Model (GDM) has been recognized as the state-of-the-art (SOTA) method in 3D SBDD, attributed to its exceptional generation capabilities on geometric graphs. However, the inherent data-driven nature of GDM occasionally neglects critical inter-molecular interactions, such as Van der Waals force and Hydrogen Bonding. Such omissions could produce molecules that violate established physical principles. Particular evidence is that GDMs exhibit atomic clashes during generation due to the overly close proximity of generated molecules to protein structures. To address this, our paper introduces a novel constrained sampling process designed to obviate such undesirable collisions. By integrating a non-convex constraint within the current Langevin Dynamics (LD) of GDM and utilizing the proximal regularization techniques, we force molecular coordinates to obey the imposed physical constraints. Notably, the proposed method requires no modifications to the training process of GDMs. Empirical evaluations show a significant reduction in atomic clashes via the proposed method compared to the original LD process of GDMs.",13,https://openreview.net/forum?id=3K3aWRpRNq,3.75,1.299038105676658,3.5,0.5
2xYO9oxh0y,DiffSDS: A geometric sequence diffusion model for protein backbone inpainting,['Conditional sequence diffusion'],"[3, 5, 3]","[5, 4, 3]",1,"[352, 492, 466]","Can a pure transformer learn protein structure under geometric constraints? Recent research has simplified protein structures as sequences of folding angles, making transformers suitable for unconstrained protein backbone generation. Unfortunately, such simplification is unsuitable for the constrained protein inpainting problem: we reveal theoretically that applying geometric constraints to the angle space would result in gradient vanishing or exploding, called \textbf{GradCurse}. As a remedy, we suggest adding a hidden \textbf{a}tomic \textbf{d}irection \textbf{s}pace (\textbf{ADS}) layer upon the transformer encoder, converting invariant backbone angles into equivariant direction vectors. Geometric constraints could be efficiently imposed on the direction space while avoiding GradCurse. Meanwhile, a Direct2Seq decoder with mathematical guarantees is also introduced to reconstruct the folding angles. We apply the \textbf{dual-space} model as the denoising neural network during the conditional diffusion process, resulting in a constrained generative model--\textbf{DiffSDS}. Extensive experiments show that the proposed DiffSDS outperforms the sequence diffusion baseline, and even achieves competitive results with coordinate diffusion models, filling the gap between sequence and coordinate diffusion models.",3,https://openreview.net/forum?id=2xYO9oxh0y,3.6666666666666665,0.9428090415820634,4.0,0.816496580927726
1IaoWBqB6K,DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility,"['diffusion', 'diffusion models', 'docking', 'generative model']","[6, 5, 6, 3]","[3, 4, 3, 4]",0,"[522, 454, 380, 360]","When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task.",15,https://openreview.net/forum?id=1IaoWBqB6K,5.0,1.224744871391589,3.5,0.5
0xT87opqKV,ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning,"['Pretrained Large Models', 'Parameter-Efficient Fine-tuning', 'Protein Representation Learning']","[3, 3, 5, 5]","[3, 3, 4, 5]",0,"[794, 187, 211, 404]","The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Recent advancements in Large Protein Models (LPMs) have demonstrated their ability in sequence and structure understanding, suggesting the potential of directly using them for efficient protein representation learning. In this work, we introduce ProteinAdapter, to efficiently transfer the general reference from the multiple Large Protein Models (LPMs), e.g., ESM-1b, to the task-specific knowledge. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. We observe that such a simple yet effective approach works well on multiple downstream tasks. Specifically, (1) with limited extra parameters, ProteinAdapter enables multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs. (2) Based on the learned embedding, we further scale the proposed ProteinAdapter to multiple conventional protein tasks. Considering different task priors, we propose a unified multi-scale predictor to fully take advantage of the learned embeddings via task-specific focus. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task settings. We hope that the proposed method could accelerate the study of protein analysis in the future.",4,https://openreview.net/forum?id=0xT87opqKV,4.0,1.0,3.75,0.82915619758885
070DFUdNh7,GraphGPT: Graph Learning with Generative Pre-trained Transformers,"['Graph', 'GPT', 'Generative', 'Pre-train', 'Fine-tune', 'Transformer', 'GraphGPT']","[5, 5, 3, 5]","[4, 3, 3, 3]",0,"[371, 376, 218, 184]","We introduce GraphGPT, a novel model for Graph learning by self-supervised Generative Pre-training Transformers. Our model transforms each graph or sampled subgraph into a sequence of tokens representing the node, edge and attributes reversibly using the Eulerian path first. Then we feed the tokens into a standard transformer decoder and pre-train it with the next-token-prediction (NTP) task. Lastly, we fine-tune the GraphGPT model with the supervised tasks. This intuitive, yet effective model achieves superior or close results to the state-of-the-art methods for the graph-, edge- and node-level tasks on the large scale molecular dataset PCQM4Mv2, the protein-protein association dataset ogbl-ppa and the ogbn-proteins dataset from the Open Graph Benchmark (OGB). Furthermore, the generative pre-training enables us to train GraphGPT up to 400M+ parameters with consistently increasing performance, which is beyond the capability of GNNs and previous graph transformers. The source code and pre-trained checkpoints will be released soon to pave the way for the graph foundation model research, and also to assist the scientific discovery in pharmaceutical, chemistry, material and bio-informatics domains, etc.",11,https://openreview.net/forum?id=070DFUdNh7,4.5,0.8660254037844386,3.25,0.4330127018922193
04UvXg4CvW,EPIC: Compressing Deep GNNs via Expressive Power Gap-Induced Knowledge Distillation,"['deep graph neural networks', 'knowledge distillation', 'expressive power gap']","[5, 5, 3]","[4, 4, 4]",0,"[282, 705, 159]","The teacher-student paradigm-based knowledge distillation (KD) has recently emerged as a promising technique for compressing graph neural networks (GNNs). Despite the great success in compressing moderate-sized GNNs, distilling deep GNNs (e.g., with over 100 layers) remains a tough challenge. A widely recognized reason is the *teacher-student expressive power gap*, i.e., the embeddings of a deep teacher may be extremely hard for a shallow student to approximate. Besides, the theoretical analysis and measurement of this gap are currently missing, resulting in a difficult trade-off between the needs of being ""lightweight'' and being ""expressive'' when selecting a student for the deep teacher. To bridge the theoretical gap and address the challenge of distilling deep GNNs, we propose the *first* GNN KD framework that quantitatively analyzes the teacher-student expressive power gap, namely **E**xpressive **P**ower gap-**I**ndu**C**ed knowledge distillation (**EPIC**). Our key idea is to formulate the estimation of the expressive power gap as an embedding regression problem based on the theory of polynomial approximation. Then, we show that the minimum approximation error has an upper bound, which decreases rapidly with respect to the number of student layers. Furthermore, we empirically demonstrate that the upper bound exponentially converges to zero as the number of student layers increases. Moreover, we propose to select an appropriate value for the number of student layers based on the upper bound, and propose an expressive power gap-induced loss term to further encourage the student to generate embeddings similar to those of the teacher. Experiments on large-scale benchmarks demonstrate that EPIC can effectively reduce the numbers of layers of deep GNNs, while achieving comparable or superior performance. Specifically, for the 1,001-layer RevGNN-Deep, we reduce the number of layers by 94\% and accelerate inference by roughly eight times, while achieving comparable performance in terms of ROC-AUC on the large-scale benchmark ogbn-proteins.",11,https://openreview.net/forum?id=04UvXg4CvW,4.333333333333333,0.9428090415820634,4.0,0.0
