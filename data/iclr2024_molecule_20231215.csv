id,title,keywords,ratings,confidences,withdraw,review_lengths,abstract,comments,url,ratings_avg,ratings_std,confidence_avg,confidence_std
zUHgYRRAWl,Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE,"['VAE', 'molecule generation']","[1, 3, 1]","[5, 4, 4]",1,"[80, 568, 34]","Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the number of active molecules generated by the multi-stage VAE in comparison to their one-stage equivalent. For each of the two tasks, our baselines include methods that use learned property predictors to incorporate target metrics directly into the training objective and we discuss complications that arise with this methodology.",3,https://openreview.net/forum?id=zUHgYRRAWl,1.6666666666666667,0.9428090415820634,4.333333333333333,0.4714045207910317
zUDbPgskDS,"Crystals with Transformers on Graphs, for predictions of crystal material properties","['AI for science', 'Graph networks', 'transformers', 'materials informatics', 'crystal materials']","[6, 3, 3, 1]","[3, 2, 3, 5]",0,"[607, 206, 350, 409]","Graph neural networks (GNN) has found extensive applications across diverse domains, notably in the modeling molecules. Crystals differ from molecules by the ionic bonding across the lattice and the highly ordered microscopic structure, which provides crystals unique symmetry and determines the macroscopic properties. Therefore, long-range orders are essential in predicting the physical and chemical properties of crystals. GNNs successfully model the local environment of atoms in crystals, however, they struggle to capture long-range interactions due to a limitation of depth. In this paper, we propose CrysToGraph ($\textbf{Crys}$tals with $\textbf{T}$ransformers $\textbf{o}$n $\textbf{Graph}$s), a novel transformer-based geometric graph network designed specifically for crystalline systems. CrysToGraph effectively captures short-range dependencies with transformer-based graph convolution blocks and long-range dependencies with graph-wise transformer blocks. Our model outperforms most existing methods by achieving new state-of-the-art results on the MatBench benchmark datasets.",11,https://openreview.net/forum?id=zUDbPgskDS,3.25,1.7853571071357126,3.25,1.0897247358851685
z3mPLBLfGY,Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning,['unified representation; molecular interaction; equivariant transformer'],"[6, 5, 8, 5]","[5, 4, 3, 3]",0,"[175, 389, 157, 335]","Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the universal underlying interaction physics. 
In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels.  Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains.",25,https://openreview.net/forum?id=z3mPLBLfGY,6.0,1.224744871391589,3.75,0.82915619758885
yrgQdA5NkI,Equivariant Matrix Function Neural Networks,"['equivariance', 'graph neural networks', 'long range']","[6, 8, 5]","[3, 4, 2]",0,"[1095, 782, 260]","Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials.
Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size.
The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems. The code and the datasets will be made public.",13,https://openreview.net/forum?id=yrgQdA5NkI,6.333333333333333,1.247219128924647,3.0,0.816496580927726
yRrPfKyJQ2,Conversational Drug Editing Using Retrieval and Domain Feedback,"['Large Language Models', 'prompt', 'retrieval', 'domain feedback', 'conversation', 'drug editing', 'drug optimization', 'controllable generation', 'small molecule', 'peptide', 'protein']","[6, 6, 6]","[2, 3, 4]",0,"[202, 195, 394]","Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",11,https://openreview.net/forum?id=yRrPfKyJQ2,6.0,0.0,3.0,0.816496580927726
xzRnzHUVE9,Enhancing Sample Efficiency in Black-box Combinatorial Optimization via Symmetric Replay Training,"['Black-box combinatorial optimization', 'sample efficiency', 'symmetries', 'drug discovery', 'hardware design', 'deep reinforcement learning', 'imitation learning']","[8, 5, 5, 6]","[3, 4, 4, 4]",0,"[922, 448, 578, 831]","Black-box combinatorial optimization (black-box CO) is frequently encountered in various industrial fields, such as drug discovery or hardware design. Despite its widespread relevance, solving black-box CO problems is highly challenging due to the vast combinatorial solution space and resource-intensive nature of black-box function evaluations. These inherent complexities induce significant constraints on the efficacy of existing deep reinforcement learning (DRL) methods when applied to practical problem settings. For efficient exploration with the limited availability of function evaluations, this paper introduces a new generic method to enhance sample efficiency. We propose symmetric replay training that leverages the high-reward samples and their under-explored regions in the symmetric space. In replay training, the policy is trained to imitate the symmetric trajectories of these high-rewarded samples. The proposed method is beneficial for the exploration of highly rewarded regions without the necessity for additional online interactions - free. The experimental results show that our method consistently improves the sample efficiency of various DRL methods on real-world tasks, including molecular optimization and hardware design. Our source code is available at https://anonymous.4open.science/r/sym_replay.",22,https://openreview.net/forum?id=xzRnzHUVE9,6.0,1.224744871391589,3.75,0.4330127018922193
xlQrAm3LE4,DiffSim: Aligning Diffusion Model and Molecular Dynamics Simulation for Accurate Blind Docking,"['Diffusion Model', 'Molecular Dynamics Simulation', 'Blind Docking', 'Drug Discovery', 'Molecular Conformation Generation']","[3, 5, 3, 3]","[5, 3, 4, 5]",0,"[208, 297, 370, 238]","Predicting the ligand’s binding conformation within a target protein is a pivotal step in drug discovey. Based on prior knowledge of the binding site (protein pocket) on the target protein, biochemical researchers use molecular docking software to generate the ligand conformation within that pocket. Despite its speed, molecular docking is ill-suited for blind docking where the pocket is unknown, and the generated ligand conformation often lacks required precision. Recently, deep generative models, especially diffusion models, have been proposed for accurate blind docking. However, it is found that while deep generative models excel in locating the pocket, they still lag behind traditional methods in terms of conformation generation. Thus, bridging such gap with a hybrid approach is naturally expected to further improve the model performance. Therefore, in this study, we introduce a blind docking approach named DiffSim to seamlessly integrate the diffusion model with molecular dynamics (MD) simulation. We propose a novel loss function to align reverse diffusion sampling with MD simulation trajectories, aiming to efficiently generate ligand conformations informed by MD-modelled protein-ligand interactions at atomic resolution. Through theoretical analysis, we unveil the consistency in dynamics between diffusion models and MD simulation, demonstrating that the diffusion model is essentially a coarse-grained simulator for MD simulation. Empirical results demonstrate the effectiveness of our approach and highlight the potential of combining physics-informed MD simulation with deep learning models in drug discovery.",8,https://openreview.net/forum?id=xlQrAm3LE4,3.5,0.8660254037844386,4.25,0.82915619758885
xh0XzueyCJ,Plug-And-Play Controllable Graph Generation With Diffusion Models,"['Controllable Graph Generation', 'Denoising Diffusion Models', 'Projected Sampling', 'Molecule Generation']","[6, 6, 6, 5]","[3, 3, 4, 5]",0,"[328, 350, 358, 414]","Diffusion models for graph generation present transformative capabilities in generating graphs for various downstream applications. However, controlling the properties of the generated graphs remains a challenging task for these methods. Few approaches tackling this challenge focus on the ability to control for a soft differentiable property using conditional graph generation, leading to an uninterpretable control. However, in real-world applications like drug discovery, it is vital to have precise control over the generated outputs for specific features (e.g. the number of bonds in a molecule). Current diffusion models fail to support such hard non-differentiable constraints over the generated samples. To address this limitation, we propose PRODIGY (PROjected DIffusion for generating constrained Graphs), a novel plug-and-play approach to sample graphs from any pre-trained diffusion model such that they satisfy precise constraints. We formalize the problem of controllable graph generation and identify a class of constraints applicable to practical graph generation tasks. PRODIGY operates by controlling the samples at each diffusion timestep using a projection operator onto the specified constrained space. Through extensive experiments on generic and molecular graphs, we demonstrate that PRODIGY enhances the ability of pre-trained diffusion models to satisfy specified hard constraints, while staying close to the data distribution. For generic graphs, it improves constraint satisfaction performance by up to $100$%, and for molecular graphs, it achieves up to $60$% boost under a variety of constraints.",25,https://openreview.net/forum?id=xh0XzueyCJ,5.75,0.4330127018922193,3.75,0.82915619758885
xLRAQiqd9I,GeoMFormer: A General Architecture for Geometric Molecular Representation Learning,"['Transformer', 'Molecular Modeling; Geometric Molecular Representation; Invariance; Equivariance']","[6, 8, 6, 5]","[4, 4, 3, 4]",0,"[284, 187, 349, 392]","Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available.",20,https://openreview.net/forum?id=xLRAQiqd9I,6.25,1.0897247358851685,3.75,0.4330127018922193
xI4yNlkaqh,Towards 3D Molecule-Text Interpretation in Language Models,"['3D molecules', 'Large Language Model', '3D-text interpretation']","[6, 5, 6, 6]","[3, 5, 4, 1]",0,"[308, 443, 572, 126]","Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and
analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder’s representation space and the LM’s input space. Moreover, to enhance 3DMoLM’s ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset – 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including moleculetext retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. We will release our codes and datasets at https://anonymous.4open.science/r/3D-MoLM.",19,https://openreview.net/forum?id=xI4yNlkaqh,5.75,0.4330127018922193,3.25,1.479019945774904
xAqcJ9XoTf,On the Stability of Expressive Positional Encodings for Graph Neural Networks,"['graph neural networks', 'positional encoding', 'stability']","[6, 5, 6, 5, 8]","[4, 4, 3, 3, 4]",0,"[413, 581, 437, 251, 461]","Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) *Non-uniqueness*: there are many different eigendecompositions of the same Laplacian, and (2) *Instability*: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be the use of ""hard partition'' of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to ``softly partition'' eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.",27,https://openreview.net/forum?id=xAqcJ9XoTf,6.0,1.0954451150103321,3.6,0.4898979485566356
wwotGBxtC3,Data-Efficient Molecular Generation with Hierarchical Textual Inversion,['Molecular generation'],"[6, 6, 6, 5]","[4, 3, 4, 4]",0,"[183, 197, 320, 289]","Developing an effective molecular generation framework even with a limited number of molecules is often important for its practical deployment, e.g., drug discovery, since acquiring task-related molecular data requires expensive and time-consuming experimental costs. 
To tackle this issue, we introduce Hierarchical textual Inversion for Molecular Generation (HI-Mol), a novel data-efficient molecular generation method.
HI-Mol is inspired by a recent textual inversion technique in the visual domain that achieves data-efficient generation via simple optimization of a new single text token of a pre-trained text-to-image generative model.
However, we find that its naive adoption fails for molecules due to their complicated and structured nature. 
Hence, we propose a hierarchical textual inversion scheme based on introducing low-level tokens that are selected differently per molecule in addition to the original single text token in textual inversion to learn the common concept among molecules.
We then generate molecules using a pre-trained text-to-molecule model by interpolating the low-level tokens.
Extensive experiments demonstrate the superiority of HI-Mol with notable data-efficiency. 
For instance, on QM9, HI-Mol outperforms the prior state-of-the-art method 
with 50$\times$ less training data. 
We also show the efficacy of HI-Mol in various applications, including molecular optimization and low-shot molecular property prediction.",20,https://openreview.net/forum?id=wwotGBxtC3,5.75,0.4330127018922193,3.75,0.4330127018922193
wmw3Jy8MVF,Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction,"['graph neural network', 'deep learning', 'molecule']","[6, 8, 6, 3]","[3, 4, 4, 3]",0,"[151, 225, 284, 370]","Equivariant deep models have recently been employed to predict atomic potentials and force fields in molecular dynamics. A key advantage of these models is their ability to learn from data without requiring explicit physical modeling. Nevertheless, use of models obeying underlying physics can not only lead to better performance, but also yield physically interpretable results. In this work, we propose a new equivariant network, known as PACE, to incorporate many-body interactions by making use of the Atomic Cluster Expansion (ACE) mechanism. To provide a solid foundation for our work, we perform theoretical analysis showing that our proposed message passing scheme can approximate any equivariant polynomial functions with constrained degree. By relying physical insights and theoretical foundations, we show that our model achieves state-of-the-art performance on atomic potential and force field prediction tasks on commonly used benchmarks.",12,https://openreview.net/forum?id=wmw3Jy8MVF,5.75,1.7853571071357126,3.5,0.5
wDd4Zcnc08,HP$^3$-NS: Hybrid Perovskite Property Prediction Using Nested Subgraph,['Hybrid organic-inorganic materials; graph representation; material designing;'],"[5, 5, 5, 5]","[4, 3, 3, 4]",0,"[307, 206, 75, 165]","Many machine learning techniques have demonstrated superiority in large-scale material screening, enabling rapid and accurate estimation of material properties. However, data representation on hybrid organic-inorganic (HOI) crystalline materials poses a distinct challenge due to their intricate nature. Current graph-based representations often struggle to effectively capture the nuanced interactions between organic and inorganic components. Furthermore, these methods typically rely on detailed structural information that hinders the applications of the methods for novel material discovery. To address these, we propose a nested graph representation HP$^3$-NS (Hybrid Perovskite Property Prediction Using Nested Subgraph) that hierarchically encodes the distinct interactions within hybrid crystals. Our encoding scheme incorporates both intra- and inter-molecular interactions and distinguishes between the organic and inorganic components. This hierarchical representation also removes the dependence on detailed structural data, enabling the model application to newly designed materials. We demonstrate the effectiveness and significance of the method on hybrid perovskite datasets, wherein the proposed HP$^3$-NS achieves significant accuracy improvement compared to current state-of-the-art techniques for hybrid material property prediction tasks. Our method shows promising potential to accelerate hybrid perovskite development by enabling effective computational screening and analysis of HOI crystals.",13,https://openreview.net/forum?id=wDd4Zcnc08,5.0,0.0,3.5,0.5
w2GlpOHdg1,Atoms as Words: A Novel Approach to Deciphering Material Properties using NLP-inspired Machine Learning on Crystallographic Information Files (CIFs),"['Condensed matter physics', 'Materials science', 'Material properties prediction', 'Crystallographic Information Files (CIFs)', 'Natural Language Processing (NLP) in materials', 'Word2Vec-inspired technique', 'Atomic embeddings', 'CIFSemantics model', 'Band gap', 'Formation energy', 'Material representation']","[3, 3, 3, 3]","[5, 4, 5, 5]",1,"[456, 779, 276, 299]","In condensed matter physics and materials science, predicting material properties necessitates understanding intricate many-body interactions. Conventional methods such as density functional theory (DFT) and molecular dynamics (MD) often resort to simplifying approximations and are computationally expensive. Meanwhile, recent machine learning methods use handcrafted descriptors for material representation which sometimes neglect vital crystallographic information and are often limited to single property prediction or a sub-class of crystal structures. In this study, we pioneer an unsupervised strategy, drawing inspiration from Natural Language Processing (NLP), to harness the underutilized potential of Crystallographic Information Files (CIFs). We conceptualize atoms and atomic positions within a CIF similarly to words in textual content. Using a Word2Vec-inspired technique, we produce atomic embeddings that capture intricate atomic relationships. Our model, CIFSemantics, trained on the extensive Material Project dataset, adeptly predicts 15 distinct material properties from the CIFs. Its performance rivals specialized models, marking a significant step forward in material property predictions.",4,https://openreview.net/forum?id=w2GlpOHdg1,3.0,0.0,4.75,0.4330127018922193
uqPnesiGGi,Motif-aware Attribute Masking for Molecular Graph Pre-training,"['Molecular Property Prediction', 'Motifs', 'Graph Learning', 'Self-Supervised Learning']","[3, 6, 5, 6, 3]","[4, 4, 4, 4, 4]",0,"[335, 150, 531, 336, 364]","Attribute reconstruction is used to predict node or edge features in the pre-training of graph neural networks. Given a large number of molecules, they learn to capture structural knowledge, which is transferable for various downstream property prediction tasks and vital in chemistry, biomedicine, and material science. Previous strategies that randomly select nodes to do attribute masking leverage the information of local neighbors. However, the over-reliance of these neighbors inhibits the model's ability to learn long-range dependencies from higher-level substructures. For example, the model would learn little from predicting three carbon atoms in a benzene ring based on the other three but could learn more from the inter-connections between the functional groups, or called chemical motifs. In this work, we propose and investigate motif-aware attribute masking strategies to capture long-range inter-motif structures by leveraging the information of atoms in neighboring motifs. Once each graph is decomposed into disjoint motifs, the features for every node within a sample motif are masked. The graph decoder then predicts the masked features of each node within the motif for reconstruction. We evaluate our approach on eight molecular property prediction datasets and demonstrate its advantages.",18,https://openreview.net/forum?id=uqPnesiGGi,4.6,1.3564659966250536,4.0,0.0
uUEvmY8Gfz,De novo Drug Design using Reinforcement Learning with Dynamic Vocabulary,"['De novo drug design', 'Molecular generation', 'Reinforcement learning', 'Dynamic vocabulary']","[3, 3, 3, 3]","[4, 3, 3, 4]",1,"[356, 333, 349, 396]","De novo drug design constitutes a fundamental challenge within the domain of computer-aided drug discovery (CADD). Generative models relying on SMILES molecular strings have emerged as promising tools for this purpose. However, extant SMILES-based generative models all adopt a fixed vocabulary, leading to deficiencies in both sampling efficiency and interpretability. 
In this paper, we propose RLDV, a reinforcement learning (RL) algorithm based on a GPT agent, which uses a dynamic chemical vocabulary (DV) during RL iterations. Specifically, we utilize SMILES pair encoding to analyze high-scoring molecular SMILES strings generated during the RL process, and extract their high-frequency common substrings, which are then added as new tokens to the agent's vocabulary. These additions aid in the generation of molecules during subsequent RL steps. Experimental results on the GuacaMol benchmark demonstrate that our algorithm outperforms existing models across multiple tasks, highlighting the practical significance of the dynamic vocabulary in drug design. Furthermore, the application of our algorithm in the design of protein-targeting drugs for SARS-CoV-2 underscores its substantial practical relevance.",4,https://openreview.net/forum?id=uUEvmY8Gfz,3.0,0.0,3.5,0.5
uMAujpVi9m,Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment,"['Drug Discovery', 'Pretraining']","[6, 6, 6, 6]","[4, 3, 1, 5]",0,"[252, 224, 379, 390]","Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.",19,https://openreview.net/forum?id=uMAujpVi9m,6.0,0.0,3.25,1.479019945774904
u8L1zzGXRq,Impact of Molecular Representations on Deep Learning Model Comparisons in Drug Response Predictions,"['Cancer Drug Response Prediction', 'Model Comparison']","[3, 3, 1, 5]","[4, 4, 2, 3]",0,"[412, 204, 332, 315]","Deep learning (DL) plays a crucial role in tackling the complexity and heterogeneity of cancer, particularly in predicting drug response. However, the effectiveness of these models is often hindered by inconsistent benchmarks and disparate data sources. To address the gaps in comparisons, we introduce CoMParison workflow for Cross Validation (CMP-CV), an automated cross-validation framework that trains multiple models with user-specified parameters and evaluation metrics. The effectiveness of DL models in predicting drug responses is closely tied to the methods used to represent drugs at the molecular level. In this contribution, we benchmarked commonly leveraged drug representations (graph, molecular descriptors, molecular fingerprints, and  SMILES) to lean and understand the predictive capabilities of the models. We compare the ability of different drug representations to encode different structural properties of the drugs by using prediction errors made by models in different drug descriptor domains. We find that, in terms of the average prediction error over the entire test set, molecular descriptor and encoded SMILES representations perform slightly better than the others. However, we also observe that the rankings of the model performance vary in different regions over the descriptor space studied in this work, emphasizing the importance of domain-based model comparison when selecting a model for a specific application. Our efforts are part of CANcer Distributed Learning Environment (CANDLE), enhancing the model comparison capabilities in cancer research and driving the development of more effective strategies for drug response prediction and optimization.",9,https://openreview.net/forum?id=u8L1zzGXRq,3.0,1.4142135623730951,3.25,0.82915619758885
tNAucRS0QQ,General-purpose Pre-trained Model Towards Cross-domain Molecule Learning,"['molecular representation learning', 'self-supervised pre-training', 'multimodal learning']","[5, 8, 5, 3]","[4, 4, 4, 4]",0,"[482, 340, 594, 349]","Self-supervised pre-training on biomolecules has achieved remarkable success in various biochemical applications, such as drug discovery and protein design. However, in most approaches, the learning model is primarily constructed based on the characteristics of either small molecules or proteins, without exploring their potential binding interactions -- an essential cross-domain relationship crucial for driving numerous biological processes. In this paper, inspired by the success of multimodal learning, we fill this gap by proposing a general-purpose foundation model named **BIT** (an abbreviation for **B**iomolecular **I**nteraction **T**ransformer), which is capable of encoding a range of biochemical entities, including small molecules, proteins, and protein-ligand complexes, as well as various data formats, encompassing both 2D and 3D structures, all within a shared Transformer backbone, via multiple unified self-supervised atom-level *denoising* tasks. We introduce *Mixture-of-Domain-Experts* (MoDE) to handle the biomolecules from diverse chemical domains and incorporate separate structural channels to capture positional dependencies in the molecular structures. The proposed MoDE allows BIT to enable both deep fusion and domain-specific encoding and learn cross-domain relationships on protein-ligand complexes with 3D cocrystal structures. Experimental results demonstrate that BIT achieves exceptional performance in both protein-ligand binding and molecular learning downstream tasks, including binding affinity prediction, virtual screening, and molecular property prediction.",13,https://openreview.net/forum?id=tNAucRS0QQ,5.25,1.7853571071357126,4.0,0.0
tGOOP7DGxs,Graph Transformers for Large Graphs,"['graph representation learning', 'graph transformers', 'large graphs']","[5, 5, 5, 5]","[4, 4, 3, 4]",0,"[626, 460, 230, 245]","Transformers have recently emerged as powerful neural networks for graph learning, showcasing state-of-the-art performance on several graph property prediction tasks. However, these results have been limited to small-scale graphs, such as ligand molecules with fewer than a hundred atoms, where the computational feasibility of the global attention mechanism is possible. The next goal is to scale up these architectures to handle very large graphs on the scale of millions or even billions of nodes. With large-scale graphs, global attention learning is proven impractical due to its quadratic complexity w.r.t. the number of nodes. On the other hand, neighborhood sampling techniques become essential to manage large graph sizes, yet finding the optimal trade-off between speed and accuracy with sampling techniques remains challenging. This work advances representation learning on single large-scale graphs with a focus on identifying model characteristics and critical design constraints for developing scalable graph transformer (GT) architectures. We argue such GT requires layers that can adeptly learn both local and global graph representations while swiftly sampling the graph topology. As such, a key innovation of this work lies in the creation of a fast neighborhood sampling technique coupled with a local attention mechanism that encompasses a 4-hop reception field, but achieved through just 2-hop operations. This local node embedding is then integrated with a global node embedding, acquired via another self-attention layer with an approximate global codebook, before finally sent through a downstream layer for node predictions. The proposed GT framework, named LargeGT, overcomes previous computational bottlenecks and is validated on three large-scale node classification benchmarks. We report a 3× speedup and 16.8% performance gain on ogbn-products and snap-patents compared to their nearest baselines respectively, while we also scale LargeGT on ogbn-papers100M with a 5.9% improvement in performance.",16,https://openreview.net/forum?id=tGOOP7DGxs,5.0,0.0,3.75,0.4330127018922193
sOXKeeVxqW,MoleSG: A Multi-Modality Molecular Pre-training Framework by Joint Non-overlapping Masked Reconstruction of SMILES and Graph,"['Molecular representation learning', 'Molecular pre-training', 'Molecular property prediction', 'AI for science']","[3, 3, 5, 5]","[4, 4, 4, 4]",1,"[398, 327, 306, 547]","Self-supervised pre-training plays an important role in molecular representation learning because labeled molecular data are usually limited in many tasks, such as chemical property prediction and virtual screening. However, most existing molecular pre-training methods focus on one modality of molecular data, and the complementary information of two important modalities, SMILES and graph, are not fully explored. In this study, we propose a straightforward yet effective multi-modality pre-training framework for Molecular SMILES and Graph (MoleSG). Specifically, the SMILES sequence data and graph data are first tokenized so that they can be processed by a unified transformer-based backbone network, which is trained by a masked reconstruction strategy. In addition, we introduce a specialized non-overlapping masking strategy to encourage fine-grained interaction between these two modalities. Experimental results show that our framework achieves state-of-the-art performance in a series of molecular property prediction tasks, and detailed ablation study demonstrates efficacy of the multi-modality structure and the masking strategy.",4,https://openreview.net/forum?id=sOXKeeVxqW,4.0,1.0,4.0,0.0
sLGliHckR8,Drug Discovery with Dynamic Goal-aware Fragments,"['molecule generation', 'drug discovery', 'molecular optimization', 'fragment-based']","[5, 8, 6]","[3, 3, 4]",0,"[130, 569, 148]","Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on  heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named *Goal-aware fragment Extraction, Assembly, and Modification* (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments that contribute to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GEAM can explore beyond the initial vocabulary with the fragment modification module, and the exploration is further enhanced through the dynamic goal-aware vocabulary update. We experimentally demonstrate that GEAM effectively discovers drug candidates through the generative cycle of the three modules in various drug discovery tasks. The anonymous code is available at https://anonymous.4open.science/r/GEAM-45EF.",9,https://openreview.net/forum?id=sLGliHckR8,6.333333333333333,1.247219128924647,3.3333333333333335,0.4714045207910317
sFJr7okOBi,NL2ProGPT: Taming Large Language Model for Conversational Protein Design,"['Protein design', 'Large language model']","[3, 5, 5, 5]","[5, 3, 3, 2]",0,"[675, 134, 812, 297]","Large Language Models (LLMs), like ChatGPT, excel in cross-modal tasks thanks to their powerful abilities in natural language comprehension, generalization, and reasoning. Meanwhile, the wealth of human-curated protein knowledge in text form presents a unique opportunity for LLMs to contribute to advanced protein design. In this work, we propose a new LLMs-based framework, namely NL2ProGPT, for macromolecular protein sequence generation that bridges the domain gap between natural and protein languages. Specifically, we first combine the protein functions and properties to create specific text guidelines for designing the protein, ensuring it follows precise controls. Second, to form a more informative and generalizable protein description, we explicitly inject protein structural information by clustering the embeddings from pre-trained protein language models. Third, we train a reward model to align the protein language model with the Rosetta energy function, following an RLAIF (reinforced learning from AI feedback) fashion. We empirically verify the effectiveness of NL2ProGPT from three aspects: (1) outperforms existing protein sequence design methods in different evaluations; (2) exhibits more than 90\% consistency in text-to-protein generation; (3) has effective exploration potential in disordered regions.",10,https://openreview.net/forum?id=sFJr7okOBi,4.5,0.8660254037844386,3.25,1.0897247358851685
rwmWd2rjP1,Molecule Relaxation by Reverse Diffusion with Time Step Prediction,"['diffusion time prediction', 'diffusion models', 'generative modeling', 'quantum chemistry', 'molecule relaxation']","[6, 5, 3, 5]","[5, 4, 3, 3]",0,"[223, 418, 516, 541]","Molecule relaxation---finding the stable state of an unstable configuration---is an important subtask for exploring the chemical compound space, for instance, to identify novel drugs or catalysts. Existing methods rely on local energy minimization with the gradients (i.e., force field) estimated through computationally intensive ab initio methods or approximated by a neural network trained on large expensive datasets encompassing \emph{labeled stable and unstable} molecules. In this work, we propose molecule relaxation by reverse diffusion (MoreRed), a novel purely statistical approach where unstable molecules are seen as \emph{noisy} samples to be denoised by a diffusion model equipped with a time step predictor to handle arbitrarily noisy inputs. Notably, MoreRed learns a simpler pseudo energy surface instead of the complex physical energy surface and is trained on a significantly smaller dataset consisting of solely \emph{unlabeled stable} molecules, which is considerably less expensive to generate. Nevertheless, our experiments demonstrate its competitive performance to the state-of-the-art baseline in terms of the quality of the relaxed molecules inferred. Furthermore, we identify the high potential that time step prediction has to enhance the performance of data generation, where our findings are promising both in molecular structure and image generation.",23,https://openreview.net/forum?id=rwmWd2rjP1,4.75,1.0897247358851685,3.75,0.82915619758885
rvDQtdMnOl,Long-Short-Range Message-Passing: A Fragmentation-Based Framework to Capture Non-Local Atomistic Interactions,"['Molecular Modeling', 'Quantum Chemistry', 'Fragmentation', 'Non-Local Interactions']","[8, 5, 6, 6]","[5, 4, 3, 2]",0,"[514, 438, 285, 465]","Computational simulation of chemical and biological systems using *ab initio* molecular dynamics has been a challenge over decades. Researchers have attempted to address the problem with machine learning and fragmentation-based methods. However, the two approaches fail to give a satisfactory description of long-range and many-body interactions, respectively. Inspired by fragmentation-based methods, we propose the Long-Short-Range Message-Passing (LSR-MP) framework as a generalization of the existing equivariant graph neural networks (EGNNs) with the intent to incorporate long-range interactions efficiently and effectively. We apply the LSR-MP framework to the recently proposed ViSNet and demonstrate the state-of-the-art results with up to 40% MAE reduction for molecules in MD22 and Chignolin datasets. Consistent improvements to various EGNNs will also be discussed to illustrate the general applicability and robustness of our LSR-MP framework. The code for our experiments and trained model weights could be found at https://anonymous.4open.science/r/LSRM-760E/.",15,https://openreview.net/forum?id=rvDQtdMnOl,6.25,1.0897247358851685,3.5,1.118033988749895
rjLgCkJH79,A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION,"['Drug Design', 'Molecular Optimisation', 'Reinforcement Learning']","[3, 3, 5]","[3, 3, 4]",0,"[715, 406, 574]","Lead optimization in drug discovery is a pivotal phase in identifying promising drug candidates for further development. Traditionally, lead optimization in the machine learning community has been treated as a constraint optimization problem where methods like generative models and reinforcement learning(RL) have been widely employed. However, these methods often rely on molecular similarity metrics to define constraints, which poses significant challenges due to the inherently ambiguous nature of molecular similarity. In this work, we present a similarity-agnostic approach to lead optimization, which we term ""Lead Optimization using Goal-conditioned Reinforcement Learning"" or LOGRL. Contrary to conventional methods, LOGRL is uniquely trained on a distinct task: source-to-target path prediction. This allows LOGRL to produce molecules with significantly higher Tanimoto similarity to target molecules, even without direct exposure to this metric during training. Furthermore, we incorporate a beam search strategy during the molecule generation process. This strategy empowers us to generate a substantial number of candidate molecules, facilitating further curation to meet desired properties. Notably, our unique approach permits us to leverage the Euclidean distance between learned action representations as a surrogate for molecular similarity during beam search.",10,https://openreview.net/forum?id=rjLgCkJH79,3.6666666666666665,0.9428090415820634,3.3333333333333335,0.4714045207910317
rUH2EDpToF,Generative Marginalization Models,"['discrete generative models', 'marginalization', 'probabilistic models']","[6, 6, 5, 5, 8]","[4, 3, 4, 3, 3]",0,"[441, 355, 489, 379, 479]","We introduce *marginalization models* (MAMs), a new family of generative models
for high-dimensional discrete data. They offer scalable and flexible generative modeling with tractable likelihoods 
by explicitly modeling all induced marginal distributions. Marginalization models enable fast evaluation of arbitrary marginal probabilities with a single forward pass of the neural network, which overcomes a major limitation of methods with exact marginal inference, such as autoregressive models (ARMs). We propose scalable methods for learning the marginals, grounded in the concept of “*marginalization self-consistency*”. Unlike previous methods, MAMs support scalable training of any-order generative models for high-dimensional problems under the setting of *energy-based training*, where the goal is to match the learned distribution to a given desired probability (specified
by an unnormalized (log) probability function such as energy function or reward function). We demonstrate the effectiveness of the proposed model on a variety of discrete data distributions, including binary images, language, physical systems, and molecules, for *maximum likelihood* and *energy-based training* settings. MAMs achieve orders of magnitude speedup in evaluating the marginal probabilities on both settings. For energy-based training tasks, MAMs enable any-order generative modeling of high-dimensional problems beyond the capability of previous methods.",29,https://openreview.net/forum?id=rUH2EDpToF,6.0,1.0954451150103321,3.4,0.4898979485566356
rEQ8OiBxbZ,3D Molecular Pretraining via Localized Geometric Generation,['molecular repsentation; self-supervised learning'],"[3, 3, 3, 3]","[5, 4, 4, 4]",0,"[301, 473, 168, 354]","Self-supervised learning on 3D molecular structures has gained prominence in AI-driven drug discovery due to the high cost of annotating biochemical data. 
However, few have studied the selection of proper modeling semantic units within 3D molecular data, which is critical for an expressive pre-trained model as recognized in natural language processing and computer vision.
In this study, we introduce \textbf{L}ocalized G\textbf{e}ometric \textbf{G}enerati\textbf{o}n (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks , leveraging their simplicity in three-dimension and their prevalence in molecular structural patterns such as carbon skeletons and functional groups.
Inspired by masked language/image modeling, LEGO perturbs a portion of tetrahedrons and learns to reconstruct them in pretraining.
The reconstruction of the noised local structures can be divided into a two-step process, namely spatial orientation prediction and internal arrangement generation.
First, we predict the global orientation of the noised local structure within the whole molecule, equipping the model with positional information for these foundational components.
Then, we geometrically reconstruct the internal arrangements of the noised local structures revealing their functional semantics.
To address the atom-bond inconsistency problem in previous denoising methods and utilize the prior of chemical bonds, we propose to model the graph as a set of nodes and edges and explicitly generate the edges during pre-training.
In this way, LEGO exploits the advantages of encoding structural geometry features as well as leveraging the expressiveness of self-supervised learning.
Extensive experiments on molecular quantum and biochemical property prediction tasks demonstrate the effectiveness of our approach.",7,https://openreview.net/forum?id=rEQ8OiBxbZ,3.0,0.0,4.25,0.4330127018922193
qaJxPhkYtD,Counting Graph Substructures with Graph Neural Networks,"['graph neural networks', 'expressive power', 'representation learning', 'subgraph isomorphism', 'cliques', 'cycles', 'motifs', 'substructures', 'count', 'message-passing']","[6, 6, 6, 6]","[3, 3, 3, 3]",0,"[253, 277, 446, 442]","Graph Neural Networks (GNNs) are powerful representation learning tools that have achieved remarkable performance in various important tasks. However, their ability to count substructures, which play a crucial role in biological and social networks, remains uncertain. In this work, we fill this gap and characterize the representation power of GNNs in terms of their ability to produce powerful representations that count graph substructures. In particular, we study the message-passing operations of GNNs with random stationary input and show that they can produce permutation equivariant representations that are associated with high-order statistical moments. Using these representations, we prove that GNNs can learn how to count cycles, quasi-cliques, and the number of connected components in a graph. We also provide new insights into the generalization capacity of GNNs. Our analysis is constructive and enables the design of a generic GNN architecture that shows remarkable performance in four distinct tasks: cycle detection, cycle counting, graph classification, and molecular property prediction.",24,https://openreview.net/forum?id=qaJxPhkYtD,6.0,0.0,3.0,0.0
qH9nrMNTIW,Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models,"['Diffusion Models', 'Structure-Based Drug Design', 'Molecule Generation']","[6, 5, 8, 6]","[3, 3, 4, 3]",0,"[326, 539, 387, 429]","Generating 3D ligand molecules that bind to specific protein targets via diffusion models has shown great promise for structure-based drug design. The key idea is to disrupt molecules into noise through a fixed forward process and learn its reverse process to generate molecules from noise in a denoising way. However, existing diffusion models primarily focus on incorporating protein-ligand interaction information solely in the reverse process, and neglect the interactions in the forward process. The inconsistency between forward and reverse processes may impair the binding affinity of generated molecules towards target protein. In this paper, we propose a novel Interaction Prior-guided Diffusion model (IPDiff) for the protein-specific 3D molecular generation by introducing geometric protein-ligand interactions into both diffusion and sampling process. Specifically, we begin by pretraining a protein-ligand interaction prior network (IPNet) by utilizing the binding affinity signals as supervision. Subsequently, we leverage the pretrained prior network to (1) integrate interactions between the target protein and the molecular ligand into the forward process for adapting the molecule diffusion trajectories (prior-shifting), and (2) enhance the binding-aware molecule sampling process (prior-conditioning). Empirical studies on CrossDocked2020 dataset show IPDiff can generate molecules with more realistic 3D structures and state-of-the-art binding affinities towards the protein targets, with up to -6.42 Avg. Vina Score, while maintaining proper molecular properties.",24,https://openreview.net/forum?id=qH9nrMNTIW,6.25,1.0897247358851685,3.25,0.4330127018922193
qA4foxO5Gf,Efficient Integrators for Diffusion Generative Models,"['Diffusion models', 'Deep Generative Models', 'Efficient Sampling']","[6, 8, 6, 5]","[4, 3, 2, 5]",0,"[598, 106, 355, 894]","Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey \& Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints will be made publicly available.",25,https://openreview.net/forum?id=qA4foxO5Gf,6.25,1.0897247358851685,3.5,1.118033988749895
ogV88XPnK6,Graph neural processes and their application to molecular functions,"['Neural processes', 'molecules', 'drug discovery', 'meta-learning', 'docking']","[3, 6, 5, 5]","[4, 3, 4, 4]",0,"[497, 211, 567, 142]","Neural processes (NPs) are models for meta-learning which output uncertainty estimates. So far, most studies of NPs have focused on low-dimensional datasets of highly-correlated tasks. While these homogeneous datasets are useful for benchmarking, they may not be representative of realistic transfer-learning. In particular, applications in scientific research may prove especially challenging due to the potential novelty of meta-testing tasks. Drug discovery is one such research area that is characterized by sparse datasets of many functions on a shared molecular space. In this paper, we study the application of graph NPs to drug discovery with DOCKSTRING, a diverse dataset of docking scores. Graph NPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in chemoinformatics, as well as alternative techniques for transfer learning and meta-learning. In order to increase meta-generalization to divergent test functions, we propose fine-tuning strategies that adapt the parameters of NPs. We find that adaptation can substantially increase NPs' regression performance while maintaining good calibration of uncertainty estimates. Finally, we present a Bayesian optimization experiment which showcases the potential advantages of NPs over GPs in molecular applications.",10,https://openreview.net/forum?id=ogV88XPnK6,4.75,1.0897247358851685,3.75,0.4330127018922193
oM7Jbxdk6Z,Multimodal Molecular Pretraining via Modality Blending,"['multimodal molecular pretraining', 'molecular representation learning', 'modality blending']","[6, 5, 8, 5]","[4, 4, 3, 2]",0,"[213, 205, 302, 338]","Self-supervised learning has recently gained growing interest in molecular modeling for scientific tasks such as AI-assisted drug discovery. Current studies consider leveraging both 2D and 3D molecular structures for representation learning. However, relying on straightforward alignment strategies that treat each modality separately, these methods fail to exploit the intrinsic correlation between 2D and 3D representations that reflect the underlying structural characteristics of molecules, and only perform coarse-grained molecule-level alignment. To derive fine-grained alignment and promote structural molecule understanding, we introduce an atomic-relation level ""blend-then-predict"" self-supervised learning approach, MoleBLEND, which first blends atom relations represented by different modalities into one unified relation matrix for joint encoding, then recovers modality-specific information for 2D and 3D structures individually. By treating atom relationships as anchors, MoleBLEND organically aligns and integrates visually dissimilar 2D and 3D modalities of the same molecule at fine-grained atomic level, painting a more comprehensive depiction of each molecule. Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D molecular benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (cross-modality prediction) and mask-then-predict (single-modality prediction) objectives into one single cohesive framework.",23,https://openreview.net/forum?id=oM7Jbxdk6Z,6.0,1.224744871391589,3.25,0.82915619758885
o0C2v4xTdS,CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation,"['conformer generation', 'coarse-grained', 'coarse-graining', '3D molecule generation', 'equivariance', 'SE(3)-equivariance', 'ligand', 'protein-ligand', 'binding affinity', 'structure-based drug discovery', 'variational autoencoder']","[5, 6, 5, 8]","[3, 3, 3, 4]",0,"[180, 281, 258, 332]","Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furthermore, we evaluate the chemical and biochemical quality of our generated conformers on multiple downstream applications, including property prediction and oracle-based protein docking. Overall, CoarsenConf generates more accurate conformer ensembles compared to prior generative models.",17,https://openreview.net/forum?id=o0C2v4xTdS,6.0,1.224744871391589,3.25,0.4330127018922193
nqlymMx42E,Searching for High-Value Molecules Using Reinforcement Learning and Transformers,"['chemistry', 'reinforcement learning', 'language models']","[8, 6, 8, 6]","[4, 4, 2, 2]",0,"[502, 334, 531, 453]","Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",20,https://openreview.net/forum?id=nqlymMx42E,7.0,1.0,3.0,1.0
na7AgFyp1r,Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism,"['Active learning', '3D molecular graphs', 'graph neural networks', 'molecular diversity']","[6, 6, 6]","[2, 4, 2]",0,"[275, 542, 345]","Molecular learning is pivotal in many real-world applications, such as drug discovery. Supervised learning requires heavy human annotation, which is particularly challenging for molecular data, e.g., the commonly used density functional theory (DFT) is computationally very expensive. Active Learning (AL) automatically queries labels for most informative samples, thereby remarkably alleviating the annotation hurdle. In this paper, we present a novel and powerful AL paradigm for molecular learning, where we treat molecules as 3D molecular graphs. Specifically, we propose a new diversity sampling method to eliminate mutual redundancy built on distributions of 3D geometries. We first propose a set of new 3D graph isometrics for 3D graph isomorphism analysis. Our method is provably more powerful than the geometric Weisfeiler-Lehman (GWL) test. The moments of the distributions of the associated geometries are then extracted for efficient diversity computing. To ensure our AL paradigm selects samples with maximal uncertainties, we carefully design a Bayesian geometric graph neural network to compute uncertainties specifically for 3D molecular graphs. We pose active sampling as a quadratic programming (QP) problem using the novel components and conduct extensive experiments on the QM9 dataset. Results demonstrate the effectiveness of our AL paradigm, as well as the proposed diversity and uncertainty methods.",15,https://openreview.net/forum?id=na7AgFyp1r,6.0,0.0,2.6666666666666665,0.9428090415820634
nO344avRib,A Simple and Scalable Representation for Graph Generation,"['Graph generative models', 'graph neural networks', 'graph representation']","[5, 6, 8, 6]","[4, 3, 4, 4]",0,"[235, 483, 805, 448]","Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar. Our findings reveal that the adoption of this compact representation not only enhances scalability but also bolsters performance by simplifying the graph generation process. We conduct a comprehensive evaluation across ten non-attributed and two molecular graph generation tasks, demonstrating the effectiveness of GEEL.",25,https://openreview.net/forum?id=nO344avRib,6.25,1.0897247358851685,3.75,0.4330127018922193
mJgymwRsWw,Active Probabilistic Drug Discovery,"['Drug Discovery', 'Active Learning', 'Molecule Clustering']","[1, 3, 3, 3]","[5, 3, 2, 5]",0,"[568, 783, 173, 926]","Early drug discovery plays a crucial role in the development of new medications
by focusing on the identification and optimization of lead molecules that specif-
ically bind to target proteins. However, this process is accompanied by various
challenges, such as the vastness of molecule libraries, high attrition rate, and the
intricate nature of molecular interactions. To overcome these challenges, there is
a paradigm shift towards integrating intelligence and automation into end-to-end
operations. Intelligent computing aids in the discovery and recommendation of
molecules, while automated experiments offer data validation and feedback. This
innovative approach can be viewed as an active probabilistic learning problem,
assuming that active molecules binding to a specific target are typically a small
proportion and exhibit cluster-distributed characteristics. Based on this formu-
lation, we propose a novel active probabilistic drug discovery (APDD) method,
which iteratively updates the binding probabilities of molecules to progressively
enhance drug discovery performance with three consecutive steps of probabilistic
clustering, selective docking, and active wet-experiment. We conduct extensive
experiments on two benchmark datasets of DUD-E and LIT-PCBA and a simu-
lated virtual library. The results demonstrate the feasibility and efficiency of our
approach, showcasing substantial cost savings with an average reduction of 80% in
computational docking expenses and 70% in wet experimental costs, while main-
taining high accuracy in lead molecule discovery.",4,https://openreview.net/forum?id=mJgymwRsWw,2.5,0.8660254037844386,3.75,1.299038105676658
m9zWBn1Y2j,Ligand Conformation Generation: from singleton to pairwise,"['molecular conformation generation', 'conditional diffusion model', 'graph neural network']","[3, 3, 3]","[2, 3, 4]",0,"[230, 278, 308]","Drug discovery is a time-consuming process, primarily due to the vast number of molecular structures that need to be explored. One of the challenges in drug design involves generating rational ligand conformations. For this task, most previous approaches fall into the singleton category, which solely rely on ligand molecular information to generate ligand conformations. In this work, we contend that the ligand-target interactions are also very important in providing crucial semantics for ligand generation. To address this, we introduce PsiDiff, a comprehensive diffusion model that incorporates target and ligand interactions, as well as ligand chemical properties. By transitioning from singleton to pairwise modeling, PsiDiff offers a more holistic approach. One challenge of the pairwise design is that the ligand-target binding site is not available in most cases and thus hinders the accurate message-passing between the ligand and target. To overcome this challenge, we employ graph prompt learning to bridge the gap between ligand and target graphs. The graph prompt learning of the insert patterns enables us to learn the hidden pairwise interaction at each diffusion step. Upon this, our model leverages the Target-Ligand Pairwise Graph Encoder (TLPE) and captures ligand prompt entity fusion and complex information. Experimental results demonstrate significant improvements in ligand conformation generation, with a remarkable 18\% enhancement in Aligned RMSD compared to the baseline approach.",6,https://openreview.net/forum?id=m9zWBn1Y2j,3.0,0.0,3.0,0.816496580927726
m5xRuGtQEi,Latent Space Simulator for Unveiling Molecular Free Energy Landscapes and Predicting Transition Dynamics,"['molecular dynamics', 'simulation', 'Boltzmann distribution', 'sampling']","[3, 3, 5, 3]","[5, 4, 5, 4]",1,"[2269, 763, 235, 422]","Free Energy Surfaces (FES) and metastable transition rates are key elements in understanding the behavior of molecules within a system. However, the typical approaches require computing force fields across billions of time steps in a molecular dynamics (MD) simulation, which is often considered intractable when dealing with large systems or databases. In this work, we propose LaMoDy, a latent-space MD simulator, to effectively tackle the intractability with around 20-fold speed improvements compared to classical MD. The model leverages a chirality-aware SE(3)-invariant encoder-decoder architecture to generate a latent space coupled with a recurrent neural network to run the time-wise dynamics. We show that LaMoDy effectively recovers realistic trajectories and FES more accurately and faster than existing methods while capturing their major dynamical and conformational properties. Furthermore, the proposed approach can generalize to molecules outside the training distribution.",4,https://openreview.net/forum?id=m5xRuGtQEi,3.5,0.8660254037844386,4.5,0.5
lmYGRGyL4i,Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential,"['Graph generation', 'One-shot generation', 'Autoregressive generation', 'Unified framework', 'Diffusion Model', 'Molecule generation']","[5, 5, 6, 3, 5]","[4, 4, 3, 3, 3]",0,"[306, 381, 308, 497, 195]","In the field of deep graph generative models, two families coexist: one-shot models, which fill the graph content in one go given a number of nodes, and sequential models, where new nodes and edges are inserted sequentially and autoregressively. Recently, one-shot models are seeing great popularity due to their rising sample quality and lower sampling time compared to the more costly autoregressive models. With this paper we unify the two worlds in a single framework, unlocking the whole spectrum of options where one-shot and sequential models are but the two extremes. We use the denoising diffusion models' theory to develop a node removal process, which destroys a given graph through many steps. An insertion model reverses this process by predicting how many nodes have been removed from the intermediate subgraphs. Then, generation happens by iteratively adding new blocks of nodes, with size sampled from the insertion model, and content generated using any one-shot model. By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS. Based on this, we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets. As a case study, we adapt DiGress, a diffusion-based one-shot model, to the whole spectrum of sequentiality, reaching new state of the art results, and motivating a renewed interest in developing autoregressive graph generative models.",12,https://openreview.net/forum?id=lmYGRGyL4i,4.8,0.9797958971132712,3.4,0.4898979485566356
liKkG1zcWq,Sliced Denoising: A Physics-Informed Molecular Pre-Training Method,['Molecule Representation; Pretraining; Denoising; Force Field'],"[8, 5, 5, 8]","[5, 4, 2, 3]",0,"[448, 254, 270, 450]","While molecular pre-training has shown great potential in enhancing drug discovery, the lack of a solid physical interpretation in current methods raises concerns about whether the learned representation truly captures the underlying explanatory factors in observed data, ultimately resulting in limited generalization and robustness. Although denoising methods offer a physical interpretation, their accuracy is often compromised by ad-hoc noise design, leading to inaccurate learned force fields. To address this limitation, this paper proposes a new method for molecular pre-training, called sliced denoising (SliDe), which is based on the classical mechanical intramolecular potential theory. SliDe utilizes a novel noise strategy that perturbs bond lengths, angles, and torsion angles to achieve better sampling over conformations. Additionally, it introduces a random slicing approach that circumvents the computationally expensive calculation of the Jacobian matrix, which is otherwise essential for estimating the force field. By aligning with physical principles, SliDe shows a 42\% improvement in the accuracy of estimated force fields compared to current state-of-the-art denoising methods, and thus outperforms traditional baselines on various molecular property prediction tasks.",19,https://openreview.net/forum?id=liKkG1zcWq,6.5,1.5,3.5,1.118033988749895
ledQ1BCrwc,GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?,"['graph generation', 'large attributed graphs', 'denoising diffusion model', 'discrete diffusion', 'graph neural networks']","[5, 3, 5]","[5, 4, 4]",0,"[620, 314, 414]","Large-scale graphs with node attributes are fundamental in real-world scenarios, such as social and financial networks. The generation of synthetic graphs that emulate real-world ones is pivotal in graph machine learning, aiding network evolution understanding and data utility preservation when original data cannot be shared. Traditional models for graph generation suffer from limited model capacity. Recent developments in diffusion models have shown promise in merely graph structure generation or the generation of small molecular graphs with attributes. However, their applicability to large attributed graphs remains unaddressed due to challenges in capturing intricate patterns and scalability. This paper introduces GraphMaker, a novel diffusion model tailored for generating large attributed graphs. We study the diffusion models that either couple or decouple graph structure and node attribute generation to address their complex correlation. We also employ node-level conditioning and adopt a minibatch strategy for scalability. We further propose a new evaluation pipeline using models trained on generated synthetic graphs and tested on original graphs to evaluate the quality of synthetic data. Empirical evaluations on real-world datasets showcase GraphMaker's superiority in generating realistic and diverse large-attributed graphs beneficial for downstream tasks.",11,https://openreview.net/forum?id=ledQ1BCrwc,4.333333333333333,0.9428090415820634,4.333333333333333,0.4714045207910317
kzGuiRXZrQ,Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation,"['Generative Modelling', 'Molecule Design', 'Denoising Diffusion Probabilistic Models', 'Ablation Study', 'Equivariant Graph Neural Network', '3D Molecule Generation', 'Diffusion Model']","[3, 6, 8, 6]","[4, 4, 4, 3]",0,"[278, 404, 843, 175]","Deep generative diffusion models are a promising avenue for 3D $\textit{de novo}$ molecular design in material science and drug discovery. 
However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. 
Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. 
Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. 
Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin.
Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples.
To further strengthen the applicability of diffusion models to limited training data, we examine the transferability of EQGAT-diff trained on the large PubChem3D dataset with implicit hydrogens to target distributions with explicit hydrogens. Fine-tuning EQGAT-diff for a couple of iterations further pushes state-of-the-art performance across datasets.
We envision that our findings will find applications in structure-based drug design, where the accuracy of generative models for small datasets of complex molecules is critical.",24,https://openreview.net/forum?id=kzGuiRXZrQ,5.75,1.7853571071357126,3.75,0.4330127018922193
kkQSwtx0p3,Leveraging Task Structures for Improved Identifiability in Neural Network Representations,"['multi-task learning', 'identifiability', 'representation learning', 'causality', 'molecules']","[6, 5, 5, 5]","[3, 2, 3, 4]",0,"[224, 134, 561, 903]","This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to linear identifiability in the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent factors reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result than linear identifiability. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for both synthetic and real-world molecular data.",15,https://openreview.net/forum?id=kkQSwtx0p3,5.25,0.4330127018922193,3.0,0.7071067811865476
kQPAAXRswY,Stabilizing Policy Gradients for Stochastic Differential Equations by enforcing Consistency with Perturbation Process,['stochastic differential equation; reinforcement learning; diffusion models'],"[6, 5, 6]","[1, 3, 2]",0,"[292, 446, 230]","Deep neural networks parameterized stochastic differential equations (SDEs) received increasing attention from the machine learning community due to their high expressiveness and solid theoretical foundations, with a wide range of applications in generative models. However, maximizing likelihood of training data, the objective of generative models, does not always meet our requirements in many real-world problems. Fortunately, introducing reinforcement learning (e.g., policy gradient) here to maximize a reward, using SDE-based policy, may bridge this gap. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. These challenges compromise the stability of policy gradients and negatively impact sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach for training SDEs using policy gradients, allowing for a versatile selection of policy gradients to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score (-9.07)  on the CrossDocked2020 dataset.",15,https://openreview.net/forum?id=kQPAAXRswY,5.666666666666667,0.4714045207910317,2.0,0.816496580927726
kKXIYUi8ff,DynamicsDiffusion: Generating and Rare Event Sampling of Molecular Dynamic Trajectories Using Diffusion Models,"['Diffusion Models', 'DDPM', 'Molecular Dynamics', 'Physics']","[3, 3, 3, 3, 3]","[5, 5, 4, 5, 4]",0,"[415, 2128, 743, 88, 325]","Molecular dynamics simulations are fundamental tools for quantitative molecular sciences. However, these simulations are computationally demanding and often struggle to sample rare events crucial for understanding spontaneous organization and reconfiguration in complex systems. To improve general speed and the ability to sample rare events in a directed fashion, we propose a method called $\textit{DynamicsDiffusion}$ based on denoising diffusion probabilistic models (DDPM) to generate molecular dynamics trajectories from noise. The generative model can then serve as a surrogate to sample rare events. We leverage the properties of DDPMs, such as conditional generation, the ability to generate variations of trajectories, and those with certain conditions, such as crossing from one state to another, using the 'inpainting' property of DDPMs, which became only applicable when generating whole trajectories and not just individual conformations. To our knowledge, this is the first deep generative modeling for generating molecular dynamics trajectories. We hope this work will motivate a new generation of generative modeling for the study of molecular dynamics.",9,https://openreview.net/forum?id=kKXIYUi8ff,3.0,0.0,4.6,0.4898979485566356
jJCeMiwHdH,BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph,"['drug discovery', 'foundation model', 'multi-modal learning', 'knowledge graph']","[8, 6, 6, 8]","[5, 3, 4, 4]",0,"[519, 302, 479, 476]","Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone.
To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs.
Our empirical results demonstrate that BioBridge can
beat the best baseline KG embedding methods (on average by $\sim 76.3\%$) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBridge presents itself as a general-purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.",19,https://openreview.net/forum?id=jJCeMiwHdH,7.0,1.0,4.0,0.7071067811865476
igUP5kQRij,Retrosynthesis Prediction via Search in (Hyper) Graph,['Retrosynthesis Prediction via \\\\ Search in (Hyper) Graph'],"[5, 5, 5]","[5, 4, 4]",1,"[376, 540, 1188]","Retrosynthesis prediction is a fundamental challenge in organic synthesis, involving the prediction of reactants based on a given core product. Recently, semi-template-based methods and graph-edits-based methods have achieved good performance in interpretability and accuracy. However, their mechanisms still fail to predict complex reactions, e.g., reactions with multiple reaction center or attaching the same leaving group to more than one atom. Hence, we propose, a semi-template-based method, the \textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph (RetroSiG) framework to alleviate these limitations. In this paper, we cast the reaction center identification and the leaving group completion as search in the product molecular graph and leaving group hypergraph respectively. RetroSiG has several advantages as a semi-template-based method: First, RetroSiG is able to handle the complex reactions mentioned above with its novel search mechanism. Second, RetroSiG naturally exploits the hypergraph to model the implicit dependencies between leaving groups. Third, RetroSiG makes full use of the prior, i.e., one-hop constraint. It reduces the search space and enhances overall performance. Comprehensive experiments demonstrate that RetroSiG achieves a competitive result. Furthermore, we conduct experiments to show the capability of RetroSiG in predicting complex reactions. Ablation experiments verify the effectiveness of individual components, including the one-hop constraint and the leaving group hypergraph.",3,https://openreview.net/forum?id=igUP5kQRij,5.0,0.0,4.333333333333333,0.4714045207910317
h7DGnWGeos,Active Retrosynthetic Planning Aware of Route Quality,"['Retrosynthetic planning', 'route evaluation', 'reinforcement learning']","[6, 8, 6, 6]","[4, 4, 3, 5]",0,"[1032, 273, 444, 241]","Retrosynthetic planning is a sequential decision-making process of identifying synthetic routes from the available building block materials to reach a desired target molecule.
Though existing planning approaches show promisingly high solving rates and low costs, the trivial route cost evaluation via pre-trained forward reaction prediction models certainly falls short of real-world chemical practice.
An alternative option is to annotate the actual cost of a route, such as yield, through chemical experiments or input from chemists, while 
this often leads to substantial query costs.
In order to strike the balance between query costs and route quality evaluation, we propose an Active Retrosynthetic Planning (ARP) framework that remains compatible with the established retrosynthetic planners.
On one hand, the proposed ARP trains an actor that decides whether to query the cost of a reaction; on the other hand, it resorts to a critic to estimate the value of a molecule with its preceding reaction cost as input. 
Those molecules with low reaction costs are preferred to expand first.
We apply our framework to different existing approaches on both the benchmark and an expert dataset and demonstrate that it outperforms the existing state-of-the-art approach by 6.2\% in route quality while reducing the query cost by 12.8\%.
In addition, 
ARP consistently plans 
high-quality routes with either abundant or sparse annotations.",22,https://openreview.net/forum?id=h7DGnWGeos,6.5,0.8660254037844386,4.0,0.7071067811865476
gOuWPd4f2U,Multil-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination,"['Multimodal learning', 'Meta Learning', 'Graph representation learning', 'Contrastive learning', 'Knowledge-Guided Instance-Wise Discrimination']","[3, 5, 5]","[3, 3, 2]",1,"[389, 358, 201]","In multimodal alignment, meta-alignment and multi-level alignment play important roles. However, it is challenging to integrate meta-alignment into a multi-level multimodal alignment framework involving the operation on both reducible substances (e.g., molecules and spectrum) and irreducible elements (e.g., atoms and spectral peaks). It not only inherits the challenges from meta-alignment (e.g., heterogeneity, loss of nuance, interference, and conflicting similarities) but also introduces new challenges: navigating the interactions among reducible substances and irreducible elements and recognizing objects at each level. Many existing alignment methods suffer from inaccurate component relation estimation and potential bias, as they hold manual definitions of pair closeness. In response, we introduce Multi-level Multimodal Alignment with Knowledge-guided Instance-wise Discrimination (K-M3AID), an innovative approach that utilizes continuous knowledge variables with inherent natural ordering for meta-alignment. K-M3AID effectively addresses these challenges by promoting both reliable distance learning and unbiased alignment within the context of cross-modality alignment for multi-level structures. Extensive empirical studies conducted on complex molecular structures underscore the substantial efficacy of K-M3AID. It significantly improves matching accuracy while augmenting multi-level alignment capabilities. This novel approach holds great promise for advancing alignment techniques across diverse molecular contexts, offering a more robust foundation for ongoing research in chemical analysis and beyond.",3,https://openreview.net/forum?id=gOuWPd4f2U,4.333333333333333,0.9428090415820634,2.6666666666666665,0.4714045207910317
gBV21wK07P,3D Autoencoding Diffusion Model for Molecule Interpolation and Manipulation,"['diffusion models', '3D molecule optimization', 'controllable generation', 'equivariant GNN']","[3, 3, 3, 5]","[5, 4, 3, 2]",0,"[550, 438, 448, 256]","Manipulating known molecules and interpolating between them is useful for many applications in drug design and protein engineering, where exploration around the molecular templates is involved. Recent studies using equivariant diffusion models have made significant progress in the de novo generation of high-quality molecules, but using these models to directly manipulate a specified template remains less explored. This is mainly due to an intrinsic property of diffusion models: the lack of a latent semantic space that is easy to operate on. To address this issue, we propose the first semantics-guided equivariant diffusion model that leverages the “semantic” embedding of a 3D molecule, learned from an auxiliary encoder, to control the generative denoising process. By modifying the embedding, we can steer the generation towards another specified molecule or a desired molecular property. We show that our model can effectively manipulate basic chemical properties, outperforming several baselines. We further verify that our approach can achieve smoother interpolation between 3D molecular pairs compared to standard diffusion models.",10,https://openreview.net/forum?id=gBV21wK07P,3.5,0.8660254037844386,3.5,1.118033988749895
g0fHn95m3D,Text-To-Energy: Accelerating Quantum Chemistry Calculations through Enhanced Text-to-Vector Encoding and Orbital-Aware Multilayer Perceptron,"['qantum mechanics', 'quantum chemistry', 'computational physics', 'density functional theory', 'material science', 'physics-informed machine learning']","[3, 1, 1, 8]","[5, 4, 5, 4]",0,"[143, 209, 301, 403]","Accurately predicting material properties remains a complex and computationally intensive task. In this work, we introduce Text-To-Energy (T2E), a novel approach combining text-to-vector encoding and a multilayer perceptron (MLP) for rapid and precise energy predictions. T2E begins by converting pivotal material attributes to a vector representation, followed by the utilization of an MLP block incorporating significant physical data. This novel integration of textual, physical, and quantum insights enables T2E to swiftly and accurately predict the total energy of material systems. The proposed methodology marks a significant departure from conventional computational techniques, offering a reduction in computational burden, which is imposed by particle count and their interactions, obviating the need for extensive quantum chemistry expertise. Comprehensive validation across a diverse range of atoms and molecules affirms the superior performance of T2E over state-of-the-art solutions such as DFT, FermiNet, and PsiFormer.",9,https://openreview.net/forum?id=g0fHn95m3D,3.25,2.8613807855648994,4.5,0.5
fxQiecl9HB,Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding,"['Materials Science', 'Attention Networks', 'Transformer', 'Physics-Informed ML']","[5, 8, 8, 8]","[4, 4, 3, 4]",0,"[434, 692, 230, 454]","Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable and physically interpretable formulation. We then propose a simple yet effective transformer-based encoder architecture for crystal structures called Crystalformer. Compared with an existing transformer-based model, the proposed model requires only 38% of number of parameters per attention block. Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets.",20,https://openreview.net/forum?id=fxQiecl9HB,7.25,1.299038105676658,3.75,0.4330127018922193
fmoknhh7CH,Harmonic Prior Flow Matching for Multi-Ligand Docking and Binding Site Design,"['flow matching', 'generative models', 'proteins', 'molecules']","[5, 5, 6, 5, 5]","[4, 5, 4, 5, 3]",0,"[255, 256, 108, 248, 416]","A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design.",22,https://openreview.net/forum?id=fmoknhh7CH,5.2,0.39999999999999997,4.2,0.7483314773547882
fj2E5OcLFn,Stochastic Gradient Descent for Gaussian Processes Done Right,"['Gaussian process', 'stochastic gradient descent']","[8, 6, 8, 5, 5]","[4, 4, 3, 3, 3]",0,"[297, 377, 241, 313, 304]","We study the optimisation problem associated with Gaussian process regression using squared loss. 
The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly on the problem or on a reduced-order version of it. 
However, stochastic gradient descent has recently gained traction in the Gaussian process literature, driven largely by its successes in deep learning. In this paper, we show that this approach when done right---by which we mean using specific insights from the optimisation and kernel communities---is highly effective.
We thus introduce a particular stochastic dual gradient descent algorithm, conveniently implementable with a few lines of code using any deep learning framework. 
We explain our design decisions by illustrating their advantage against alternatives with ablation studies.
We then show that the new method is highly competitive: our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from conjugate gradients, variational Gaussian process approximations, and a prior version of stochastic gradient descent tailored for Gaussian processes. 
On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with graph neural networks.",19,https://openreview.net/forum?id=fj2E5OcLFn,6.4,1.3564659966250536,3.4,0.4898979485566356
fihkcXeG7N,Gate-guided and subgraph-aware Bilateral Fusion for Molecular Property Prediction,['Molecular property prediction'],"[5, 3, 1, 3]","[4, 5, 5, 5]",0,"[315, 337, 300, 653]","Predicting molecular properties is crucial in scientific research and industry applications.  Molecules are often modeled as graphs where atoms and chemical bonds are represented as nodes and edges, respectively, and Graph Neural Networks (GNNs) have been commonly utilized to predict atom-related properties, such as reactivity and solubility. However, some properties, such as efficacy, and metabolic properties, are closely related to functional groups (subgraphs), which cannot be solely determined by individual atoms. In this paper, we introduce the Gate-guided and Subgraph-aware Bilateral Fusion (GSBF) model for molecular property prediction. GSBF overcomes the limitations of prior atom-wise and subgraph-wise models by integrating both types of information into two distinct branches within the model. We provide a gate-guided mechanism to control the utilization of two branches. Considering existing atom-wise GNNs cannot properly extract invariant subgraph features, we propose a decomposition-polymerization GNN architecture for the subgraph-wise branch. Furthermore, we propose cooperative node-level and graph-level self-supervised learning strategies for GSBF to improve its generalization. Our method offers a more comprehensive way to learn representations for molecular property prediction. Extensive experiments have demonstrated the effectiveness of our method.",4,https://openreview.net/forum?id=fihkcXeG7N,3.0,1.4142135623730951,4.75,0.4330127018922193
dn87xnULwF,Maximally Expressive GNNs for Outerplanar Graphs,"['expressive graph representation learning', 'outerplanar graphs']","[6, 3, 6, 5, 5]","[3, 5, 3, 4, 3]",0,"[273, 582, 282, 714, 382]","We propose a _linear time_ graph transformation that enables the Weisfeiler-Leman (WL) test and message passing graph neural networks (MPNNs) to be maximally expressive on _outerplanar_ graphs. Our approach is motivated by the fact that most pharmaceutical molecules correspond to outerplanar graphs. Existing research predominantly enhances the expressivity of graph neural networks without specific graph families in mind. This often leads to methods that are impractical due to their computational complexity. In contrast, the restriction to outerplanar graphs enables us to encode the Hamiltonian cycle of each biconnected component in linear time. As the main contribution of the paper we prove that our method achieves maximum expressivity on outerplanar graphs. Experiments confirm that our graph transformation improves the predictive performance of MPNNs on molecular benchmark datasets at negligible computational overhead.",24,https://openreview.net/forum?id=dn87xnULwF,5.0,1.0954451150103321,3.6,0.8
dl0u4ODCuW,Retro-fallback: retrosynthetic planning in an uncertain world,"['Retrosynthesis', 'planning', 'chemistry', 'search']","[6, 6, 6]","[4, 3, 4]",0,"[178, 350, 459]","Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",16,https://openreview.net/forum?id=dl0u4ODCuW,6.0,0.0,3.6666666666666665,0.4714045207910317
dUTwqiEked,RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation,"['Retrosynthesis', 'Diffusion Model']","[6, 3, 3, 5]","[4, 3, 4, 5]",0,"[298, 457, 1145, 123]","Retrosynthesis poses a fundamental challenge in biopharmaceuticals, aiming to aid chemists in finding appropriate reactant molecules and synthetic pathways given determined product molecules. With the reactant and product represented as 2D graphs, retrosynthesis constitutes a conditional graph-to-graph generative task.
Inspired by the recent advancements in discrete diffusion models for graph generation, we introduce RetroSynthesis Diffusion (RetroDiff), a novel diffusion-based method designed to address this problem. 
However, integrating a diffusion-based graph-to-graph framework while retaining essential chemical reaction template information presents a notable challenge.
Our key innovation is to develop a multi-stage diffusion process. In this method, we decompose the retrosynthesis procedure to first sample external graph motifs from the dummy distribution given products and then generate the external bonds to connect the products and generated motifs. Interestingly, such a generation process is exactly the reverse of the widely adapted semi-template retrosynthesis procedure, i.e. from reaction center identification to synthon completion, which significantly reduces the error accumulation. 
Experimental results on the benchmark have demonstrated the superiority of our method over all other semi-template methods.",16,https://openreview.net/forum?id=dUTwqiEked,4.25,1.299038105676658,4.0,0.7071067811865476
dPHLbUqGbr,"Fast, Expressive $\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space","['Equivariance', 'Point Clouds', 'Message Passing Neural Network', 'Molecules', 'Diffusion Model']","[5, 6, 8]","[2, 4, 3]",0,"[114, 244, 113]","Based on the theory of homogeneous spaces we derive geometrically optimal edge attributes to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 {\times} S^2$, and the group $\mathrm{SE}(3)$ itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to the ability to represent directional information, which $\mathbb{R}^3$ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full $\mathrm{SE}(3)$ group. We empirically support this claim by reaching state-of-the-art results --in accuracy and speed-- on three different benchmarks: interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models.",7,https://openreview.net/forum?id=dPHLbUqGbr,6.333333333333333,1.247219128924647,3.0,0.816496580927726
dFQL7nwksh,PDC-Net: Probability Density Cloud Representations of Proteins for Mutation Effect Prediction,"['Protein-protein interaction', 'Thermodynamics', 'Geometric Deep Learning']","[5, 3, 3, 6]","[2, 3, 3, 4]",1,"[274, 416, 862, 477]","Understanding the ramifications of mutations at a protein level can have significant implications in various domains such as drug development, disease pathways, and the broader field of genomics. Despite the promise of data-driven and deep learning (DL) strategies, existing algorithms still face a significant challenge in integrating the dynamic changes of biomolecules to accurately predict protein-protein interaction binding affinity changes following mutations ($\Delta \Delta G$). Within this study, we introduce an inventive approach aimed at capturing the equilibrium fluctuations and discerning induced conformational changes at the interface, which is particularly important for forecasting mutational effects on binding. This novel technique harnesses probability density clouds (PDC) to describe the magnitude and intensity of their movement during and after the binding process and puts forth aligned networks to propagate distributions of the equilibrium of molecular systems. To fully unleash the potential of PDC-Net, we further present two physics-inspired pretraining tasks to employ the molecular dynamics (MD) simulation trajectories and the extensive collection of static crystal protein structures. Experiments demonstrate that our approach surpasses the performance of both empirical energy functions and alternative DL methods.",4,https://openreview.net/forum?id=dFQL7nwksh,4.25,1.299038105676658,3.0,0.7071067811865476
cXbnGtO0NZ,Latent 3D Graph Diffusion,"['3D graphs', 'latent diffusion models', 'in/equivariant representations']","[6, 6, 6, 5, 6, 8]","[3, 5, 4, 3, 3, 4]",0,"[426, 608, 618, 582, 265, 236]","Generating 3D graphs of \textit{symmetry-group equivariance} is of intriguing potential in broad applications from machine vision to molecular discovery.
Emerging approaches adopt diffusion generative models (DGMs) with proper re-engineering to capture 3D graph distributions.
In this paper, we raise an orthogonal and fundamental question of \textit{in what (latent) space we should diffuse 3D graphs}.
\ding{182} We motivate the study with theoretical analysis showing that the performance bound of 3D graph diffusion could be improved in a latent space versus the original space, provided that there are (i) low dimensionality yet (ii) high quality (i.e., low reconstruction error) of the latent space, and (iii) symmetry preservation as an inductive bias of latent DGMs.
\ding{183} Guided by the theoretical guidelines, we propose to perform 3D graph diffusion in a low-dimensional latent space, which is learned through cascaded 2D--3D graph autoencoders for low-error reconstruction and symmetry-group invariance.
The overall pipeline is dubbed \textbf{latent 3D graph diffusion}.
\ding{184} Motivated by applications in molecular discovery,  we further extend latent 3D graph diffusion to conditional generation given SE(3)-invariant attributes or equivariant 3D objects.
\ding{185} We also demonstrate empirically that out-of-distribution conditional generation can be further improved by regularizing the latent space via graph self-supervised learning.
We validate through comprehensive experiments that our method generates 3D molecules of higher validity / drug-likeliness and comparable conformations / energetics, while being an order of magnitude faster in training. Codes will be released upon acceptance.",22,https://openreview.net/forum?id=cXbnGtO0NZ,6.166666666666667,0.8975274678557507,3.6666666666666665,0.7453559924999298
c4JNoRRNtV,CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations,"['Neural Representation', 'Cryo-EM reconstruction', 'Structural Biology']","[3, 5, 5, 6]","[4, 5, 3, 2]",1,"[382, 566, 352, 170]","Cryo-electron microscopy (cryo-EM) allows for the high-resolution reconstruction of 3D structures of proteins and other biomolecules. Successful reconstruction of both shape and movement greatly helps understand the fundamental processes of life. However, it is still challenging to reconstruct the continuous motions of 3D structures from hundreds of thousands of noisy and randomly oriented 2D cryo-EM images. While recent advancements using Fourier domain coordinate-based neural networks show compelling results in modeling continuous 3D conformations, these methods often struggle to capture local flexible regions accurately. We propose CryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction. Our approach constructs an implicit feature volume directly in the real domain as the 3D representation. We also design a query-based deformation transformer decoder to effectively predict the density. Our approach is capable of refining pre-computed pose estimations and locating flexible regions. In experiments, our method outperforms current approaches on three public datasets (1 synthetic and 2 experimental) and a new synthetic dataset of PEDV spike protein. The code and new synthetic dataset will be released for better reproducibility of our results.",8,https://openreview.net/forum?id=c4JNoRRNtV,4.75,1.0897247358851685,3.5,1.118033988749895
bgIZDxd2bM,"Generation, Reconstruction, Representation All-in-One: A Joint Autoencoding Diffusion Model","['diffusion model', 'generative model', 'latent model']","[6, 3, 3]","[3, 4, 3]",1,"[287, 425, 400]","The vast applications of deep generative models are founded on the premise of three fundamental capabilities: generating new instances (e.g., image/text synthesis and molecule design), reconstructing inputs (e.g., data editing and restoration), and learning latent representations (e.g., structure discovery and downstream classification). Existing model families, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities but fall short in others. We introduce Joint Autoencoding Diffusion (JEDI), a new generative framework that unifies all three core capabilities, offering versatile applications and strong performance in a single model. Specifically, JEDI generalizes the noising/denoising transformations (based on simple Gaussian noise) in diffusion process by introducing parameterized encoder/decoder transformations between raw data and compact representations. Crucially, the encoder/decoder parameters are learned jointly with all other diffusion model parameters under the standard probabilistic diffusion formalism. This results in a model that not only inherits the strong generation abilities of diffusion models but also enables compact data representation and faithful reconstruction. Additionally, by choosing appropriate encoder/decoder, JEDI can naturally accommodate discrete data (such as text and protein sequences) which have been difficult for diffusion models. Extensive experiments across different data modalities, including images, text, and proteins, demonstrate JEDI's general applicability to diverse tasks and strong improvement over existing specialized deep generative models.",3,https://openreview.net/forum?id=bgIZDxd2bM,4.0,1.4142135623730951,3.3333333333333335,0.4714045207910317
bZHz9WYs9z,Molecule Generation by Heterophilious Triple Flows,"['Molecule generation', 'graph neural networks', 'heterophily', 'generative models']","[6, 3, 5, 5]","[3, 4, 3, 3]",0,"[245, 278, 443, 332]","Generating molecules with desirable properties is key to domains like material design and drug discovery. The predominant approach is to encode molecular graphs using graph neural networks or their continuous-depth analogues. However, these methods often implicitly assume strong homophily (i.e., affinity) between neighbours, overlooking repulsions between dissimilar atoms and making them vulnerable to oversmoothing. To address this, we introduce HTFlows. It uses multiple interactive flows to capture heterophily patterns in the molecular space and harnesses these (dis-)similarities in generation, consistently showing good performance on chemoinformatics benchmarks.",17,https://openreview.net/forum?id=bZHz9WYs9z,4.75,1.0897247358851685,3.25,0.4330127018922193
aqTipMg9CZ,Contextual Molecule Representation Learning from Chemical Reaction Knowledge,"['Self-supervised Pre-training', 'Chemical Reaction', 'Molecular Representation Learning']","[5, 8, 5, 3]","[5, 4, 4, 4]",0,"[397, 490, 605, 277]","Self-supervised learning has emerged as a powerful tool for harnessing large amounts of unlabelled data to obtain meaningful representations. However, prevailing techniques such as reconstructing masked sub-units are inapplicable to Molecular Representation Learning (MRL) due to the high degree of freedom in possible combinations of atoms in molecules. In this work, we propose a self-supervised learning framework, \textit{REMO}, which pre-trains graph/Transformer encoders on 1.7 million chemical reactions by taking advantage of well-defined rules of atom combinations in common chemical reactions. Specifically, two pre-training objectives are proposed, including masked reaction centre reconstruction and reaction centre identification. \textit{REMO} offers a novel solution to MRL by leveraging the unique characteristics of chemical reactions as knowledge context for pre-training, which effectively supports diverse downstream molecular tasks with minimum finetuning. Experimental results show that \textit{REMO} outperforms masked modeling of single molecule in various downstream tasks.",14,https://openreview.net/forum?id=aqTipMg9CZ,5.25,1.7853571071357126,4.25,0.4330127018922193
afQuNt3Ruh,Entropy Coding of Unordered Data Structures,"['graph compression', 'entropy coding', 'neural compression', 'bits-back coding', 'lossless compression', 'generative models', 'information theory', 'probabilistic models', 'graph neural networks', 'multiset compression', 'asymmetric numeral systems', 'compression', 'entropy', 'shuffle coding']","[8, 6, 6, 5]","[4, 3, 3, 2]",0,"[347, 230, 1440, 281]","We present shuffle coding, a general method for optimal compression of sequences of unordered objects using bits-back coding. Data structures that can be compressed using shuffle coding include multisets, graphs, hypergraphs, and others. We release an implementation that can easily be adapted to different data types and statistical models, and demonstrate that our implementation achieves state-of-the-art compression rates on a range of graph datasets including molecular data.",13,https://openreview.net/forum?id=afQuNt3Ruh,6.25,1.0897247358851685,3.0,0.7071067811865476
Zc2aIcucwc,Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets,"['graph neural networks', 'Datasets', 'molecules', 'molecular graphs', 'Quantum', 'Multi-task', 'foundation model']","[8, 6, 8, 6, 3]","[3, 4, 2, 4, 4]",0,"[382, 619, 225, 385, 491]","Recently, pre-trained foundation models have enabled significant advancements in multiple fields. 
In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models.
In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset.
In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. 
Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.",20,https://openreview.net/forum?id=Zc2aIcucwc,6.2,1.8330302779823362,3.4,0.8
Y3BbxvAQS9,DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization,"['Structure-based drug design', 'molecule optimization', 'diffusion models', '3D molecule generation']","[6, 5, 6, 8, 6]","[4, 4, 3, 3, 3]",0,"[303, 344, 253, 217, 191]","Recently, 3D generative models have shown promising performances in structure-based drug design by learning to generate ligands given target binding sites. However, only modeling the target-ligand distribution can hardly fulfill one of the main goals in drug discovery -- designing novel ligands with desired properties, e.g., high binding affinity, easily synthesizable, etc. This challenge becomes particularly pronounced when the target-ligand pairs used for training do not align with these desired properties. Moreover, most existing methods aim at solving de novo design task, while many generative scenarios requiring flexible controllability, such as R-group optimization and scaffold hopping, have received little attention. In this work, we propose DecompOpt, a structure-based molecular optimization method based on a controllable and decomposed diffusion model. DecompOpt presents a new generation paradigm which combines optimization with conditional diffusion models to achieve desired properties while adhering to the molecular grammar. Additionally, DecompOpt offers a unified framework covering both de novo design and controllable generation. To achieve so, ligands are decomposed into substructures which allows fine-grained control and local optimization. Experiments show that DecompOpt can efficiently generate molecules with improved properties than strong de novo baselines, and demonstrate great potential in controllable generation tasks.",20,https://openreview.net/forum?id=Y3BbxvAQS9,6.2,0.9797958971132712,3.4,0.4898979485566356
Y2Txh5uGRe,Text2Data: Low-Resource Data Generation with Textual Control,"['low resource', 'text-to-data generation']","[6, 3, 5, 3]","[2, 4, 3, 2]",0,"[334, 404, 372, 309]","Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, like molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding both generation quality and controllability across various modalities, including molecules, motions and time series, when compared to existing baselines.",12,https://openreview.net/forum?id=Y2Txh5uGRe,4.25,1.299038105676658,2.75,0.82915619758885
XSwxy3bojg,Generating Molecular Conformer Fields,"['diffusion model', 'molecular conformations', 'fields']","[5, 6, 3, 3, 5]","[4, 3, 4, 4, 4]",0,"[1184, 342, 987, 416, 203]","In this paper we tackle the problem of generating conformers of a molecule in 3D space given its molecular graph. We parameterize these conformers as continuous functions that map elements from the molecular graph to points in 3D space. We then formulate the problem of learning to generate conformers as learning a distribution over these functions using a diffusion generative model, called Molecular Conformer Fields (MCF). Our approach is simple and scalable, and obtains results that are comparable or better than the previous state-of-the-art while making no assumptions about the explicit structure of molecules (\eg modeling torsional angles). MCF represents an advance in extending diffusion models to handle complex scientific problems in a conceptually simple, scalable and effective manner.",26,https://openreview.net/forum?id=XSwxy3bojg,4.4,1.2,3.8,0.39999999999999997
X7gqOBG8ow,Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields,"['equivariant neural networks', 'graph neural networks', 'computational physics']","[6, 5, 5, 5, 5]","[2, 3, 4, 4, 4]",0,"[668, 261, 608, 694, 463]","Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on pre-training via denoising, which are limited to equilibrium structures, the proposed DeNS generalizes to a much larger set of non-equilibrium structures without relying on another dataset for pre-training. The key enabler is the encoding of input forces. A non-equilibrium structure has non-zero forces and thus many possible atomic positions, making denoising an ill-posed problem. To address the issue, we additionally take the forces of the original structure as inputs to specify which non-equilibrium structure we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings.

We demonstrate the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets. For OC20, EquiformerV2 trained with DeNS achieves better S2EF results and comparable IS2RE results compared to EquiformerV2 trained without DeNS. For OC22, EquiformerV2 trained with DeNS establishs new state-of-the-art results. For MD17, Equiformer ($L_{max} = 2$) trained
with DeNS achieves better results than Equiformer ($L_{max} = 3$) without DeNS and saves 3.1$\times$ training time. We also show that DeNS can improve other equivariant networks like eSCN on OC20 and SEGNN-like networks on MD17.",17,https://openreview.net/forum?id=X7gqOBG8ow,5.2,0.39999999999999997,3.4,0.8
X41c4uB4k0,Training-free Multi-objective Diffusion Model for 3D Molecule Generation,"['Multi-objective Diffusion Model', '3D Molecule']","[8, 6, 6, 6]","[3, 4, 4, 2]",0,"[405, 574, 184, 71]","Searching for novel and diverse molecular candidates is a critical undertaking in drug and material discovery. Existing approaches have successfully adapted the diffusion model, the most effective generative model in image generation, to create 1D SMILES strings, 2D chemical graphs, or 3D molecular conformers. However, these methods are not efficient and flexible enough to generate 3D molecules with multiple desired properties, as they require additional training for the models for each new property or even a new combination of existing properties. Moreover, some properties may potentially conflict, making it impossible to find a molecule that satisfies all of them simultaneously. To address these challenges, we present a training-free conditional 3D molecular generation algorithm based on off-the-shelf unconditional diffusion models and property prediction models. The key techniques include modeling the loss of property prediction models as energy functions, considering the property relation between multiple conditions as a probabilistic graph, and developing a stable posterior estimation for computing the conditional score function. We conducted experiments on both single-objective and multi-objective 3D molecule generation, focusing on quantum properties, and compared our approach with the trained or fine-tuned diffusion models. Our proposed model achieves superior performance in generating molecules that meet the conditions, without any additional training cost.",9,https://openreview.net/forum?id=X41c4uB4k0,6.5,0.8660254037844386,3.25,0.82915619758885
Wn82BUF7jH,On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space,"['graph generative model', 'molecule conformation generation', 'differential equations']","[5, 3, 3]","[4, 4, 4]",0,"[745, 380, 428]","Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. 
Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world.
In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space.
Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to existing methods.
Code is open-sourced at https://anonymous.4open.science/r/Fast-Sampling-41A6.",11,https://openreview.net/forum?id=Wn82BUF7jH,3.6666666666666665,0.9428090415820634,4.0,0.0
VXDPXuq4oG,Order-Preserving GFlowNets,"['probabilistic sampling', 'multi-objective optimization', 'GFlowNet']","[6, 6, 8, 6, 6]","[4, 3, 4, 3, 4]",0,"[590, 503, 797, 592, 353]","Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective maximization tasks. The sparsification concentrates on candidates of a higher hierarchy in the ordering, ensuring exploration at the beginning and exploitation towards the end of the training. We demonstrate OP-GFN's state-of-the-art performance in single-objective maximization (totally ordered) and multi-objective Pareto front approximation (partially ordered) tasks, including synthetic datasets, molecule generation, and neural architecture search.",26,https://openreview.net/forum?id=VXDPXuq4oG,6.4,0.7999999999999999,3.6,0.4898979485566356
UQVhOVhUi4,Graph Generation with Destination-Predicting Diffusion Mixture,"['Graph Generation', 'Diffusion Model']","[6, 5, 6, 8]","[3, 2, 3, 3]",0,"[328, 364, 162, 576]","Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of our framework that can explicitly model the graph topology and exploit the inductive bias of the data. Through extensive experimental validation on general graph and 2D/3D molecule generation tasks, we show that our method outperforms previous generative models, generating graphs with correct topology with both continuous (e.g. 3D coordinates) and discrete (e.g. atom types) features.",22,https://openreview.net/forum?id=UQVhOVhUi4,6.25,1.0897247358851685,2.75,0.4330127018922193
UIGAtKp8nW,MUBen: Benchmarking the Uncertainty of Molecular Representation Models,"['Uncertainty Quantification', 'Molecular Property', 'Pre-trained Models', 'Molecular Representation Models']","[8, 5, 5, 5]","[4, 3, 5, 3]",0,"[547, 592, 750, 372]","Large molecular representation models pre-trained on massive unlabeled data have shown great success in predicting molecular properties.
  However, these models may tend to overfit the fine-tuning data, resulting in over-confident predictions on test data that fall outside of the training distribution.
  To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions.
  Although many UQ approaches exist, not all of them lead to improved performance.
  While some studies have included UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored.
  To address this gap, we present MUBen, which evaluates different UQ methods for state-of-the-art backbone molecular representation models to investigate their capabilities.
  By fine-tuning various backbones using different molecular descriptors as inputs with UQ methods from different categories, we critically assess the influence of architectural decisions and training strategies.
  Our study offers insights for selecting UQ for backbone models, which can facilitate research on uncertainty-critical applications in fields such as materials science and drug discovery.",24,https://openreview.net/forum?id=UIGAtKp8nW,5.75,1.299038105676658,3.75,0.82915619758885
U9XuNY9crx,Complete and continuous representations of Euclidean graphs,"['complete representation', 'Euclidean graph', 'rigid molecule', 'isometry invariant', 'continuous metric', 'structure-property relation']","[3, 8, 5, 3, 6]","[2, 3, 5, 4, 2]",1,"[361, 164, 737, 608, 390]","Euclidean graphs have unordered vertices and non-intersecting straight-line edges in any Euclidean space. The main application is for molecular graphs with vertices at atomic centers and edges representing inter-atomic bonds. Euclidean graphs are considered equivalent if they are related by isometry (any distance-preserving transformation). This paper introduces the strongest descriptors that are provably (1) invariant under any isometry, (2) complete and sufficient to reconstruct any Euclidean graph up to isometry,  (3) Lipschitz continuous so that perturbations of all vertices within their epsilon-neighborhoods change the complete invariant up to a constant multiple of epsilon in a suitable metric, and (4) computable (both invariant and metric) in a polynomial time in the number of vertices for a fixed dimension. These strongest invariants transparently explained a continuous structure-property landscape for molecular graphs from the QM9 database of 130K+ molecules.",20,https://openreview.net/forum?id=U9XuNY9crx,5.0,1.8973665961010275,3.2,1.16619037896906
Tlsdsb6l9n,Mol-Instructions - A Large-Scale Biomolecular Instruction Dataset for Large Language Models,"['instruction dataset', 'large language models', 'biomolecular studies', 'molecule', 'protein']","[6, 8, 8, 6]","[3, 4, 3, 4]",0,"[414, 185, 255, 747]","Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",16,https://openreview.net/forum?id=Tlsdsb6l9n,7.0,1.0,3.5,0.5
SmZD7yxpPC,GlycoNMR: A Carbohydrate-Specific NMR Chemical Shift Dataset for Machine Learning Research,"['AI for science', 'Glycoscience', 'Graph Neural Network', 'Nuclear Magnetic Resonance']","[5, 6, 6]","[2, 3, 4]",0,"[266, 319, 423]","Molecular representation learning (MRL) is a powerful contribution by machine learning to chemistry as it converts molecules into numerical representations, which serves as fundamental for diverse biochemical applications, such as property prediction and drug design. While MRL has had great success with proteins and general biomolecules, it has yet to be explored for carbohydrates in the growing fields of glycoscience and glycomaterials (the study and design of carbohydrates). This under-exploration can be primarily attributed to the limited availability of comprehensive and well-curated carbohydrate-specific datasets and a lack of machine learning (ML) techniques tailored to meet the unique problems presented by carbohydrate data. Interpreting and annotating carbohydrate data is generally more complicated than protein data, and requires substantial domain knowledge. In addition, existing MRL methods were predominately optimized for proteins and small biomolecules, and may not be effective for carbohydrate applications without special modifications. To address this challenge, accelerate progress in glycoscience and glycomaterials, and enrich the data resources of the ML community, we introduce GlycoNMR. GlycoNMR contains two laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotated nuclear magnetic resonance (NMR) atomic-level chemical shifts that can be used to train ML models for precise atomic-level prediction. NMR data is one of the most appealing starting points for developing ML techniques to facilitate glycoscience and glycomaterials research, as NMR is the preeminent technique in carbohydrate structure research, and biomolecule structure is among the foremost predictors of functions and properties. We tailored a set of carbohydrate-specific features and adapted existing MRL models to effectively tackle the problem of predicting NMR shifts. For illustration, we benchmark these modified MRL models on the GlycoNMR.",13,https://openreview.net/forum?id=SmZD7yxpPC,5.666666666666667,0.4714045207910317,3.0,0.816496580927726
SMZGQu6lld,LLM-Prop: Predicting Physical And Electronic Properties of Crystalline Solids From Their Text Descriptions,"['Text embedding', 'Property prediction', 'Materials science', 'Machine learning']","[5, 3, 3]","[4, 3, 3]",0,"[349, 177, 166]","The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% on predicting band gap, 3% on classifying whether the band gap is direct or indirect, and 66% on predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction.",10,https://openreview.net/forum?id=SMZGQu6lld,3.6666666666666665,0.9428090415820634,3.3333333333333335,0.4714045207910317
SFCHv2G33F,Protein Language Models Enable Accurate Cryptic Ligand Binding Pocket Prediction,"['Protein Language Models', 'Protein Binding Pockets', 'Protein Binding Sites', 'Cryptic Protein Binding Pockets']","[3, 3, 3, 5]","[5, 3, 3, 4]",0,"[666, 185, 162, 519]","Accurate prediction of protein-ligand binding pockets is a critical task in protein functional analysis and small molecule pharmaceutical design.  However, the flexible and dynamic nature of proteins conceal an unknown number of potentially invaluable ""cryptic"" pockets.  Current approaches for cryptic pocket discovery rely on molecular dynamics (MD), leading to poor scalability and bias.  Even recent ML-based cryptic pocket discovery approaches require large, post-processed MD datasets to train their models.  In contrast, this work presents ``Efficient Sequence-based cryptic Pocket prediction'' (ESP) leveraging advanced Protein Language Models (PLMs), and demonstrates significant improvement in predictive efficacy compared to ML-based cryptic pocket prediction SOTA (ROCAUC 0.93 vs 0.87).  ESP achieves detection of cryptic pockets via training on readily available, non cryptic-pocket-specific data from the PDBBind dataset, rather than costly simulation and post-processing.  Further, while SOTA's predictions often include positive signal broadly distributed over a target structure, ESP produces more spatially-focused predictions which increase downstream utility.",9,https://openreview.net/forum?id=SFCHv2G33F,3.5,0.8660254037844386,3.75,0.82915619758885
S62iZf0cba,Multi-Objective Molecular Design through Learning Latent Pareto Set,"['multi-objective optimization', 'molecular design', 'Pareto set learning', 'Bayesian optimization']","[5, 3, 6]","[5, 3, 4]",0,"[324, 373, 777]","Molecular design inherently involves the optimization of multiple conflicting objectives, such as enhancing bio-activity and ensuring synthesizability. Evaluating these objectives often requires resource-intensive computations or physical experiments. Current molecular design methodologies typically approximate the Pareto set using a limited number of molecules. In this paper, we present an innovative approach, called Multi-Objective Molecular Design through Learning Latent Pareto Set (MLPS). MLPS initially utilizes an encoder-decoder model to seamlessly transform the discrete chemical space into a continuous latent space. We then employ local Bayesian optimization models to efficiently search for local optimal solutions (i.e., molecules) within predefined trust regions. Using surrogate objective values derived from these local models, we train a global Pareto set learning model to understand the mapping between direction vectors (called ``preferences'') in the objective space and the entire Pareto set in the continuous latent space. Both the global Pareto set learning model and local Bayesian optimization models collaborate to discover high-quality solutions and adapt the trust regions dynamically. Our work is the first endeavor towards learning the Pareto set for multi-objective molecular design, providing decision-makers with the capability to fine-tune their preferences and thoroughly explore the Pareto set. Experimental results demonstrate that MLPS achieves state-of-the-art performance across various multi-objective scenarios, encompassing diverse objective types and varying numbers of objectives.",19,https://openreview.net/forum?id=S62iZf0cba,4.666666666666667,1.247219128924647,4.0,0.816496580927726
S4zpk61r6G,DiffMaSIF: Score-Based Diffusion Models for Protein Surfaces,"['geometric deep learning', 'protein-protein docking', 'diffusion model', 'equivariant network', 'protein surface']","[3, 6, 5]","[4, 4, 5]",0,"[1160, 395, 359]","Predicting protein-protein complexes is one of the central challenges of computational structural biology. Inspired by recent generative machine learning (ML) techniques which have shown promise in the realms of protein docking, we introduce DiffMaSIF, a novel score-based diffusion model for rigid protein-protein docking. While existing methods rely on co-evolution learned on a residue level, this information is not sufficient for transient, weakly evolved, or newly designed interfaces. DiffMaSIF’s efficacy hinges on its surface-based molecular representation which can capture the complementarity inherent in the physical surfaces of interacting protein interfaces. We follow an end-to-end two-tier prediction schema: initially identifying contact sites on the protein surface and constraining each molecular graph to these sites, followed by an equivariant network to position the two proteins. This data reduction step enables the use of more sophisticated networks and more training steps. In addition to developing this model, we introduce new dataset splits accounting for structural leakage at the interface and thus tailored for benchmarking protein-protein interface prediction performance. Our results demonstrate that DiffMaSIF not only outperforms contemporary ML methods in rigid protein docking, but also matches traditional docking tools at considerably fewer numbers of generated decoys. Through DiffMaSIF, we pave the way for surface-centric interface prediction methods, thus advancing accurate prediction of protein interactions across a wide spectrum of difficult and novelty.",3,https://openreview.net/forum?id=S4zpk61r6G,4.666666666666667,1.247219128924647,4.333333333333333,0.4714045207910317
RemfXx7ebP,Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design,['Bioinformatics'],"[3, 3, 6]","[5, 4, 4]",0,"[367, 969, 824]","While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset will be released publicly.",9,https://openreview.net/forum?id=RemfXx7ebP,4.0,1.4142135623730951,4.333333333333333,0.4714045207910317
RY0yYsTxGM,Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interaction Prediction,"['compound-protein interaction', 'graph transformer', 'hierarchy graph', 'biomolecule affinity prediction']","[1, 3, 5, 3]","[4, 4, 5, 4]",1,"[588, 228, 528, 557]","Predicting compound-protein interactions (CPIs) is critical for AI-aided drug design. Recent deep learning (DL) methods have successfully modeled molecular interactions at the atomic level, achieving both efficiency and accuracy improvements compared to traditional energy-based methods. However, these models do not always align with the chemical realities of CPIs, as molecular fragments (i.e., motifs) often participate in the interactions dominantly. In this paper, we aim to fill this gap by considering the role of motifs for CPIs. We propose a pair-wise hierarchical interaction representation learning (Phi-former) method. Phi-former represents the compound or protein hierarchically and employs a pair-wise specific pre-training framework for modeling the interactions in a more systematic way~(i.e., atom-atom, motif-motif, and atom-motif). We propose an intra-level and inter-level Phi-former pipeline for learning the pair-wise biomolecular graph representation, making learning the different interaction levels mutually beneficial. We demonstrate that Phi-former can achieve superior performance on CPI-related tasks. Furthermore, a case study indicates that our method can accurately identify the specific atoms or motifs activated in CPIs, and thus provide good model explanations that may give insights into molecular structural optimization.",4,https://openreview.net/forum?id=RY0yYsTxGM,3.0,1.4142135623730951,4.25,0.4330127018922193
RSincg5RBe,Hierarchical Graph Latent Diffusion Model for Molecule Generation,"['Hierarchical Graph Latent Diffusion', 'Molecule Generation']","[5, 3, 8, 5]","[3, 4, 4, 4]",0,"[576, 622, 314, 380]","Recently, generative models based on the diffusion process have emerged as a promising direction for automating the design of molecules. However, directly adding continuous Gaussian noise to discrete graphs leads to the problem of the final noisy data not conforming to the standard Gaussian distribution. Current graph diffusion models either corrupt discrete data through a transition matrix or relax the discrete data to continuous space for the diffusion process. These approaches not only require significant computation resources due to the inclusion of the bond type matrix but also cannot easily perform scalable conditional generation, such as adding cross-attention layers, due to the lack of embedding representations. In this paper, we first introduce the Graph Latent Diffusion Model (GLDM), a novel variant of latent diffusion models that overcomes the mismatch problem of continuous diffusion space and discrete data space. Meanwhile, the latent diffusion framework avoids the issues of computational resource consumption and lack of embeddings for conditional generation faced by current graph diffusion models. However, it only utilizes graph-level embeddings for molecule generation, losing node-level and structural information. Therefore, we further ex- tend the GLDM to the Hierarchical Graph Latent Diffusion Model (HGLDM). By including node embeddings and subgraph embeddings that contain structural in- formation, our model significantly reduces computation time compared to the cur- rent graph diffusion models. We evaluate our model on three benchmarks through unconditional generation and conditional generation tasks, which demonstrate its superior performance.",26,https://openreview.net/forum?id=RSincg5RBe,5.25,1.7853571071357126,3.75,0.4330127018922193
RIEW6M9YoV,Graph Generation with  $K^2$-trees,"['Graph generative models', 'graph neural networks']","[5, 5, 8, 8]","[3, 4, 4, 3]",0,"[266, 631, 199, 242]","Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$ representation, originally designed for lossless graph compression. The $K^2$ representation enables compact generation while concurrently capturing an inherent hierarchical structure of a graph. In addition, we make contributions by (1) presenting a sequential $K^2$ representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.",17,https://openreview.net/forum?id=RIEW6M9YoV,6.5,1.5,3.5,0.5
QQ81YsbSij,Learning Conditional Policy for Crystal Design using Offline Reinforcement Learning,"['Material discovery', 'crystal design', 'offline reinforcement learning', 'density functional theory', 'AI for science']","[5, 5, 3, 3]","[2, 2, 5, 4]",1,"[240, 190, 333, 631]","Navigating through the exponentially large chemical space to search for desirable materials is an extremely challenging task in material discovery. Recent developments in generative and geometric deep learning have shown promising results in molecule and material discovery but often lack evaluation with high-accuracy computational methods. This work aims to design novel and stable crystalline materials conditioned on a desired band gap. To achieve conditional generation, we:  1. Formulate crystal design as a sequential decision-making problem, create relevant trajectories based on high-quality materials data and use conservative Q-learning to learn a conditional policy from these trajectories. To do so, we formulate a reward function that incorporates constraints for energetic and electronic properties obtained directly from density functional theory (DFT) calculations;  2. Evaluate the generated materials from the policy using DFT calculations for both energy and band gap; 3. Compare our results to relevant baselines, including a random policy, behavioral cloning, and unconditioned policy learning. Our experiments show that our conditioned policies achieve more targeted crystal structure designs and demonstrate the capability to perform crystal structure design evaluated with accurate and computationally expensive DFT calculations.",13,https://openreview.net/forum?id=QQ81YsbSij,4.0,1.0,3.25,1.299038105676658
Pu3qMB9aKD,Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements,"['Generative Modelling', 'Graph Structured Data', 'Multimodal Conditional Generation', 'Molecule Generation']","[5, 5, 3, 5]","[4, 3, 4, 4]",0,"[373, 240, 229, 546]","This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.",29,https://openreview.net/forum?id=Pu3qMB9aKD,4.5,0.8660254037844386,3.75,0.4330127018922193
PiPaNgOaVP,De Novo Drug Design with Joint Transformers,"['generative model', 'joint model', 'transformer', 'molecule generation', 'drug design']","[3, 3, 5, 3]","[4, 4, 4, 4]",1,"[473, 229, 506, 742]","\emph{De novo} drug design requires generating novel molecules outside of the training data and simultaneously predicting their target properties, making it a hard task for machine learning models. To address this, we propose Joint Transformer that combines a Transformer decoder, Transformer encoder and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in simultaneously matching state-of-the-art performance in a molecule generation task and successfully predicting molecular properties of newly sampled molecules. Notably, the jointly trained model decreases the prediction error, as compared to a fine-tuned decoder-only Transformer, by $42$%. We propose a probabilistic black box optimization algorithm employing Joint Transformer for  the problem of \emph{de novo} drug design, which generates novel molecules with corresponding target properties better than in the data, outperforming other SMILES-based optimization methods.",4,https://openreview.net/forum?id=PiPaNgOaVP,3.5,0.8660254037844386,4.0,0.0
PfPnugdxup,From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction,"['atomic property prediction', 'pre-training', '3D atomic pre-training', 'graph neural networks', 'multi-task learning', 'molecules', 'materials']","[5, 8, 5, 5]","[4, 4, 4, 4]",0,"[601, 331, 145, 655]","The role of machine learning in computing atomic properties is expanding rapidly for a wide range of applications from healthcare to climate change. One important ingredient that has enabled this development is the creation of large and diverse molecular datasets. Given the extreme computational cost of these datasets, an important question moving forward is: Can we limit the need for exhaustive large dataset creation by pre-training a foundation style model over multiple chemical domains to generate transferable atomic representations for downstream fine-tuning tasks? Generalization across the entire molecular space is challenging due to the range and complexity of atomic interactions that exist. In this paper, we present Joint Multi-domain Pre-training (JMP), a robust supervised pre-training strategy that utilizes data from multiple chemical domains, $\sim$120 million examples in total. We demonstrate state-of-the-art results across many targets of the rMD17, QM9, MatBench, QMOF, SPICE, and MD22 datasets. Finally, we conduct ablations to study the impact of different components of JMP on downstream performance.",19,https://openreview.net/forum?id=PfPnugdxup,5.75,1.299038105676658,4.0,0.0
P15CHILQlg,Learning Energy Decompositions for Partial Inference of GFlowNets,"['Generative flow networks', 'reinforcement learning', 'generative models']","[8, 8, 8, 8]","[5, 5, 2, 3]",0,"[942, 750, 180, 574]","This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credits, we propose to regularize the potential to change smoothly over the sequence of actions. It is also noteworthy that training GFlowNet with our learned potential can preserve the optimal policy. We empirically verify the superiority of LED-GFN in five problems including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences.",17,https://openreview.net/forum?id=P15CHILQlg,8.0,0.0,3.75,1.299038105676658
Od39h4XQ3Y,Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models,"['Sharpness-Aware Minimization', 'Molecular Graph']","[6, 6, 6, 6]","[4, 5, 3, 4]",0,"[228, 477, 256, 496]","Sharpness-aware minimization (SAM) has received increasing attention in computer vision since it can effectively eliminate the sharp local minima from the training trajectory and mitigate generalization degradation. However, SAM requires two sequential gradient computations during the optimization of each step: one to obtain the perturbation gradient and the other to obtain the updating gradient. 
Compared with the base optimizer (e.g., Adam), SAM doubles the time overhead due to the additional perturbation gradient. By dissecting the theory of SAM and observing the training gradient of the molecular graph transformer, we propose a new algorithm named GraphSAM,  which reduces the training cost of SAM and improves the generalization performance of graph transformer models. 
There are two key factors that contribute to this result: (i) \textit{gradient approximation}: we use the updating gradient of the previous step to approximate the perturbation gradient at the intermediate steps smoothly (\textbf{increases efficiency}); (ii) \textit{loss landscape approximation}: we theoretically prove that the loss landscape of GraphSAM is limited to a small range centered on the expected loss of SAM (\textbf{guarantees generalization performance}). The extensive experiments on six datasets with different tasks demonstrate the superiority of GraphSAM, especially in optimizing the model update process.",13,https://openreview.net/forum?id=Od39h4XQ3Y,6.0,0.0,4.0,0.7071067811865476
NvJxTjTQtq,EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations,"['Graph neural network', 'equivariant neural network', 'atomistic simulations', 'molecular dynamics']","[5, 8, 5]","[4, 4, 4]",0,"[404, 313, 585]","Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs’ inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain.",18,https://openreview.net/forum?id=NvJxTjTQtq,6.0,1.4142135623730951,4.0,0.0
NSVtmmzeRB,Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks,"['Drug Design', 'Molecule Generation', 'Deep Learning', 'Computational Biology']","[8, 8, 8, 8]","[4, 2, 4, 4]",0,"[217, 241, 540, 531]","Advanced generative model (\textit{e.g.}, diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the \textit{multi-modality} and \textit{noise-sensitive} nature of molecule geometry. 
This work introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains the SE-(3) invariant density modeling property by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. 
Through optimized training and sampling techniques, we demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D molecule generation benchmarks in terms of generation quality (90.87\% molecule stability in QM9 and 85.6\% atom stability in GEOM-DRUG\footnote{The scores are reported at 1k sampling steps for fair comparison, and our scores could be further improved if sampling sufficiently longer steps.}). GeoBFN can also conduct sampling with any number of steps to reach an optimal trade-off between efficiency and quality (\textit{e.g.}, 20$\times$ speedup without sacrificing performance).",24,https://openreview.net/forum?id=NSVtmmzeRB,8.0,0.0,3.5,0.8660254037844386
NSDszJ2uIV,Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks,"['conformer ensembles', 'geometric learning']","[8, 5, 6]","[4, 5, 3]",0,"[208, 647, 158]","Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules. In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models. Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.",15,https://openreview.net/forum?id=NSDszJ2uIV,6.333333333333333,1.247219128924647,4.0,0.816496580927726
NPViqdhTIi,Parameter-Free Molecular Classification and Regression with Gzip,"['cheminformatics', 'chemistry', 'language', 'compression']","[5, 3, 8, 3]","[3, 4, 4, 5]",0,"[935, 468, 968, 326]","In recent years, natural language processing approaches to machine learning, most prominently deep neural network-based transformers, have been extensively applied to molecular classification and regression tasks, including the prediction of pharmacokinetic and quantum-chemical properties. However, models based on deep neural networks generally require extensive training, large training data sets, and resource-consuming hyperparameter tuning. Recently, a low-resource and universal alternative to deep learning approaches based on Gzip compression for text classification has been proposed, which reportedly performs surprisingly well compared to large language models such as BERT, given its conceptually simplistic nature. Here, we adapt the proposed method to support multiprocessing, multi-class classification, class-weighing, and regression and apply it to classification and regression tasks on various data sets of molecules from the organic chemistry, biochemistry, drug discovery, and material science domains. We further propose converting numerical descriptors into string representations, enabling the integration of language input with domain-informed descriptors. Our results show that the method can be used to classify and predict a variety of properties of molecules, can reach the performance of large-scale chemical language models in a subset of tasks, and has the potential for application in information retrieval from large chemical databases.",4,https://openreview.net/forum?id=NPViqdhTIi,4.75,2.0463381929681126,4.0,0.7071067811865476
Mtlt3RQTXJ,Bi-level Contrastive Learning for Knowledge Enhanced Molecule Representations,"['Molecule Representation', 'Knowledge Graph', 'Contrastive Learning']","[3, 5, 3, 8]","[4, 4, 4, 4]",0,"[217, 321, 528, 278]","Molecule representation learning underpins diverse downstream applications such as molecular property and side effect understanding and prediction. In this paper, we recognize the two-level structure of individual molecule as having intrinsic graph structure as well as being a node in a large molecule knowledge graph, and present Gode, a new approach that seamlessly integrates graph representations of individual molecules with multi-domain biomedical data from knowledge graphs. By pre-training two graph neural networks (GNNs) on different graph structures, combined with contrastive learning, Gode adeptly fuses molecular structures with their corresponding knowledge graph substructures. This fusion results in a more robust and informative representation, enhancing molecular property prediction by harnessing both chemical and biological information. When fine-tuned across 11 chemical property tasks, our model excels beyond existing benchmarks, registering an average ROC-AUC uplift of 13.8% for classification tasks and an average RMSE/MAE enhancement of 35.1% for regression tasks. Impressively, it surpasses the current leading model in molecule property predictions with average advancements of 2.1% in classification and 6.4% in regression tasks.",30,https://openreview.net/forum?id=Mtlt3RQTXJ,4.75,2.0463381929681126,4.0,0.0
MamHzZHs0h,SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design,"['in-context learning', 'drug synergy prediction', 'inverse drug design', 'precision medicine']","[5, 3, 5, 3]","[5, 5, 2, 4]",0,"[380, 498, 482, 428]","Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small ""personalized dataset"" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to ""in-context learn"" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn ""drug synergy functions"". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient's ""personalized dataset"". Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.",4,https://openreview.net/forum?id=MamHzZHs0h,4.0,1.0,4.0,1.224744871391589
MNwXif6AWA,Periodic Set Transformer: Material Property Prediction from Continuous Isometry Invariants,['Material Property Prediction'],"[5, 3, 3, 6]","[4, 4, 5, 3]",0,"[727, 752, 359, 514]","Material or crystal property prediction using machine learning has grown popular in recent years as it provides an accurate and computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a fixed number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and incorporate compositional information via a spatial encoding method. This model is tested thoroughly with and without the use of compositional information on a variety of crystal datasets including the commonly used crystals of the Materials Project.",20,https://openreview.net/forum?id=MNwXif6AWA,4.25,1.299038105676658,4.0,0.7071067811865476
MIEnYtlGyv,Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation,"['molecule', 'spherical harmonics', 'equivariant', 'symmetry', 'generation']","[6, 6, 6, 8]","[3, 4, 4, 3]",0,"[286, 380, 618, 635]","We present Symphony, an $E(3)$ equivariant autoregressive generative model for 3D molecular geometries
that iteratively builds a molecule from molecular fragments.
Existing autoregressive models such as G-SchNet and G-SphereNet  for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules.
In contrast, Symphony uses message-passing with higher-degree $E(3)$-equivariant features.
This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of
molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing
autoregressive models and approaching the performance of diffusion models.",19,https://openreview.net/forum?id=MIEnYtlGyv,6.5,0.8660254037844386,3.5,0.5
MBIGXMT0qC,Multi-Scale Protein Language Model for Unified Molecular Modeling,"['Protein Pre-training', 'Unified Molecular Modeling']","[3, 6, 6, 5]","[5, 3, 3, 4]",0,"[627, 367, 112, 315]","Protein language models have shown great potential in protein engineering. However, the current protein language models mainly work in the residue scale, which cannot offer information in the atom scale. The strong power of protein language models could not be fully exploited to benefit the applications that cross protein and small molecules. In this paper, we propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding. Experimental results show that msESM outperforms previous methods in protein-molecule tasks and is on par with the state-of-the-art in protein-only and molecule-only tasks.",38,https://openreview.net/forum?id=MBIGXMT0qC,5.0,1.224744871391589,3.75,0.82915619758885
LOyYjE0blM,Neural scaling laws for phenotypic drug discovery,"['Drug discovery', 'neural scaling laws', 'small molecules', 'phenotypic screening']","[3, 1, 5]","[4, 2, 3]",1,"[510, 401, 346]","Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark — a diverse set of drug discovery tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs first trained with IBP then probed for performance on the Pheno-CA significantly outperform task-supervised DNNs. More importantly, the performance of these IBP-trained DNNs monotonically improves with data and model scale. Our findings reveal that the DNN ingredients needed to accurately solve small molecule drug discovery tasks are already in our hands, and project how much more experimental data is needed to achieve any desired level of improvement. We release our Pheno-CA benchmark and code to encourage further study of neural scaling laws for small molecule drug discovery.",3,https://openreview.net/forum?id=LOyYjE0blM,3.0,1.632993161855452,3.0,0.816496580927726
KqbCvIFBY7,Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models,"['diffusion models', 'score-based models', 'diversity', 'guidance', 'conformer generation', 'image generation']","[5, 5, 8, 6]","[5, 5, 4, 4]",0,"[330, 298, 777, 359]","In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring in a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. For this we propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average.",17,https://openreview.net/forum?id=KqbCvIFBY7,6.0,1.224744871391589,4.5,0.5
J4V3lW9hq6,A Multi-Grained Group Symmetric Framework for Learning Protein-Ligand Binding Dynamics,"['protein-ligand binding', 'group symmetric', 'multi-grained', 'molecular simulation', 'Newtonian dynamics']","[3, 5, 6, 6]","[4, 3, 3, 5]",0,"[704, 566, 221, 200]","In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural ordinary differential equation solver that learns the trajectory under Newtonian mechanics. For the experiment, we design ten single-trajectory and three multi-trajectory binding simulation tasks. We show the efficiency and effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical MD simulation and outperforming all other ML approaches by up to ~80\% under the stability metric. We further qualitatively show that NeuralMD reaches more stable binding predictions.",21,https://openreview.net/forum?id=J4V3lW9hq6,5.0,1.224744871391589,3.75,0.82915619758885
IuLVxo0T0R,Enhancing Precision Drug Recommendations via Fine-grained Exploration of Motif Relationships,"['Drug discovery', 'Recommender System', 'Molecular Representation Learning']","[5, 5, 3, 3]","[3, 3, 4, 3]",1,"[630, 1224, 515, 399]","Making accurate and safe drug recommendation for patients has always been a challenging task. Even though rule-based, instance-based, and longitudinal data-based approaches have made notable strides in drug modeling, they often neglect to fully leverage the rich motifs information. However, it is widely acknowledged that motifs exert a significant influence on both drug action and patient symptomatology. Therefore, there is a pressing need for more comprehensive exploration this invaluable information to further enhance drug recommendation systems. To tackle the aforementioned challenges, we present DEPOT, a novel drug recommendation framework that leverages motifs as higher-level structures to enhance recommendations. In our approach, we employ chemical decomposition to partition drug molecules into motif-trees, enabling us to capture the structural information among substructures. To investigate the relationship between disease progression and motifs, we conduct a meticulous exploration from two perspectives: repetition and exploration. This comprehensive analysis allows us to gain valuable insights into the drug turnover, with the former focusing on reusability and the latter on discovering new requirements. Furthermore, we incorporate historical DDI effects and employ a nonlinear optimization objective to stabilize the training process, ensuring the safety of recommended drug combinations. Extensive experiments are conducted on two data sets to validate the uniqueness of the DEPOT framework and the efficacy of the individual submodules.",4,https://openreview.net/forum?id=IuLVxo0T0R,4.0,1.0,3.25,0.4330127018922193
IhD1rBHhDy,Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures,"['large language model', 'LLM', 'ChatGPT', 'data mining', 'chemistry', 'function', 'antiviral', 'drug discovery', 'CheF', 'patents', 'functionality', 'chemical function', 'electroluminescence', 'serotonin', '5-ht', 'hcv', 'hepatitis']","[3, 8, 6, 5]","[4, 4, 3, 4]",0,"[288, 408, 164, 321]","Predicting chemical function from structure is a major goal of the chemical sciences, from the discovery and repurposing of novel drugs to the creation of new materials. Recently, new machine learning algorithms are opening up the possibility of general predictive models spanning many different chemical functions. Here, we consider the challenge of applying large language models to chemical patents in order to consolidate and leverage the information about chemical functionality captured by these resources. Chemical patents contain vast knowledge on chemical function, but their usefulness as a dataset has historically been neglected due to the impracticality of extracting high-quality functional labels. Using a scalable ChatGPT-assisted patent summarization and word-embedding label cleaning pipeline, we derive a Chemical Function (CheF) dataset, containing 100K molecules and their patent-derived functional labels. The functional labels were validated to be of high quality, allowing us to detect a strong relationship between functional label and chemical structural spaces. Further, we find that the co-occurrence graph of the functional labels contains a robust semantic structure, which allowed us in turn to examine functional relatedness among the compounds. We then trained a model on the CheF dataset, allowing us to assign new functional labels to compounds. Using this model, we were able to retrodict approved Hepatitis C antivirals, uncover an antiviral mechanism undisclosed in the patent, and identify plausible serotonin-related drugs. The CheF dataset and associated model offers a promising new approach to predict chemical functionality.",12,https://openreview.net/forum?id=IhD1rBHhDy,5.5,1.8027756377319946,3.75,0.4330127018922193
IP28nY6TJQ,In-Context Learning for Few-Shot Molecular Property Prediction,"['Meta-learning', 'molecular property prediction']","[5, 3, 3, 3]","[4, 4, 4, 4]",1,"[464, 360, 345, 679]","In-context learning has become an important approach for few-shot learning in Large Language Models because of its ability to rapidly adapt to new tasks without fine-tuning model parameters. However, it is restricted to applications in natural language and inapplicable to other domains. In this paper, we adapt the concepts underpinning in-context learning to develop a new algorithm for few-shot molecular property prediction. Our approach learns to predict molecular properties from a context of (molecule, property measurement) pairs and rapidly adapts to new properties without fine-tuning. On the FS-Mol and BACE molecular property prediction benchmarks, we find this method surpasses the performance of recent meta-learning algorithms at small support sizes and is competitive with the best methods at large support sizes.",4,https://openreview.net/forum?id=IP28nY6TJQ,3.5,0.8660254037844386,4.0,0.0
IL9o1meezQ,Random Walk Diffusion For Graph Generation,"['Graph Generation', 'Diffusion Models']","[3, 6, 3, 6]","[5, 4, 4, 4]",0,"[402, 292, 806, 530]","Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. Recently, the task of graph generation has gained increasing attention with applications ranging from data augmentation to constructing molecular graphs with specific properties. Previous diffusion-based approaches have shown promising results in terms of the quality of the generated graphs. However, most methods are designed for generating small graphs and do not scale well to large graphs. In this work, we introduce ARROW-Diff, a novel random walk-based diffusion approach for graph generation. It utilizes an order agnostic autoregressive diffusion model enabling us to generate graphs at a very large scale. ARROW-Diff encompasses an iterative procedure that builds the final graph from sampled random walks based on an edge classification task and directed by node degrees. Our method outperforms all baseline methods in terms of training and generation time and can be trained both on single- and multi-graph datasets. Moreover, it outperforms most baselines on multiple graph statistics reflecting the high quality of the generated graphs.",11,https://openreview.net/forum?id=IL9o1meezQ,4.5,1.5,4.25,0.4330127018922193
IJBsKYXaH4,Molecular Conformation Generation via Shifting Scores,"['molecule conformation generation', 'generative diffusion model']","[3, 5, 5, 3]","[4, 4, 4, 3]",0,"[564, 223, 755, 195]","Molecular conformation generation, a critical aspect of computational chemistry, involves producing the three-dimensional conformer geometry for a given molecule. Generating molecular conformation via diffusion requires learning to reverse a noising process. Diffusion on inter-atomic distances instead of conformation preserves SE(3)-equivalence and shows superior performance compared to alternative techniques, whereas related generative modelings are predominantly based upon heuristical assumptions. In response to this, we propose a novel molecular conformation generation approach driven by the observation that the disintegration of a molecule can be viewed as casting increasing force fields to its composing atoms, such that the distribution of the change of inter-atomic distance shifts from Gaussian to Maxwell-Boltzmann distribution. The corresponding generative modeling ensures a feasible inter-atomic distance geometry and exhibits time reversibility. Experimental results on molecular datasets demonstrate the advantages of the proposed shifting distribution compared to the state-of-the-art.",16,https://openreview.net/forum?id=IJBsKYXaH4,4.0,1.0,3.75,0.4330127018922193
HrTGl8AhnS,PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property Prediction,"['molecular property prediction', 'few-shot learning', 'hypernetwork']","[3, 6, 6, 5]","[4, 3, 2, 4]",0,"[310, 376, 286, 285]","Molecular property prediction (MPP) plays a crucial role in biomedical applications, but it often encounters challenges due to a scarcity of labeled data. Existing works commonly adopt gradient-based strategy to update a large amount of parameter for property-level adaptation.  However, the increase of adaptive parameters can cause overfitting and lead to poor performance. Observing that graph neural network (GNN) performs well as both encoder and predictor, we propose PACIA, a parameter-efficient GNN adapter for few-shot MPP. We design a unified adapter to generate a few adaptive parameters to modulate the message passing process of GNN. We then adopt hierarchical adaptation mechanism to adapt the encoder on property-level and the predictor on molecule-level by the unified GNN adapter. Extensive results show that PACIA obtains the state-of-the-art performance in few-shot MPP problems, and our proposed hierarchical adaptation mechanism is rational and effective.",12,https://openreview.net/forum?id=HrTGl8AhnS,5.0,1.224744871391589,3.25,0.82915619758885
GsNp4ob8BY,Mark My Words: Repurposing LLMs for Specialized Domains via Ability Tokens,"['LLM Adaptation', 'Specialized Domains']","[5, 5, 5, 6]","[4, 4, 4, 3]",0,"[395, 255, 310, 375]","Large Language Models (LLMs) have demonstrated remarkable proficiency in natural language understanding and generation. However, their capabilities wane in highly specialized domains, such as biomedical sciences, which are sparsely represented in the pretraining corpus. In this work, we explore how to repurpose general LMs as specialized task solvers. We introduce a novel and systematic framework for adding markup-style language extensions (which we term *`ability tokens""*) to pretrained LMs. These tokens are learned embeddings appended to the LM's embedding matrix, preserving the pretrained weights and the model's original capabilities. We introduce two types of ability tokens: *domain markers*, which delimit and aid in the processing of specialized inputs (e.g., molecular formulas), and *functional tokens*, which guide the model on how to leverage these inputs to solve specific tasks (e.g., predicting molecule properties). During inference, these tokens are inserted into the input text to wrap specialized information and provide problem context. Experimental results show that (i) our markup extensions significantly boost performance in various specialized domains, such as protein and molecular property prediction, matching and outperforming expert models specifically tailored to these tasks, and (ii) we can learn the ability tokens separately and combine them in a modular fashion, achieving zero-shot generalization to  unseen tasks. Overall, our framework offers a promising method to enhance LMs with domain-specific knowledge while maintaining their general capacities.",12,https://openreview.net/forum?id=GsNp4ob8BY,5.25,0.4330127018922193,3.75,0.4330127018922193
GKxmmAwxj1,Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules,"['normalizing flow', 'protein conformations', 'boltzmann generators', 'generative models']","[6, 6, 5, 6]","[4, 3, 4, 3]",0,"[884, 517, 285, 250]","The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35.",18,https://openreview.net/forum?id=GKxmmAwxj1,5.75,0.4330127018922193,3.5,0.5
G536mmC2HL,TorSeq: Torsion Sequential Modeling for Molecular 3D Conformation Generation,"['conformer generation', 'sequential model', 'geometric deep learning']","[3, 3, 3, 3]","[4, 5, 4, 4]",0,"[528, 343, 143, 738]","In the realms of chemistry and drug discovery, the generation of 3D low-energy molecular conformers is critical. While various methods, including deep generative and diffusion-based techniques, have been developed to predict 3D atomic coordinates and molecular geometry elements like bond lengths, angles, and torsion angles, they often neglect the intrinsic correlations among these elements. This oversight, especially regarding torsion angles, can produce less-than-optimal 3D conformers in the context of energy efficiency. Addressing this gap, we introduce a method that explicitly models the dependencies of geometry elements through sequential probability factorization, with a particular focus on optimizing torsion angle correlations. Experimental evaluations on benchmark datasets for molecule conformer generation underscore our approach's superior efficiency and efficacy.",15,https://openreview.net/forum?id=G536mmC2HL,3.0,0.0,4.25,0.4330127018922193
FdUloEgBSE,Text-guided Diffusion Model for 3D Molecule Generation,"['Drug design', 'Generative Models', 'Diffusion Models']","[1, 6, 3, 5]","[4, 4, 4, 3]",0,"[564, 243, 366, 965]","The *de novo* generation of molecules with desired properties is a critical task in fields like biology, chemistry, and drug discovery.
Recent advancements in diffusion models, particularly equivariant diffusion models, have shown promise in generating 3D molecular structures.
However, these models largely work under value guidance, typically conditioning on a single property value, which might limit their ability to address complex real-world requirements.
To address this, we propose the text guidance instead, and introduce TEDMol, a new *Text-guided Diffusion Model for 3D Molecule Generation*.
It aims to integrate the capabilities of language models with diffusion models, thereby providing a deeper level of language understanding in 3D molecule generation.
Specifically, TEDMol utilizes textual conditions to guide the reverse process, enabling the adept and flexible generation of 3D molecules.
Our experimental results on various tasks demonstrate that TEDMol not only enhances the stability and diversity of the generated molecules, but also excels in capturing and utilizing information derived from textual descriptions.
Our approach forms a flexible and efficient text-guided molecular diffusion framework, providing a powerful tool for generating 3D molecular structures in response to complex, textual conditions.",8,https://openreview.net/forum?id=FdUloEgBSE,3.75,1.920286436967152,3.75,0.4330127018922193
FWsGuAFn3n,Prompt-based 3D Molecular Diffusion Models for  Structure-based Drug Design,"['Diffusion Model', 'Structure-based Drug Design', 'Molecule Generation']","[3, 3, 6, 3]","[4, 4, 3, 4]",0,"[626, 325, 692, 179]","Generating ligand molecules that bind to specific protein targets via generative models holds substantial promise for advancing structure-based drug design. Existing methods generate molecules from scratch without reference or template ligands, which poses challenges in model optimization and may yield suboptimal outcomes. To address this problem, we propose an innovative prompt-based 3D molecular diffusion model named PromptDiff to facilitate target-aware molecule generation. PromptDiff leverages a curated set of ligand prompts, i.e., those with desired properties such as high binding affinity and synthesizability, to steer the diffusion model towards synthesizing ligands that satisfy design criteria. Specifically, we design a geometric protein-molecule interaction network (PMINet), and pretrain it with binding affinity signals to: (i) retrieve target-aware ligand molecules with high binding affinity to serve as prompts, and (ii) incorporate essential protein-ligand binding structures for steering molecular diffusion generation with two effective prompting mechanisms, i.e., exemplar prompting and self prompting. Empirical studies on CrossDocked2020 dataset show PromptDiff can generate molecules with more realistic 3D structures and achieve state-of-the-art binding affinities towards the protein targets, while maintaining proper molecular properties.",4,https://openreview.net/forum?id=FWsGuAFn3n,3.75,1.299038105676658,3.75,0.4330127018922193
FMMF1a9ifL,Gradual Optimization Learning for Conformational Energy Minimization,"['energy minimization', 'conformational optimization']","[8, 6, 6, 6]","[4, 3, 4, 3]",0,"[261, 1714, 430, 198]","Molecular conformation optimization is crucial to computer-aided drug discovery and materials design.
Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients.
However, this is a computationally expensive approach that requires many interactions with a physical simulator.
One way to accelerate this procedure is to replace the physical simulator with a neural network.
Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization.
We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data.
Still, it takes around $5 \times 10^5$ additional conformations to match the physical simulator's optimization quality.
In this work, we present the Gradual Optimization Learning Framework (GOLF) for energy minimization with neural networks that significantly reduces the required additional data.
The framework consists of an efficient data-collecting scheme and an external optimizer.
The external optimizer utilizes gradients from the energy prediction model to generate optimization trajectories, and the data-collecting scheme selects additional training data to be processed by the physical simulator. 
Our results demonstrate that the neural network trained with GOLF performs \textit{on par} with the oracle on a benchmark of diverse drug-like molecules using $50$x less additional data.",14,https://openreview.net/forum?id=FMMF1a9ifL,6.5,0.8660254037844386,3.5,0.5
F7QnIKlC1N,GTMGC: Using Graph Transformer to Predict Molecule’s Ground-State Conformation,"['molecular conformation prediction', 'molecule modeling', 'graph neural network', 'graph transformer']","[3, 8, 8]","[4, 4, 4]",0,"[487, 204, 483]","The ground-state conformation of a molecule is often decisive for its properties. However, experimental or computational methods, such as density functional theory (DFT), are time-consuming and labor-intensive for obtaining this conformation. Deep learning (DL) based molecular representation learning (MRL) has made significant advancements in molecular modeling and has achieved remarkable results in various tasks. Consequently, it has emerged as a promising approach for directly predicting the ground-state conformation of molecules. In this regard, we introduce GTMGC, a novel network based on Graph-Transformer (GT) that seamlessly predicts the spatial configuration of molecules in a 3D space from their 2D topological architecture in an end-to-end manner. Moreover, we propose a novel self-attention mechanism called Molecule Structural Residual Self-Attention (MSRSA) for molecular structure modeling. This mechanism not only guarantees high model performance and easy implementation but also lends itself well to other molecular modeling tasks. Our method has been evaluated on the Molecule3D benchmark dataset and the QM9 dataset. Experimental results demonstrate that our approach achieves remarkable performance and outperforms current state-of-the-art methods as well as the widely used open-source software RDkit.",13,https://openreview.net/forum?id=F7QnIKlC1N,6.333333333333333,2.357022603955158,4.0,0.0
EgASiEujt6,Towards Controllable Diffusion Models via Training-Phase Guided Exploration,"['diffusion model', 'generative model', 'controllable generation']","[5, 5, 3, 5]","[2, 4, 5, 4]",0,"[489, 564, 1157, 571]","By formulating data samples’ formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel frame- work named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum entropy RL, which enables calculating gradients of policy parameters via sample episodes from a pay-off distribution proportional to exponentiated scaled rewards, rather than from policies themselves. Experiments on 3D shape and molecule generation tasks show significant improvements over existing conditional diffusion models.",4,https://openreview.net/forum?id=EgASiEujt6,4.5,0.8660254037844386,3.75,1.0897247358851685
EAvcKbUXwb,Interpreting Equivariant Representations,"['permutation equivariance', 'graph interpolation', 'variational autoencoder', 'VAE', 'molecule generation', 'visualization', 'representation learning', 'equivariance', 'invariance', 'graphs', 'group action', 'group convolutional neural network', 'GCNN']","[6, 5, 5]","[4, 2, 2]",0,"[294, 164, 425]","Latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. Invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. In this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. We show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. We propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: First, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the resulting invariant representation. Next, we study a rotation-equivariant representation used for image classification. Here, we illustrate how random invariant projections can be used to obtain an invariant representation with a high degree of retained information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation. Thus, while these ambiguities may be known by experienced developers of equivariant models, we make both the knowledge as well as effective tools to handle the ambiguities available to the broader community.",9,https://openreview.net/forum?id=EAvcKbUXwb,5.333333333333333,0.4714045207910317,2.6666666666666665,0.9428090415820634
Dr4qD9bzZd,Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design,"['metalloprotein design', 'protein sequence and structure co-design', 'functional protein design']","[6, 3, 3]","[5, 5, 4]",0,"[1181, 242, 674]","Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capability of our model to design protein sequences and structures that closely resemble their natural counterparts. Furthermore, in-depth analysis further confirms our model's ability to generate highly effective proteins capable of binding to their target metallocofactors.",31,https://openreview.net/forum?id=Dr4qD9bzZd,4.0,1.4142135623730951,4.666666666666667,0.4714045207910317
Dc4rXq3HIA,Improving Out-of-Domain Generalization with Domain Relations,['Out-of-Domain Generalization; Domain Relations; Distribution Shift'],"[6, 6, 6, 8, 8, 6]","[4, 3, 3, 4, 4, 4]",0,"[326, 250, 257, 235, 348, 395]","Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. In this paper, we focus on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called DG. Unlike previous approaches that aim to learn a single model that is domain invariant, DG leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, DG learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stronger out-of-domain generalization compared to the conventional averaging approach. Empirically, we evaluate the effectiveness of DG using both toy and real-world datasets for tasks such as temperature regression, land use classification, and molecule-protein binding affinity prediction. Our results show that DG consistently outperforms state-of-the-art methods.",25,https://openreview.net/forum?id=Dc4rXq3HIA,6.666666666666667,0.9428090415820634,3.6666666666666665,0.4714045207910317
DbRfXmzwjc,MAGNet: Motif-Agnostic Generation of Molecules from Shapes,"['Molecule Generation', 'Distribution Learning', 'GNNs']","[3, 8, 6, 1]","[3, 4, 4, 4]",0,"[388, 540, 426, 581]","Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from *in silico* predictions.
Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods struggle to represent substructures beyond their known motif set. To alleviate this issue and increase flexibility across datasets, we propose MAGNet, a graph-based model that generates abstract shapes before allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that accounts for the molecules' global context and facilitates learning adequate assignments of atoms and bonds onto shapes. Despite the added complexity of shape abstractions, MAGNet outperforms most other graph-based approaches on standard benchmarks. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more topologically distinct structures and, at the same time, diverse atom and bond assignments.",18,https://openreview.net/forum?id=DbRfXmzwjc,4.5,2.692582403567252,3.75,0.4330127018922193
CWoIj2XJuT,Unbalanced Diffusion Schrödinger Bridge,"['Schrödinger bridges', 'diffusion', 'entropic interpolation', 'unbalanced', 'finite measures']","[5, 5, 5, 3]","[3, 1, 3, 3]",0,"[651, 216, 686, 314]","_Schrödinger bridges_ (SBs) provide an elegant framework for modeling the temporal evolution of populations in physical, chemical, or biological systems. Such natural processes are commonly subject to changes in population size over time due to the emergence of new species or birth and death events. However, existing neural parameterizations of SBs such as _diffusion Schrödinger bridges_ ( DSBs) are restricted to settings in which the endpoints of the stochastic process are both _probability measures_ and assume _conservation of mass_ constraints. To address this limitation, we introduce _unbalanced_ DSBs which model the temporal evolution of marginals with arbitrary finite mass. This is achieved by deriving the time reversal of _stochastic differential equations_ (SDEs) with killing and birth terms. We present two novel algorithmic schemes that comprise a scalable objective function for training unbalanced DSBs and provide a theoretical analysis alongside challenging applications on predicting heterogeneous molecular single-cell responses to various cancer drugs and simulating the emergence and spread of new viral variants.",11,https://openreview.net/forum?id=CWoIj2XJuT,4.5,0.8660254037844386,2.5,0.8660254037844386
CUfSCwcgqm,Long-range Neural Atom Learning for Molecular Graphs,['Long Range Interaction; Molecular Graph;'],"[6, 8, 6]","[4, 3, 3]",0,"[285, 392, 447]","Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few \textit{Neural Atoms}, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms’ representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiments on three long-range graph benchmarks, covering both graph-level and link-level tasks on molecular graphs. We empirically justify that our method can be equipped with an arbitrary GNN and help to capture LRI.",22,https://openreview.net/forum?id=CUfSCwcgqm,6.666666666666667,0.9428090415820634,3.3333333333333335,0.4714045207910317
C4BikKsgmK,Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling,"['proteins', 'conformational sampling', 'diffusion models', 'score-based models', 'generative modeling', 'equivariant network']","[6, 6, 6, 6]","[3, 4, 3, 3]",0,"[474, 404, 339, 773]","The dynamic nature of proteins is crucial for determining their biological functions and properties, for which Monte Carlo (MC) and molecular dynamics (MD) simulations stand as predominant tools to study such phenomena. By utilizing empirically derived force fields, MC or MD simulations explore the conformational space through numerically evolving the system via Markov chain or Newtonian mechanics. However, the high-energy barrier of the force fields can hamper the exploration of both methods by the rare event, resulting in inadequately sampled ensemble without exhaustive running. Existing learning-based approaches perform direct sampling yet heavily rely on target-specific simulation data for training, which suffers from high data acquisition cost and poor generalizability. Inspired by simulated annealing, we propose Str2Str, a novel structure-to-structure translation framework capable of zero-shot conformation sampling with roto-translation equivariant property. Our method leverages an amortized denoising score matching objective trained on general crystal structures and has no reliance on simulation data during both training and inference. Experimental results across several benchmarking protein systems demonstrate that Str2Str outperforms previous state-of-the-art generative structure prediction models and can be orders of magnitude faster compared with long MD simulations.",21,https://openreview.net/forum?id=C4BikKsgmK,6.0,0.0,3.25,0.4330127018922193
ByOPJantEd,Wigner kernels: body-ordered equivariant machine learning without a basis,"['Geometric machine learning', 'kernel methods', 'chemistry', 'molecular modeling']","[6, 5, 6, 5]","[3, 2, 2, 4]",0,"[200, 138, 212, 537]","Machine-learning models based on a point-cloud representation of a physical object are ubiquitous in scientific applications and particularly well-suited to the atomic-scale description of molecules and materials. Among the many different approaches that have been pursued, the description of local atomic environments in terms of their discretized neighbor densities has been used widely and very successfully. 
We propose a novel density-based method which involves computing ""Wigner kernels''. These are fully equivariant and body-ordered kernels that can be computed iteratively with a cost that is independent of the basis used to discretize the density and grows only linearly with the maximum body-order considered. This is in marked contrast to feature-space models, whose number of terms and computational cost scale exponentially with increasing order of correlations.
We present several examples of the accuracy of models based on Wigner kernels in chemical applications, for both scalar and tensorial targets, reaching state-of-the-art accuracy on the popular QM9 benchmark dataset. We discuss the broader relevance of these findings to equivariant geometric machine-learning.",15,https://openreview.net/forum?id=ByOPJantEd,5.5,0.5,2.75,0.82915619758885
BZtEthuXRF,Manifold Diffusion Fields,"['Diffusion models', 'riemannian manifolds', 'graphs']","[6, 6, 8]","[4, 3, 4]",0,"[250, 412, 476]","We present Manifold Diffusion Fields (MDF), an approach that unlocks learning of diffusion models of data in general non-euclidean geometries. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator.  MDF represents functions using an explicit parametrization formed by a set of multiple input-output pairs. Our approach allows to sample continuous functions on manifolds and is invariant with respect to rigid and isometric transformations of the manifold. In addition, we show that MDF generalizes to the case where the training set contains functions on different manifolds. Empirical results on multiple datasets and manifolds including challenging scientific problems like weather prediction or  molecular conformation show that MDF can capture distributions of such functions with better diversity and fidelity than previous approaches.",16,https://openreview.net/forum?id=BZtEthuXRF,6.666666666666667,0.9428090415820634,3.6666666666666665,0.4714045207910317
BIglOUjfXX,Forked Diffusion for Conditional Graph Generation,"['conditional generative model', 'graph neural network', 'score-based diffusion']","[3, 5, 5, 3]","[4, 3, 3, 4]",0,"[435, 516, 315, 363]","We introduce a novel score-based diffusion framework that incorporates forking for conditional generation. In this framework, a single  parent diffusion process is associated with a primary variable (e.g., structure), while multiple child diffusion processes are employed, each dedicated to a dependent variable (e.g., property). The parent process guides the co-evolution of its child processes towards segregated representation spaces. This approach allows our models to manage conditional information flow effectively, uncover intricate interactions and dependencies, and ultimately unlock new generative capabilities. Our experimental results demonstrate the significant superiority of our method over contemporary baselines in the context of conditional graph generation, highlighting the potential of forking diffusion for enhancing conditional generation tasks and inverse molecular design tasks.",5,https://openreview.net/forum?id=BIglOUjfXX,4.0,1.0,3.5,0.5
BBD6KXIGJL,Hybrid Directional Graph Neural Network for Molecules,['Graph Neural Networks; Equivariance; Molecular model'],"[6, 8, 8]","[2, 2, 4]",0,"[181, 55, 1813]","Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can impose excessive constraints on the function form and network flexibility. To address these challenges, we introduce a novel network called the Hybrid Directional Graph Neural Network (HDGNN), which effectively combines strictly equivariant operations with learnable modules. We evaluate the performance of HDGNN on the QM9 dataset and the IS2RE dataset of OC20, demonstrating its state-of-the-art performance on several tasks and competitive performance on others. Our code is anonymously released on https://github.com/AnonymousACode/HDGNN.",21,https://openreview.net/forum?id=BBD6KXIGJL,7.333333333333333,0.9428090415820634,2.6666666666666665,0.9428090415820634
AxYTFpdlvj,Graph Decoding via Generalized Random Dot Product Graph,"['graph autoencoders', 'inner dot product decoder', 'generalized random dot product', 'link prediction', 'node clustering', 'molecular graph']","[3, 3, 1, 1]","[3, 4, 4, 5]",0,"[264, 213, 259, 287]","Graph Neural Networks (GNNs) have established themselves as the state-of-the-art methodology for a multitude of graph-related tasks, including but not limited to link prediction, node clustering, and classification. Despite their efficacy, the performance of GNNs in encoder-decoder architectures is often constrained by the limitations inherent in traditional decoders, particularly in the reconstruction of adjacency matrices.

In this paper, we introduce a novel graph decoding approach through the use of the Generalized Random Dot Product Graph (GRDPG) as a generative model for graph decoding. This novel methodology enhances the performance of encoder-decoder architectures across a range of tasks, owing to GRDPG's better capability to capture structures embedded within adjacency matrices.

To  evaluate our approach, we design a benchmark focused on graphs of varying sizes, thereby enriching the diversity of existing benchmarks for link prediction and node clustering tasks. Our experiments span a variety of tasks, encompassing both traditional benchmarks and specialized domains such as molecular graphs.

The empirical results show the capability of GRDPG on faithfully capturing properties of the original graphs while simultaneously improving the performance metrics of encoder-decoder architectures. By addressing the subtleties involved in adjacency matrix reconstruction, we  elevate the overall performance of GNN-based architectures, rendering them more robust and versatile for a wide array of real-world applications, with special regard on molecular graphs.",8,https://openreview.net/forum?id=AxYTFpdlvj,2.0,1.0,4.0,0.7071067811865476
AialDkY6y3,Deep Graph Predictions using Dirac-Bianconi Graph Neural Networks,"['Graph Neural Networks', 'graph convolution', 'physic inspired machine learning']","[6, 5, 3, 5, 3]","[4, 3, 4, 3, 4]",0,"[496, 346, 490, 518, 357]","Viewing Graph Neural Networks as network dynamical systems on graphs has proven a fruitful inspiration for designing interesting GNN architectures. This work introduces the Dirac-Bianconi Graph Neural Network (DBGNN) based on Bianconi's topological Dirac equation on graphs. While heat equations based on network Laplacian tend to smooth out differences, Dirac equations typically feature long-range propagation. We indeed find that the DBGNN layer does not lead to an equilibration, or smoothing, of nodal features, even after hundreds of steps. A further distinguishing feature of the topological Dirac equation is that it treats edges and nodes on the same footing. Consequently, we expect DBGNN to be useful in contexts where edges encode more than mere logical connectivity, but have physical properties as well. We show competitive performance for molecular property prediction and superior performance for predicting the dynamic stability of power grids. In the case of power grids, DBGNN achieves robust out-of-distribution generalization, showing that structural relations are learned.",21,https://openreview.net/forum?id=AialDkY6y3,4.4,1.2,3.6,0.4898979485566356
A6kK5e3DhR,Controllable Data Generation via Iterative Data-Property Mutual Mappings,"['Controllable data generation', 'generative models']","[3, 5, 5, 1]","[4, 4, 4, 4]",1,"[318, 497, 765, 408]","Deep generative models have been widely used for their ability to generate realistic data samples in various areas, such as images, molecules, text, and speech. One major goal of data generation is controllability, namely to generate new data with desired properties. Despite growing interest in the area of controllable generation, significant challenges still remain, including 1) Disentangling desired properties with unrelated latent variables, 2) out-of-distribution property control, and 3) objective optimization for out-of-distribution property control. To address these challenges, in this paper, we propose a general framework to enhance VAE-based data generators with property controllability and disentanglement ensure. Our proposed objective can be optimized on both data seen and unseen in the training set. We propose a training procedure to train the objective in a semi-supervised manner by iteratively conducting mutual mappings between the data and properties. The proposed framework is implemented on four VAE-based controllable generators to evaluate its performance on property error, disentanglement performance, generation quality, and training time. The results indicate that our proposed framework enables more precise control over the properties of generated samples in a short training time, ensuring the disentanglement stated above and keeping the validity of the generated samples.",4,https://openreview.net/forum?id=A6kK5e3DhR,3.5,1.6583123951777,4.0,0.0
9rPyHyjfwP,Domain-Agnostic Molecular Generation with Self-feedback,"['molecule generation', 'pre-trained language models', 'SELFIES', 'natural products', 'self-feedback']","[8, 6, 8, 6]","[4, 4, 4, 4]",0,"[489, 709, 218, 418]","The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To tackle these challenges, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. Through the reconstruction of over 100 million molecular SELFIES, MolGen internalizes profound structural and grammatical insights. This is further enhanced by domain-agnostic molecular prefix tuning, fostering robust knowledge transfer across diverse domains. Importantly, our self-feedback paradigm steers the model away from ""molecular hallucinations"", ensuring alignment between the model's estimated probabilities and real-world chemical preferences. Extensive experiments on well-known benchmarks underscore MolGen's optimization capabilities in properties such as penalized logP, QED, and molecular docking. Additional analyses affirm its proficiency in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space. The pre-trained model, codes, and datasets are publicly available for future research.",21,https://openreview.net/forum?id=9rPyHyjfwP,7.0,1.0,4.0,0.0
9g8h5HwZMy,Subgraph Diffusion for 3D Molecular Representation Learning: Combining Continuous and Discrete,"['Diffusion model', 'Molecular representation learning', 'Subgraph', 'Masked vector']","[6, 6, 5, 3]","[4, 3, 4, 5]",0,"[340, 170, 384, 432]","Molecular representation learning has shown great success in AI-based drug discovery. The 3D geometric structure contains crucial information about the underlying energy function, related to the physical and chemical properties. Recently, denoising diffusion probabilistic models have achieved impressive results in molecular conformation generation. However, the knowledge of pre-trained diffusion models has not been fully exploited in molecular representation learning. In this paper, we study the ability of representation learning inherent in the diffusion model for conformation generation. We introduce a new general diffusion model framework called MaskedDiff for molecular representation learning. Instead of adding noise to atoms like conventional diffusion models, MaskedDiff uses a discrete distribution to select a subset of the atoms to add continuous Gaussian noise at each step during the forward process. Further, we develop a novel subgraph diffusion model termed SUBGDIFF for enhancing the perception of molecular substructure in the denoising network (noise predictor), by incorporating auxiliary subgraph predictors during training. Experiments on molecular conformation generation and 3D molecular property prediction demonstrate the superior performance of our approach.",4,https://openreview.net/forum?id=9g8h5HwZMy,5.0,1.224744871391589,4.0,0.7071067811865476
8jKuUHsndT,Re-evaluating Retrosynthesis Algorithms with Syntheseus,"['retrosynthesis', 'reaction prediction', 'chemistry', 'drug design', 'science']","[8, 3, 6, 5]","[4, 4, 3, 4]",0,"[450, 327, 275, 245]","The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.",18,https://openreview.net/forum?id=8jKuUHsndT,5.5,1.8027756377319946,3.75,0.4330127018922193
8DLVrWL78S,Streamlining Generative Models for Structure-Based Drug Design,"['drug design', 'binding', 'docking', 'graph neural networks', 'generalization bounds']","[3, 3, 5, 5]","[3, 4, 4, 4]",0,"[348, 379, 361, 230]","Generative models for structure-based drug design (SBDD) aim to generate novel 3D molecules for specified protein targets $\textit{in silico}$. The prevailing paradigm focuses on model expressivity - typically with powerful Graph Neural Network (GNN) models - but is agnostic to binding affinity during training, potentially overlooking better molecules. We address this issue with a two-pronged approach: learn an economical surrogate for affinity to infer an unlabeled molecular graph, and optimize for labels conditioned on this graph and desired molecular properties (e.g., QED, SA). The resulting model FastSBDD achieves state-of-the-art results as well as streamlined computation and model size (up to 1000x faster and with 100x fewer trainable parameters compared to existing methods), paving way for improved docking software. We also establish rigorous theoretical results to expose the representation limits of GNNs in SBDD contexts and the generalizability of our affinity scoring model, advocating more emphasis on generalization going forward.",20,https://openreview.net/forum?id=8DLVrWL78S,4.0,1.0,3.75,0.4330127018922193
7ezBaMwOqY,Trading-off Multiple Properties for Molecular Optimization,"['Molecular Optimization', 'Multiple Properties']","[3, 5, 5, 6]","[4, 4, 4, 2]",0,"[445, 854, 679, 214]","Molecular optimization, a critical research area in drug discovery, aims to enhance the properties or performance of molecules through systematic modifications of their chemical structures. Recently, existing Multi-Objective Molecular Optimization (MOMO) methods are extended from Single-Objective Molecular Optimization (SOMO) approaches by employing techniques such as Linear Scalarization, Evolutionary Algorithms, and Multi-Objective Bayesian Optimization. In Multi-Objective Optimization, the ideal goal is to find Pareto optimal solutions over different preferences, which indicate the importance of different objectives. However, these straightforward extensions often struggle with trading off multiple properties due to the conflicting or correlated nature of certain properties.  More specifically, current MOMO methods derived from SOMO are still challenged in finding preference-conditioned Pareto solutions and exhibit low efficiency in Pareto search. To address the aforementioned problems, we propose the \textbf{P}reference-\textbf{C}onditioned \textbf{I}nversion (PCI) framework,  efficiently ``inverting'' a pre-trained surrogate oracle under the guidance of a non-dominated gradient, to generate candidate Pareto optimal molecules over preference-conditioned distributions. Additionally, we provide theoretical guarantees for PCI's capability in converging to preference-conditioned solutions. This unique characteristic enables PCI to search the full Pareto front approximately, thereby assisting in the discovery of diverse molecules with varying ratios of properties. Comprehensive experimental evaluations show that our model significantly outperforms state-of-the-art baselines in multi-objective molecular optimization settings.",15,https://openreview.net/forum?id=7ezBaMwOqY,4.75,1.0897247358851685,3.5,0.8660254037844386
7UhxsmbdaQ,Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design,"['Molecular generative models', 'reinforcement learning', 'natural language processing', 'drug discovery', 'sample-efficiency', 'explainability']","[3, 8, 8, 8]","[4, 5, 4, 4]",0,"[267, 424, 754, 363]","Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration is the first method to jointly address explainability and sample efficiency for molecular design.",30,https://openreview.net/forum?id=7UhxsmbdaQ,6.75,2.165063509461097,4.25,0.4330127018922193
7TOs9gjAg1,Removing Biases from Molecular Representations via Information Maximization,"['Molecular Representation', 'Batch Effect', 'Contrastive Learning', 'Information Maximization', 'Drug Discovery']","[6, 8, 6, 6]","[2, 3, 3, 3]",0,"[303, 337, 380, 857]","High-throughput drug screening -- using cell imaging or gene expression measurements as readouts of drug effect -- is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be divided into multiple experiments, a key difficulty is dealing with batch effects, which can introduce systematic errors and non-biological associations in the data. We propose InfoCORE, an Information maximization approach for COnfounder REmoval, to effectively deal with batch effects and obtain refined molecular representations. InfoCORE establishes a variational lower bound on the conditional mutual information of the latent representations given a batch identifier. It adaptively reweights samples to equalize their implied batch distribution. Extensive experiments on drug screening data reveal InfoCORE's superior performance in a multitude of tasks including molecular property prediction and molecule-phenotype retrieval. Additionally, we show results for how InfoCORE offers a versatile framework and resolves general distribution shifts and issues of data fairness by minimizing correlation with spurious features or removing sensitive attributes.",23,https://openreview.net/forum?id=7TOs9gjAg1,6.5,0.8660254037844386,2.75,0.4330127018922193
7Jer2DQt9V,The Unreasonable Effectiveness of Pretraining in Graph OOD,"['Graph pre-training', 'Graph out of distribution']","[5, 5, 5, 3]","[4, 4, 4, 4]",0,"[346, 474, 508, 275]","Graph neural networks have shown significant progress in various tasks, yet their ability to generalize in out-of-distribution (OOD) scenarios remains an open question. In this study, we conduct a comprehensive benchmarking of the efficacy of graph pre-trained models in the context of OOD challenges, named as PODGenGraph. We conduct extensive experiments across diverse datasets, spanning general and molecular graph domains and encompassing different graph sizes. Our benchmark is framed around distinct distribution shifts, including both concept and covariate shifts, whilst also varying the degree of shift. Our findings are striking: even basic pre-trained models exhibit performance that is not only comparable to, but often surpasses, specifically designed to handle distribution shift. We further investigate the results, examining the influence of the key factors (e.g., sample size, learning rates, in-distribution performance etc) of pre-trained models for OOD generalization. In general, our work shows that pre-training could be a flexible and simple approach to OOD generalization in graph learning. Leveraging pre-trained models together for graph OOD generalization in real-world applications stands as a promising avenue for future research.",12,https://openreview.net/forum?id=7Jer2DQt9V,4.5,0.8660254037844386,4.0,0.0
7JRbs3i9Ei,Machine Learning for PROTAC Engineering,"['Deep learning', 'Chemoinformatics', 'PROTAC', 'Drug design.']","[5, 5, 3]","[3, 4, 4]",0,"[464, 488, 673]","PROTACs are a promising therapeutic technology that harnesses the cell's built-in degradation processes to degrade specific proteins. Despite their potential, developing new PROTAC molecules is challenging and requires significant expertise, time, and cost. Meanwhile, machine learning has transformed various scientific fields, including drug development. In this work, we present a strategy for curating open-source PROTAC data and propose an open-source toolkit for predicting the degradation effectiveness, i.e., activity, of novel PROTAC molecules. We organized the curated data into 16 different datasets ready to be processed by machine learning models. The datasets incorporate important features such as $pDC_{50}$, $D_{max}$, E3 ligase type, POI amino acid sequence, and experimental cell type. Our toolkit includes a configurable PyTorch dataset class tailored to process PROTAC features, a customizable machine learning model for processing various PROTAC features, and a hyperparameter optimization mechanism powered by Optuna. To evaluate the system, three surrogate models were developed utilizing different PROTAC representations. Using our automatically-curated public datasets, the best models achieved a 71.4% validation accuracy and a 0.73 ROC-AUC validation score. This is not only comparable to state-of-the-art models for protein degradation prediction, but also open-source, easily-reproducible, and less computationally complex than existing approaches.",6,https://openreview.net/forum?id=7JRbs3i9Ei,4.333333333333333,0.9428090415820634,3.6666666666666665,0.4714045207910317
770DetV8He,RetroBridge: Modeling Retrosynthesis with Markov Bridges,"['Retrosynthesis', 'Reactions', 'Chemistry', 'Drug Discovery', 'Markov Bridge']","[8, 6, 8, 6]","[4, 3, 4, 5]",0,"[160, 366, 309, 248]","Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing multi-step reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior distribution. We then address the retrosynthesis planning problem with our novel framework and introduce RetroBridge, a template-free retrosynthesis modeling approach that achieves state-of-the-art results on standard evaluation benchmarks.",16,https://openreview.net/forum?id=770DetV8He,7.0,1.0,4.0,0.7071067811865476
760br3YEtY,($\texttt{PEEP}$) $\textbf{P}$redicting $\textbf{E}$nzym$\textbf{e}$ $\textbf{P}$romiscuity with its Molecule Mate – an Attentive Metric Learning Solution,['Protein Engineering; Metric Learning;'],"[5, 6, 5, 6, 6]","[2, 3, 4, 4, 3]",0,"[178, 196, 231, 620, 275]","Annotating the functions of proteins (e.g., enzymes) is a fundamental challenge, due to their diverse functionalities and rapidly increased number of protein sequences in databases. Traditional approaches have limited capability and suffer from false positive predictions. Recent machine learning (ML) methods reach satisfactory prediction accuracy but still fail to generalize, especially for less-studied proteins and those with previously uncharacterized functions or promiscuity. To address these pain points, we propose a novel ML algorithm, PEEP, to predict enzyme promiscuity, which integrates biology priors of protein functionality to regularize the model learning. To be specific, at the input level, PEEP fuses the corresponding molecule into protein embeddings to gain their reaction information; at the model level, a tailored self-attention is leveraged to capture importance residues which we found are aligned with the active site in protein pocket structure; at the objective level, we embed functionality label hierarchy into metric learning objectives by imposing larger distance margin between proteins that have less functionality in common. PEEP is extensively validated on three public benchmarks, achieving up to 4.6%,3.1%,3.7% improvements on F-1 scores compared to existing methods. Moreover, it demonstrates impressive generalization to unseen protein sequences with unseen functionalities. Codes are included in the supplement.",16,https://openreview.net/forum?id=760br3YEtY,5.6,0.48989794855663565,3.2,0.7483314773547882
6Uc7Fgwrsm,OmniMixup: Generalize Mixup with Mixing-Pair Sampling Distribution,"['Mixup', 'Machine Learning', 'molecule property prediction', 'image classification', 'data augmentation']","[3, 3, 5, 1, 5]","[5, 3, 3, 4, 4]",0,"[360, 624, 391, 447, 290]","Mixup is a widely-adopted data augmentation techniques to mitigates the overfitting issue in empirical risk minimization. Current works of modifying Mixup are modality-specific, thereby limiting the applicability across diverse modalities. Although alternative approaches try circumventing such barrier via mixing-up data from latent features based on sampling distribution, they still require domain knowledge for designing sampling distribution. Moreover, a unified theoretical framework for analyzing the generalization bound for this line of research remains absent. In this paper, we introduce OmniMixup, a generalization of prior works by introducing Mixing-Pair Sampling Distribution (MPSD), accompanied by a holistic theoretical analysis framwork. We find both theoretically and empirically that the Mahalanobis distance (M-Score), derived from the sampling distribution, offers significant insights into OmniMixup's generalization capabilities. Accordingly, we propose OmniEval, an evaluation framework designed to autonomously identify the optimal sampling distribution. The empirical study on both images and molecules demonstrates that 1) OmniEval is adept at determining the appropriate sampling distribution for OmniMixup, and 2) OmniMixup exhibits promising capability for application across various modalities and domains.",15,https://openreview.net/forum?id=6Uc7Fgwrsm,3.4,1.4966629547095767,3.8,0.7483314773547882
4IT2pgc9v6,One For All: Towards Training One Graph Model For All Classification Tasks,"['Graph Neural Network', 'Large Language Model', 'In-context Learning']","[1, 6, 6, 6]","[5, 4, 4, 3]",0,"[191, 547, 512, 152]","Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it difficult to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose **One for All (OFA)**, the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose graph classification model across domains.",19,https://openreview.net/forum?id=4IT2pgc9v6,4.75,2.165063509461097,4.0,0.7071067811865476
3z60EWfh1p,Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks,"['Transfer Learning', 'Inductive Transfer', 'Geometrical Deeplearning', 'Regression']","[5, 6, 8, 6]","[4, 3, 2, 3]",0,"[351, 592, 473, 320]","Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (${\it GATE}$). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that ${\it GATE}$ outperforms conventional methods and exhibits stable behavior in both the latent space and extrapolation regions for various molecular graph datasets.",13,https://openreview.net/forum?id=3z60EWfh1p,6.25,1.0897247358851685,3.0,0.7071067811865476
3oTPsORaDH,Improving Generalization in Equivariant Graph Neural Networks with Physical Inductive Biases,"['Equivariant Graph Neural Network', 'Graph Neural Network']","[6, 8, 6]","[4, 3, 4]",0,"[395, 367, 715]","Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines.",15,https://openreview.net/forum?id=3oTPsORaDH,6.666666666666667,0.9428090415820634,3.6666666666666665,0.4714045207910317
3QR230r11w,Multi-Fidelity Active Learning with GFlowNets,"['gflownets', 'multi-fidelity', 'active learning', 'bayesian optimization', 'scientific discovery', 'biological sequence design', 'molecular modelling', 'material discovery']","[8, 6, 3, 3]","[3, 4, 4, 4]",0,"[573, 245, 454, 1154]","In the last decades, the capacity to generate large amounts of data in science and engineering applications has been growing steadily. Meanwhile, the progress in machine learning has turned it into a suitable tool to process and utilise the available data. Nonetheless, many relevant scientific and engineering problems present challenges where current machine learning methods cannot yet efficiently leverage the available data and resources. For example, in scientific discovery, we are often faced with the problem of exploring very large, structured and high-dimensional spaces, and where querying a high fidelity, black-box objective function is very expensive. Progress in machine learning methods that can efficiently tackle such problems would help accelerate currently crucial areas such as drug and materials discovery. In this paper, we propose a multi-fidelity active learning algorithm with GFlowNets as a sampler, to efficiently discover diverse, high-scoring candidates where multiple approximations of the black-box function are available at lower fidelity and cost. Our evaluation on molecular discovery tasks show that multi-fidelity active learning with GFlowNets can discover high-scoring candidates at a fraction of the budget of its single-fidelity counterpart while maintaining diversity, unlike RL-based alternatives. These results open new avenues for multi-fidelity active learning to accelerate scientific discovery and engineering design.",23,https://openreview.net/forum?id=3QR230r11w,5.0,2.1213203435596424,3.75,0.4330127018922193
3K3aWRpRNq,Reducing Atomic Clashes in Geometric Diffusion Models for 3D Structure-Based Drug Design,"['Structure Based Drug Design', 'Geometric Molecular Generation', 'Diffusion Models']","[3, 3, 6, 3]","[3, 4, 4, 3]",0,"[233, 238, 413, 418]","In the domain of Three-dimensional Structure-Based Drug Design (3D SBDD), the 3D spatial structures of target pockets serve as inputs for the generation of molecular geometric graphs. The Geometric Diffusion Model (GDM) has been recognized as the state-of-the-art (SOTA) method in 3D SBDD, attributed to its exceptional generation capabilities on geometric graphs. However, the inherent data-driven nature of GDM occasionally neglects critical inter-molecular interactions, such as Van der Waals force and Hydrogen Bonding. Such omissions could produce molecules that violate established physical principles. Particular evidence is that GDMs exhibit atomic clashes during generation due to the overly close proximity of generated molecules to protein structures. To address this, our paper introduces a novel constrained sampling process designed to obviate such undesirable collisions. By integrating a non-convex constraint within the current Langevin Dynamics (LD) of GDM and utilizing the proximal regularization techniques, we force molecular coordinates to obey the imposed physical constraints. Notably, the proposed method requires no modifications to the training process of GDMs. Empirical evaluations show a significant reduction in atomic clashes via the proposed method compared to the original LD process of GDMs.",13,https://openreview.net/forum?id=3K3aWRpRNq,3.75,1.299038105676658,3.5,0.5
370Bvdd3z7,EC-Conf: An Ultra-fast Diffusion Model for Molecular Conformation Generation with Equivariant Consistency,"['fast diffusion model', 'equivariant consistency', 'molecule conformation generation']",[],[],0,[],"Despite recent advancement in 3D molecule conformation generation driven by diffusion models, its high computational cost in iterative diffusion/denoising process limits its application. In this paper, an equivariant consistency model (EC-Conf) was proposed as a fast diffusion method for low-energy conformation generation. In EC-Conf, a modified SE (3)-equivariant transformer model was directly used to encode the Cartesian molecular conformations and a highly efficient consistency diffusion process was carried out to generate molecular conformations. It was demonstrated that, with only one sampling step, it can already achieve comparable quality to other diffusion-based models running with thousands denoising steps. Its performance can be further improved with a few more sampling iterations. The performance of EC-Conf is evaluated on both GEOM-QM9 and GEOM-Drugs sets. Our results demonstrate that the efficiency of EC-Conf for learning the distribution of low energy molecular conformation is at least two magnitudes higher than current SOTA diffusion models and could potentially become a useful tool for conformation generation and sampling. In the near future, we will release our code.",0,https://openreview.net/forum?id=370Bvdd3z7,,,,
2UlfvGU6rL,Equivariant Graph Neural Operator for Modeling 3D Dynamics,"['Equivariant Graph Neural Network', 'Neural Operator', '3D Dynamics']","[5, 6, 5, 8]","[3, 3, 4, 3]",0,"[320, 344, 356, 219]","Modeling the complex three-dimensional (3D) dynamics of relational systems is an important problem in the natural sciences, with applications ranging from molecular simulations to particle mechanics. Machine learning methods have achieved good success by learning graph neural networks to model spatial interactions. However, these approaches do not faithfully capture temporal correlations since they only model next-step predictions. In this work, we propose Equivariant Graph Neural Operator (EGNO), a novel and principled method that directly models dynamics as trajectories instead of just as next-step prediction. Different from existing methods, EGNO explicitly learns the temporal evolution of 3D dynamics where we formulate the dynamics as a function over time and learn neural operators to approximate it. To capture the temporal correlations while keeping the intrinsic SE(3)-equivariance, we develop equivariant temporal convolutions parameterized in the Fourier space and build EGNO by stacking the Fourier layers over equivariant networks. Comprehensive experiments in multiple domains, including particle simulations, human motion capture, and molecular dynamics, demonstrate the significantly superior performance of EGNO against existing methods, thanks to the equivariant temporal modeling.",18,https://openreview.net/forum?id=2UlfvGU6rL,6.0,1.224744871391589,3.25,0.4330127018922193
1IaoWBqB6K,DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility,"['diffusion', 'diffusion models', 'docking', 'generative model']","[6, 5, 6, 3]","[3, 4, 3, 4]",0,"[522, 454, 380, 360]","When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task.",15,https://openreview.net/forum?id=1IaoWBqB6K,5.0,1.224744871391589,3.5,0.5
0oIkKERYhH,DOG: Discriminator-only Generation Beats GANs on Graphs,"['generative modeling', 'graph generation']","[5, 3, 6, 3]","[4, 4, 3, 4]",0,"[587, 476, 289, 249]","We propose discriminator-only generation (DOG) as a generative modeling approach that bridges the gap between energy-based models (EBMs) and generative adversarial networks (GANs). DOG generates samples through iterative gradient descent on a discriminator's input, eliminating the need for a separate generator model. This simplification obviates the extensive tuning of generator architectures required by GANs. In the graph domain, where GANs have lagged behind diffusion approaches in generation quality, DOG demonstrates significant improvements over GANs using the same discriminator architectures. Surprisingly, despite its computationally intensive iterative generation, DOG produces higher-quality samples than GANs on the QM9 molecule dataset in less training time.",14,https://openreview.net/forum?id=0oIkKERYhH,4.25,1.299038105676658,3.75,0.4330127018922193
0fSNU64FV7,Sorting Out Quantum Monte Carlo,"['quantum chemistry', 'scientific machine learning', 'quantum monte carlo', 'quantum statisical mechanics', 'inductive bias']","[6, 3, 5, 3]","[4, 4, 3, 5]",0,"[273, 694, 1039, 256]","Molecular modeling at the quantum level requires choosing a parameterization of the wavefunction that both respects the required symmetries, and is scalable to systems of many particles. For the simulation of fermions, valid parameterizations must be antisymmetric with the transposition of particles. Typically, antisymmetry is enforced by leveraging the anti-symmetry of determinants with respect to exchange of matrix rows, but this involves computing a full determinant each time the wavefunction is evaluated. Instead, we introduce a new antisymmetrization layer derived from sorting, the $\text{\emph{sortlet}}$, which scales as $O(N \log N )$ in the number of particles, in contrast to the $O(N^3)$ of the determinant. We show experimentally that applying this anti-symmeterization layer on top of an attention based neural-network backbone yields a flexible wavefunction parameterization capable of reaching chemical accuracy when approximating the ground state of first-row atoms and molecules.",16,https://openreview.net/forum?id=0fSNU64FV7,4.25,1.299038105676658,4.0,0.7071067811865476
0VBsoluxR2,MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design,"['Materials design', 'diffusion model', 'metal-organic framework', 'carbon capture', 'generative model', 'AI for Science']","[8, 8, 8, 8]","[4, 3, 3, 4]",0,"[354, 409, 562, 573]","Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. As the diffusion model generates 3D MOF structures by predicting scores in E(3), we employ equivariant graph neural networks that respect the permutational and roto-translational symmetries. We comprehensively evaluate our model's capability to generate valid and novel MOF structures and its effectiveness in designing outstanding MOF materials for carbon capture applications with molecular simulations.",18,https://openreview.net/forum?id=0VBsoluxR2,8.0,0.0,3.5,0.5
070DFUdNh7,GraphGPT: Graph Learning with Generative Pre-trained Transformers,"['Graph', 'GPT', 'Generative', 'Pre-train', 'Fine-tune', 'Transformer', 'GraphGPT']","[5, 5, 3, 5]","[4, 3, 3, 3]",0,"[371, 376, 218, 184]","We introduce GraphGPT, a novel model for Graph learning by self-supervised Generative Pre-training Transformers. Our model transforms each graph or sampled subgraph into a sequence of tokens representing the node, edge and attributes reversibly using the Eulerian path first. Then we feed the tokens into a standard transformer decoder and pre-train it with the next-token-prediction (NTP) task. Lastly, we fine-tune the GraphGPT model with the supervised tasks. This intuitive, yet effective model achieves superior or close results to the state-of-the-art methods for the graph-, edge- and node-level tasks on the large scale molecular dataset PCQM4Mv2, the protein-protein association dataset ogbl-ppa and the ogbn-proteins dataset from the Open Graph Benchmark (OGB). Furthermore, the generative pre-training enables us to train GraphGPT up to 400M+ parameters with consistently increasing performance, which is beyond the capability of GNNs and previous graph transformers. The source code and pre-trained checkpoints will be released soon to pave the way for the graph foundation model research, and also to assist the scientific discovery in pharmaceutical, chemistry, material and bio-informatics domains, etc.",11,https://openreview.net/forum?id=070DFUdNh7,4.5,0.8660254037844386,3.25,0.4330127018922193
